https://researchcomputing.princeton.edu/memory

htop -u $USER
Look at RES column

MAP profiler:
https://researchcomputing.princeton.edu/faq/profiling-with-allinea-ma

squeue -u $USER
scontrol show jobid -dd <jobid>
sstat --format=JobID,AveCPUFreq,AveDiskWrite,TresUsageInMax%80,TresUsageInMaxNode%80 -j <JobID> --allsteps

seff <JobID>
#SBATCH --mail-type=begin        # send email when job begins
#SBATCH --mail-type=end          # send email when job ends
#SBATCH --mail-type=fail         # send email if job fails
#SBATCH --mail-user=<YourNetID>@princeton.edu #Not needed for NYU



snodes

There are no Slurm directives for specifying the GPU memory. In the event of an 
out-of-memory (OOM) error, one must modify the application script or the application
 itself to resolve the error. When training neural networks, the most common cause
  of out-of-memory errors on the GPU is using too large of a batch size.
  
Slurm email reports and seff say nothing about the amount of GPU memory being used
 by a job. To see this value one must SSH to the compute node where the job is running
  and run the nvidia-smi command. For more on this see the bottom of this page.


NYU Slurm best practices:
seff JOB_ID (For completed jobs mainly)

Check resources usage of a currently running job
https://sites.google.com/a/nyu.edu/nyu-hpc/documentation/prince/batch/slurm-best-practices
ssh <node-name>
top -u $USER
nvidia-smi
nvidia-smi -l 60
nvidia-smi -h
nvidia-smi -L
nvidia-smi -q
nvidia-smi -u

Take a look how much GPU processing power your job is using.
It may happen that your code does not scale well, and it is better to use 1 or 2 GPUs instead of 4

You can also take a look at GPU memory utilization.

Most of our nodes don't have NVlink, which may make scaling across GPUs much less 
efficient comparing to systems which do.

To understand why your job is waiting in the queue you can run 
squeue  -j <JobID> -o "%.18i %.9P %.8j %.8u %.8T %.10M %.9l %.6D %R"
Last column of the output would indicate a reason. You can find out more about 
squeue output format from man squeue. 

The column "NODELIST(REASON)" in the end is job status due to the reason(s), which can be :

    Priority: higher priority jobs exist
    Resources: waiting for resources to become available
    BeginTime: start time not reached yet
    Dependency: wait for a depended job to finish
    QOSMaxCpuPerUserLimit: number of CPU cores limit is reached
    JobArrayTaskLimit: limit of tasks in the JobArray is reached
    

LIMITS
    There is a limit of 999 jobs per user. 
    Job lifetime is limited to 7 days (168 hours), but you can request an extension by emailing HPC team (hpc@nyu.edu).

    One important limitation regulates how many CPU cores are available per user in different partitions. You can get this by running:

sacctmgr list qos format=name,maxwall,maxtresperuser%15,flags%35 where name=cpu48,cpu168,gpu48,gpu168
      Name     MaxWall       MaxTRESPU                               Flags 
---------- ----------- --------------- ----------------------------------- 
     cpu48  2-00:00:00        cpu=1000      OverPartQOS,PartitionTimeLimit 
    cpu168  7-00:00:00         cpu=600      OverPartQOS,PartitionTimeLimit 
     gpu48  2-00:00:00     gres/gpu=30      OverPartQOS,PartitionTimeLimit 
    gpu168  7-00:00:00     gres/gpu=20      OverPartQOS,PartitionTimeLimit



    Another important limits to be aware of are the limits on how many CPU cores can be used together with GPU(s). Here are some of these limits:

| # gpus | max_cpus | max_memory |

         gpu type = "k80"      
|--------+----------+------------|
|      1 |       12 |        100 |
|      2 |       14 |        150 |
|      3 |       24 |        200 |
|      4 |       28 |        250 |
|      5 |        7 |         70 |
|      6 |        8 |         90 |
|      7 |        9 |        105 |
|      8 |       10 |        120 |

         gpu type = "p1080"            
|--------+----------+------------|
|      1 |       12 |         50 |
|      2 |       14 |         75 |
|      3 |       24 |        100 |
|      4 |       28 |        125 |

         gpu type = "p40"             
|--------+----------+------------|
|      1 |       12 |        100 |
|      2 |       14 |        150 |
|      3 |       24 |        200 |
|      4 |       28 |        250 |

         gpu type =  "p100"
|--------+----------+------------|
|      1 |       12 |        100 |
|      2 |       14 |        150 |
|      3 |       24 |        200 |
|      4 |       28 |        250 |

                    gpu type =  "v100"
|--------+----------+------------|
|      1 |       20 |        150 |
|      2 |       15 |        300 |
|      3 |       18 |        350 |
|      4 |       20 |        400 |
|      5 |       30 |        425 |
|      6 |       35 |        450 |
|      7 |       38 |        475 |
|      8 |       40 |        500 |
|--------+----------+------------|
From this table you can for example see, that job asking for 8 P100 GPUs will not be 
queued (there aren't any nodes, providing access to 8 P100s). Another example is that
 request for 2 V100s and 16 cores will also not be granted. 


Some other useful SLURM commands that can help getting information about running and pending jobs are
# detailed information for a job:
scontrol show jobid -dd <jobid>

# show status of a currently running job
# (see 'man sstat' for other available JOB STATUS FIELDS)
sstat --format=TresUsageInMax%80,TresUsageInMaxNode%80 -j <JobID> --allsteps
DOESN't SAY MUCH ABOUT GPU. Only CPU related:
cpu=00:03:56,energy=0,fs/disk=4371923002,mem=4990692K,pages=0,vmem=163320964K

# get stats for completed jobs 
# (see 'man sacct' for other JOB ACCOUNTING FIELDS)
sacct -j <jobid> --format=JobID,JobName,MaxRSS,Elapsed

# the same information for all jobs of a user:
sacct -u <username> --format=JobID,JobName,MaxRSS,Elapsed 


Cluster Status:
https://sites.google.com/a/nyu.edu/nyu-hpc/systems/status


When a batch job is finished it produces an exit code (among other useful data). 
To view the error code of the job you can use: 
sacct -b -j <JobID>
When reaching out to the HPC team asking for help with failing jobs, it is useful
 to find an exit code from the job at question.  
