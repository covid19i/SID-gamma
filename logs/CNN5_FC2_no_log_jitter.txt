(base) [ir967@gv08 Learning-to-See-in-the-Dark]$ vi CNN5_FC2_no_log.py
(base) [ir967@gv08 Learning-to-See-in-the-Dark]$ python CNN5_FC2_no_log.py 
Traceback (most recent call last):
  File "CNN5_FC2_no_log.py", line 6, in <module>
    import os, time, scipy.io
ModuleNotFoundError: No module named 'scipy'
(base) [ir967@gv08 Learning-to-See-in-the-Dark]$ conda activate sid2
(sid2) [ir967@gv08 Learning-to-See-in-the-Dark]$ python CNN5_FC2_no_log.py 




Current date and time : 
2020-12-13 01:20:37
Found 161 images to train with

Training on 161 images only

2020-12-13 01:20:37.650865: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-13 01:20:37.799788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38
pciBusID: 0000:06:00.0
totalMemory: 31.75GiB freeMemory: 31.45GiB
2020-12-13 01:20:37.799820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-13 01:20:38.074035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-13 01:20:38.074070: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-13 01:20:38.074091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-13 01:20:38.074186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30507 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:06:00.0, compute capability: 7.0)
No checkpoint found at ./gt_Sony_CNN5_FC2_no_log_jitter/. Hence, will create the folder.

last epoch of previous run: 0
rawpy read the 0th file at location: ./dataset/Sony/long/00018_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 0.00211, 0.00030, 300.00000, 242663
min, max, mean, gamma, argmax: 0.00001, 0.00400, 0.00099, 250.00000, 3485919
min, max, mean, gamma, argmax: 0.00001, 0.01000, 0.00150, 100.00000, 1
min, max, mean, gamma, argmax: 0.00000, 0.00401, 0.00021, 100.00000, 7350917
min, max, mean, gamma, argmax: 0.00000, 0.00105, 0.00012, 300.00000, 8226691
min, max, mean, gamma, argmax: 0.00000, 0.00400, 0.00016, 250.00000, 1177951
min, max, mean, gamma, argmax: 0.00000, 0.00333, 0.00032, 300.00000, 803131
min, max, mean, gamma, argmax: 0.00000, 0.00400, 0.00051, 250.00000, 3700
min, max, mean, gamma, argmax: 0.00001, 0.00400, 0.00047, 250.00000, 2882267
min, max, mean, gamma, argmax: 0.00000, 0.00400, 0.00023, 250.00000, 5540515
rawpy read the 10th file at location: ./dataset/Sony/long/00038_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 0.01000, 0.00078, 100.00000, 2598691
min, max, mean, gamma, argmax: 0.00000, 0.00400, 0.00022, 250.00000, 4653941
min, max, mean, gamma, argmax: 0.00000, 0.01000, 0.00044, 100.00000, 6102869
min, max, mean, gamma, argmax: 0.00000, 0.00400, 0.00013, 250.00000, 1458187
min, max, mean, gamma, argmax: 0.00000, 0.00400, 0.00009, 250.00000, 3426263
min, max, mean, gamma, argmax: 0.00000, 0.00333, 0.00027, 300.00000, 4586373
min, max, mean, gamma, argmax: 0.00000, 0.00333, 0.00014, 300.00000, 909101
min, max, mean, gamma, argmax: 0.00000, 0.00040, 0.00005, 300.00000, 2159003
min, max, mean, gamma, argmax: 0.00000, 0.00400, 0.00032, 250.00000, 3630407
min, max, mean, gamma, argmax: 0.00000, 0.00400, 0.00038, 250.00000, 6735543
rawpy read the 20th file at location: ./dataset/Sony/long/00072_00_30s.ARW
rawpy read the 30th file at location: ./dataset/Sony/long/00039_00_10s.ARW
rawpy read the 40th file at location: ./dataset/Sony/long/00200_00_10s.ARW
rawpy read the 50th file at location: ./dataset/Sony/long/00024_00_10s.ARW
rawpy read the 60th file at location: ./dataset/Sony/long/00084_00_30s.ARW
rawpy read the 70th file at location: ./dataset/Sony/long/00164_00_30s.ARW
rawpy read the 80th file at location: ./dataset/Sony/long/00128_00_30s.ARW
rawpy read the 90th file at location: ./dataset/Sony/long/00057_00_10s.ARW
rawpy read the 100th file at location: ./dataset/Sony/long/00059_00_10s.ARW
rawpy read the 110th file at location: ./dataset/Sony/long/00026_00_10s.ARW
rawpy read the 120th file at location: ./dataset/Sony/long/00012_00_10s.ARW
rawpy read the 130th file at location: ./dataset/Sony/long/00090_00_30s.ARW
rawpy read the 140th file at location: ./dataset/Sony/long/00114_00_30s.ARW
rawpy read the 150th file at location: ./dataset/Sony/long/00156_00_30s.ARW
rawpy read the 160th file at location: ./dataset/Sony/long/00219_00_10s.ARW
161 images loaded to CPU RAM in Time=35.320 seconds.

Moved images data to a numpy array.



BATCH_SIZE 16 ,final_epoch 4001 ,no_of_batches 10 ,ps 128 ,result_dir ./gt_Sony_CNN5_FC2_no_log_jitter/ ,len(train_ids) 161
Scaling the linear regression labels now.

Starting Training on index [ 38  93 160 106  39  43  23  99 105  78 135  70  10  29  18 135]
dataset index: [173  50 219 196  64 155  91  28 189 108 121 164  38 231 123 121]
Starting Training on gammas [300 300 100 300 250 250 250 100 100 100 100 100 100 100 250 100]
Epoch 0: at batch 1: Training dataset Loss=0.835, Batch Time=1.315
Loss vector (slice for the first 20 images)
[[1.39483619]
 [0.        ]
 [1.39483619]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.        ]
 [1.20821393]
 [0.        ]
 [1.20821393]
 [0.83479625]
 [1.39483619]
 [1.05469394]
 [0.        ]
 [0.        ]
 [2.05528069]
 [0.        ]
 [0.74752295]
 [1.39483619]
 [1.04192197]]
Epoch 1: at batch 1: Training dataset Loss=1.515, Batch Time=0.030
		Epoch 1: Epoch time = 2.055, Avg epoch time=0.283, Total Time=1.028

Loss vector (slice for the first 20 images)
[[0.96354342]
 [0.        ]
 [1.39483619]
 [0.28881848]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.8469547 ]
 [0.51593113]
 [1.20821393]
 [1.18269157]
 [1.39483619]
 [1.18269157]
 [1.04188275]
 [0.        ]
 [0.43653399]
 [1.04188275]
 [1.18269157]
 [1.04188275]
 [1.04192197]]
Epoch 2: at batch 1: Training dataset Loss=1.010, Batch Time=0.025
Loss vector (slice for the first 20 images)
[[0.96354342]
 [0.        ]
 [0.42614809]
 [0.28881848]
 [0.18960059]
 [0.42614809]
 [0.71547556]
 [0.8469547 ]
 [0.61748827]
 [0.42614809]
 [0.71547556]
 [0.46003276]
 [0.71547556]
 [0.36698532]
 [0.86986232]
 [0.43653399]
 [1.04188275]
 [0.14280914]
 [0.36698532]
 [0.42614809]]
Epoch 3: at batch 1: Training dataset Loss=0.797, Batch Time=0.028
Loss vector (slice for the first 20 images)
[[0.96354342]
 [0.2480409 ]
 [0.42614809]
 [0.58517158]
 [1.31163788]
 [0.48054475]
 [0.37340012]
 [0.54549128]
 [0.61748827]
 [0.42614809]
 [0.71547556]
 [0.46003276]
 [0.39900291]
 [0.36698532]
 [0.86986232]
 [0.58517158]
 [1.04188275]
 [0.14280914]
 [0.54549128]
 [1.45607865]]
Epoch 4: at batch 1: Training dataset Loss=0.644, Batch Time=0.026
Loss vector (slice for the first 20 images)
[[0.96354342]
 [0.2480409 ]
 [0.60342491]
 [1.16949844]
 [1.07296908]
 [0.13162367]
 [0.37340012]
 [1.03975058]
 [1.16949844]
 [0.42614809]
 [1.03975058]
 [0.46003276]
 [0.39900291]
 [1.03975058]
 [0.60342491]
 [1.03975058]
 [1.07296908]
 [0.67954051]
 [0.13162367]
 [1.45607865]]
Epoch 5: at batch 1: Training dataset Loss=0.656, Batch Time=0.021
Epoch 6: at batch 1: Training dataset Loss=0.823, Batch Time=0.023
Epoch 7: at batch 1: Training dataset Loss=0.681, Batch Time=0.023
Epoch 8: at batch 1: Training dataset Loss=0.694, Batch Time=0.028
Epoch 9: at batch 1: Training dataset Loss=0.650, Batch Time=0.024
Epoch 11: at batch 1: Training dataset Loss=0.687, Batch Time=0.025
Epoch 21: at batch 1: Training dataset Loss=0.357, Batch Time=0.022
Epoch 31: at batch 1: Training dataset Loss=0.391, Batch Time=0.025
Epoch 41: at batch 1: Training dataset Loss=0.336, Batch Time=0.029
Epoch 51: at batch 1: Training dataset Loss=0.360, Batch Time=0.019
Epoch 61: at batch 1: Training dataset Loss=0.329, Batch Time=0.028
Epoch 71: at batch 1: Training dataset Loss=0.303, Batch Time=0.022
Epoch 81: at batch 1: Training dataset Loss=0.320, Batch Time=0.020
Epoch 91: at batch 1: Training dataset Loss=0.348, Batch Time=0.024
Epoch 101: at batch 1: Training dataset Loss=0.337, Batch Time=0.026
Loss vector (slice for the first 20 images)
[[0.49938667]
 [0.31913555]
 [0.22592989]
 [0.36335599]
 [0.57866764]
 [0.41997397]
 [0.49938667]
 [0.31913555]
 [0.26015055]
 [0.25596949]
 [0.46987218]
 [0.1941312 ]
 [0.25596949]
 [0.40928927]
 [0.57866764]
 [0.49938667]
 [0.49938667]
 [0.57866764]
 [0.40928927]
 [0.36335599]]
Epoch 111: at batch 1: Training dataset Loss=0.394, Batch Time=0.020
Epoch 121: at batch 1: Training dataset Loss=0.342, Batch Time=0.021
Epoch 131: at batch 1: Training dataset Loss=0.349, Batch Time=0.025
Epoch 141: at batch 1: Training dataset Loss=0.352, Batch Time=0.024
Epoch 151: at batch 1: Training dataset Loss=0.358, Batch Time=0.020
Epoch 161: at batch 1: Training dataset Loss=0.326, Batch Time=0.026
Epoch 171: at batch 1: Training dataset Loss=0.303, Batch Time=0.028
Epoch 181: at batch 1: Training dataset Loss=0.334, Batch Time=0.023
Epoch 191: at batch 1: Training dataset Loss=0.344, Batch Time=0.031
Epoch 201: at batch 1: Training dataset Loss=0.293, Batch Time=0.028
Loss vector (slice for the first 20 images)
[[0.31643689]
 [0.21336555]
 [0.48329261]
 [0.21336555]
 [0.41830796]
 [0.3795169 ]
 [0.31269994]
 [0.17412318]
 [0.31643689]
 [0.17412318]
 [0.40816161]
 [0.37530583]
 [0.31269994]
 [0.41830796]
 [0.16664267]
 [0.48329261]
 [0.31643689]
 [0.46480048]
 [0.3125    ]
 [0.3195352 ]]
Epoch 211: at batch 1: Training dataset Loss=0.366, Batch Time=0.028
Epoch 221: at batch 1: Training dataset Loss=0.334, Batch Time=0.019
Epoch 231: at batch 1: Training dataset Loss=0.346, Batch Time=0.023
Epoch 241: at batch 1: Training dataset Loss=0.340, Batch Time=0.027
Epoch 251: at batch 1: Training dataset Loss=0.356, Batch Time=0.023
Epoch 261: at batch 1: Training dataset Loss=0.326, Batch Time=0.031
Epoch 271: at batch 1: Training dataset Loss=0.397, Batch Time=0.029
Epoch 281: at batch 1: Training dataset Loss=0.355, Batch Time=0.026
Epoch 291: at batch 1: Training dataset Loss=0.336, Batch Time=0.029
Epoch 301: at batch 1: Training dataset Loss=0.311, Batch Time=0.029
		Epoch 301: Epoch time = 119.529, Avg epoch time=0.220, Total Time=0.396

Loss vector (slice for the first 20 images)
[[0.78141516]
 [0.42442006]
 [0.42442006]
 [0.39723861]
 [0.35308614]
 [0.39723861]
 [0.78141516]
 [0.27749872]
 [0.38549048]
 [1.06010985]
 [0.27505672]
 [0.35308614]
 [0.35308614]
 [0.78141516]
 [0.5       ]
 [0.32391095]
 [0.39723861]
 [0.33983961]
 [0.4737587 ]
 [1.06010985]]
Epoch 311: at batch 1: Training dataset Loss=0.344, Batch Time=0.029
Epoch 321: at batch 1: Training dataset Loss=0.309, Batch Time=0.029
Epoch 331: at batch 1: Training dataset Loss=0.349, Batch Time=0.028
Epoch 341: at batch 1: Training dataset Loss=0.371, Batch Time=0.027
Epoch 351: at batch 1: Training dataset Loss=0.336, Batch Time=0.028
Epoch 361: at batch 1: Training dataset Loss=0.291, Batch Time=0.029
Epoch 371: at batch 1: Training dataset Loss=0.311, Batch Time=0.021
Epoch 381: at batch 1: Training dataset Loss=0.348, Batch Time=0.020
Epoch 391: at batch 1: Training dataset Loss=0.347, Batch Time=0.029
Epoch 401: at batch 1: Training dataset Loss=0.358, Batch Time=0.023
Loss vector (slice for the first 20 images)
[[0.5       ]
 [0.50167239]
 [0.375     ]
 [0.3125    ]
 [0.3125    ]
 [0.34760398]
 [0.3490091 ]
 [0.3125    ]
 [0.125     ]
 [0.50167239]
 [0.34760398]
 [0.4375    ]
 [0.375     ]
 [0.34760398]
 [0.34760398]
 [0.3125    ]
 [0.3125    ]
 [0.375     ]
 [0.25      ]
 [0.3125    ]]
Epoch 411: at batch 1: Training dataset Loss=0.386, Batch Time=0.028
Epoch 421: at batch 1: Training dataset Loss=0.371, Batch Time=0.029
Epoch 431: at batch 1: Training dataset Loss=0.324, Batch Time=0.024
Epoch 441: at batch 1: Training dataset Loss=0.368, Batch Time=0.029
Epoch 451: at batch 1: Training dataset Loss=0.290, Batch Time=0.025
Epoch 461: at batch 1: Training dataset Loss=0.335, Batch Time=0.023
Epoch 471: at batch 1: Training dataset Loss=0.314, Batch Time=0.028
Epoch 481: at batch 1: Training dataset Loss=0.328, Batch Time=0.023
Epoch 491: at batch 1: Training dataset Loss=0.381, Batch Time=0.022
Epoch 501: at batch 1: Training dataset Loss=0.297, Batch Time=0.025
Loss vector (slice for the first 20 images)
[[0.46372074]
 [0.35803801]
 [0.15135421]
 [0.16106576]
 [0.25010934]
 [0.28927049]
 [0.28814447]
 [0.24775571]
 [0.25      ]
 [0.28814447]
 [0.31529567]
 [0.50081664]
 [0.25      ]
 [0.25      ]
 [0.31529567]
 [0.375     ]
 [0.50081664]
 [0.31529567]
 [0.50081664]
 [0.22962666]]
Epoch 511: at batch 1: Training dataset Loss=0.361, Batch Time=0.022
Epoch 521: at batch 1: Training dataset Loss=0.353, Batch Time=0.020
Epoch 531: at batch 1: Training dataset Loss=0.323, Batch Time=0.028
Epoch 541: at batch 1: Training dataset Loss=0.313, Batch Time=0.028
Epoch 551: at batch 1: Training dataset Loss=0.346, Batch Time=0.029
Epoch 561: at batch 1: Training dataset Loss=0.322, Batch Time=0.023
Epoch 571: at batch 1: Training dataset Loss=0.345, Batch Time=0.028
Epoch 581: at batch 1: Training dataset Loss=0.307, Batch Time=0.023
Epoch 591: at batch 1: Training dataset Loss=0.357, Batch Time=0.020
Epoch 601: at batch 1: Training dataset Loss=0.327, Batch Time=0.024
		Epoch 601: Epoch time = 236.953, Avg epoch time=0.248, Total Time=0.394

Loss vector (slice for the first 20 images)
[[0.25      ]
 [0.17500776]
 [0.37007803]
 [0.12805945]
 [0.31359839]
 [0.4375    ]
 [0.3125    ]
 [0.4375    ]
 [0.375     ]
 [0.33786464]
 [0.43994117]
 [0.12805945]
 [0.4375    ]
 [0.43500319]
 [0.12805945]
 [0.17500776]
 [0.43500319]
 [0.33002514]
 [0.25      ]
 [0.3951655 ]]
Epoch 611: at batch 1: Training dataset Loss=0.323, Batch Time=0.022
Epoch 621: at batch 1: Training dataset Loss=0.398, Batch Time=0.022
Epoch 631: at batch 1: Training dataset Loss=0.324, Batch Time=0.019
Epoch 641: at batch 1: Training dataset Loss=0.335, Batch Time=0.026
Epoch 651: at batch 1: Training dataset Loss=0.317, Batch Time=0.022
Epoch 661: at batch 1: Training dataset Loss=0.315, Batch Time=0.025
Epoch 671: at batch 1: Training dataset Loss=0.322, Batch Time=0.032
Epoch 681: at batch 1: Training dataset Loss=0.310, Batch Time=0.025
Epoch 691: at batch 1: Training dataset Loss=0.360, Batch Time=0.022
Epoch 701: at batch 1: Training dataset Loss=0.355, Batch Time=0.023
Loss vector (slice for the first 20 images)
[[0.30545279]
 [0.24599673]
 [0.12355012]
 [0.18994786]
 [0.43958783]
 [0.24270545]
 [0.3862609 ]
 [0.3065922 ]
 [0.12355012]
 [0.47806436]
 [0.3862609 ]
 [0.27629781]
 [0.12355012]
 [0.3704963 ]
 [0.43922082]
 [0.3862609 ]
 [0.3862609 ]
 [0.3862609 ]
 [0.24270545]
 [0.27629781]]
Epoch 711: at batch 1: Training dataset Loss=0.384, Batch Time=0.027
Epoch 721: at batch 1: Training dataset Loss=0.305, Batch Time=0.023
Epoch 731: at batch 1: Training dataset Loss=0.314, Batch Time=0.031
Epoch 741: at batch 1: Training dataset Loss=0.319, Batch Time=0.027
Epoch 751: at batch 1: Training dataset Loss=0.344, Batch Time=0.023
Epoch 761: at batch 1: Training dataset Loss=0.326, Batch Time=0.024
Epoch 771: at batch 1: Training dataset Loss=0.285, Batch Time=0.019
Epoch 781: at batch 1: Training dataset Loss=0.329, Batch Time=0.026
Epoch 791: at batch 1: Training dataset Loss=0.329, Batch Time=0.022
Epoch 801: at batch 1: Training dataset Loss=0.317, Batch Time=0.029
Loss vector (slice for the first 20 images)
[[0.25      ]
 [0.25      ]
 [0.3125    ]
 [0.35810921]
 [0.4375    ]
 [0.25      ]
 [0.1875    ]
 [0.43752471]
 [0.3125    ]
 [0.1287626 ]
 [0.27099544]
 [0.25      ]
 [0.31308091]
 [0.3125    ]
 [0.2592482 ]
 [0.5625    ]
 [0.375     ]
 [0.31217483]
 [0.375     ]
 [0.31217483]]
Epoch 811: at batch 1: Training dataset Loss=0.371, Batch Time=0.025
Epoch 821: at batch 1: Training dataset Loss=0.355, Batch Time=0.027
Epoch 831: at batch 1: Training dataset Loss=0.293, Batch Time=0.021
Epoch 841: at batch 1: Training dataset Loss=0.321, Batch Time=0.028
Epoch 851: at batch 1: Training dataset Loss=0.329, Batch Time=0.028
Epoch 861: at batch 1: Training dataset Loss=0.300, Batch Time=0.020
Epoch 871: at batch 1: Training dataset Loss=0.350, Batch Time=0.022
Epoch 881: at batch 1: Training dataset Loss=0.310, Batch Time=0.026
Epoch 891: at batch 1: Training dataset Loss=0.290, Batch Time=0.022
Epoch 901: at batch 1: Training dataset Loss=0.303, Batch Time=0.023
		Epoch 901: Epoch time = 354.889, Avg epoch time=0.240, Total Time=0.393

Loss vector (slice for the first 20 images)
[[0.29898104]
 [0.28637809]
 [0.30572599]
 [0.47234783]
 [0.26032704]
 [0.02956199]
 [0.36055911]
 [0.40221974]
 [0.25169262]
 [0.30572599]
 [0.11886562]
 [0.30572599]
 [0.59913009]
 [0.02956199]
 [0.02956199]
 [0.29898104]
 [0.28637809]
 [0.485919  ]
 [0.31481317]
 [0.02956199]]
Epoch 911: at batch 1: Training dataset Loss=0.299, Batch Time=0.029
Epoch 921: at batch 1: Training dataset Loss=0.268, Batch Time=0.026
Epoch 931: at batch 1: Training dataset Loss=0.250, Batch Time=0.026
Epoch 941: at batch 1: Training dataset Loss=0.279, Batch Time=0.019
Epoch 951: at batch 1: Training dataset Loss=0.258, Batch Time=0.022
Epoch 961: at batch 1: Training dataset Loss=0.280, Batch Time=0.027
Epoch 971: at batch 1: Training dataset Loss=0.217, Batch Time=0.024
Epoch 981: at batch 1: Training dataset Loss=0.258, Batch Time=0.023
Epoch 991: at batch 1: Training dataset Loss=0.227, Batch Time=0.022
Epoch 1001: at batch 1: Training dataset Loss=0.247, Batch Time=0.022
Loss vector (slice for the first 20 images)
[[0.25253177]
 [0.30553728]
 [0.15208289]
 [0.18550432]
 [0.18719533]
 [0.25107771]
 [0.31874019]
 [0.146521  ]
 [0.30656087]
 [0.29819432]
 [0.18719533]
 [0.25253177]
 [0.25107771]
 [0.31874019]
 [0.18550432]
 [0.25253177]
 [0.31874019]
 [0.31874019]
 [0.35254824]
 [0.31327325]]
Epoch 1011: at batch 1: Training dataset Loss=0.263, Batch Time=0.023
Epoch 1021: at batch 1: Training dataset Loss=0.236, Batch Time=0.023
Epoch 1031: at batch 1: Training dataset Loss=0.199, Batch Time=0.026
Epoch 1041: at batch 1: Training dataset Loss=0.265, Batch Time=0.027
Epoch 1051: at batch 1: Training dataset Loss=0.238, Batch Time=0.023
Epoch 1061: at batch 1: Training dataset Loss=0.233, Batch Time=0.025
Epoch 1071: at batch 1: Training dataset Loss=0.218, Batch Time=0.030
Epoch 1081: at batch 1: Training dataset Loss=0.231, Batch Time=0.023
Epoch 1091: at batch 1: Training dataset Loss=0.212, Batch Time=0.031
Epoch 1101: at batch 1: Training dataset Loss=0.233, Batch Time=0.021
Loss vector (slice for the first 20 images)
[[0.17849983]
 [0.3620525 ]
 [0.19898623]
 [0.24967776]
 [0.28499055]
 [0.23116009]
 [0.23116009]
 [0.23116009]
 [0.3620525 ]
 [0.18305221]
 [0.41552907]
 [0.19898623]
 [0.29487911]
 [0.26710066]
 [0.19477928]
 [0.21462981]
 [0.3620525 ]
 [0.26759166]
 [0.26710066]
 [0.2592673 ]]
Epoch 1111: at batch 1: Training dataset Loss=0.268, Batch Time=0.030
Epoch 1121: at batch 1: Training dataset Loss=0.207, Batch Time=0.020
Epoch 1131: at batch 1: Training dataset Loss=0.234, Batch Time=0.022
Epoch 1141: at batch 1: Training dataset Loss=0.238, Batch Time=0.022
Epoch 1151: at batch 1: Training dataset Loss=0.210, Batch Time=0.024
Epoch 1161: at batch 1: Training dataset Loss=0.226, Batch Time=0.028
Epoch 1171: at batch 1: Training dataset Loss=0.226, Batch Time=0.028
Epoch 1181: at batch 1: Training dataset Loss=0.217, Batch Time=0.023
Epoch 1191: at batch 1: Training dataset Loss=0.233, Batch Time=0.025
Epoch 1201: at batch 1: Training dataset Loss=0.241, Batch Time=0.023
		Epoch 1201: Epoch time = 472.680, Avg epoch time=0.241, Total Time=0.393

Loss vector (slice for the first 20 images)
[[0.2625578 ]
 [0.1725022 ]
 [0.30956346]
 [0.20138703]
 [0.1725022 ]
 [0.16275725]
 [0.25541386]
 [0.20138703]
 [0.22515336]
 [0.26181799]
 [0.20304909]
 [0.20729172]
 [0.18855606]
 [0.1725022 ]
 [0.16275725]
 [0.18855606]
 [0.31604791]
 [0.26529238]
 [0.29307508]
 [0.1725022 ]]
Epoch 1211: at batch 1: Training dataset Loss=0.216, Batch Time=0.029
Epoch 1221: at batch 1: Training dataset Loss=0.222, Batch Time=0.029
Epoch 1231: at batch 1: Training dataset Loss=0.226, Batch Time=0.026
Epoch 1241: at batch 1: Training dataset Loss=0.214, Batch Time=0.022
Epoch 1251: at batch 1: Training dataset Loss=0.227, Batch Time=0.020
Epoch 1261: at batch 1: Training dataset Loss=0.265, Batch Time=0.020
Epoch 1271: at batch 1: Training dataset Loss=0.220, Batch Time=0.023
Epoch 1281: at batch 1: Training dataset Loss=0.199, Batch Time=0.023
Epoch 1291: at batch 1: Training dataset Loss=0.220, Batch Time=0.020
Epoch 1301: at batch 1: Training dataset Loss=0.263, Batch Time=0.031
Loss vector (slice for the first 20 images)
[[0.28312525]
 [0.29347056]
 [0.28107607]
 [0.30218399]
 [0.29487014]
 [0.13257939]
 [0.30218399]
 [0.28312525]
 [0.15782696]
 [0.30218399]
 [0.1973393 ]
 [0.22713017]
 [0.28312525]
 [0.13814801]
 [0.26395932]
 [0.18854295]
 [0.31227496]
 [0.23455553]
 [0.18678124]
 [0.15782696]]
Epoch 1311: at batch 1: Training dataset Loss=0.222, Batch Time=0.026
Epoch 1321: at batch 1: Training dataset Loss=0.230, Batch Time=0.025
Epoch 1331: at batch 1: Training dataset Loss=0.246, Batch Time=0.032
Epoch 1341: at batch 1: Training dataset Loss=0.232, Batch Time=0.028
Epoch 1351: at batch 1: Training dataset Loss=0.232, Batch Time=0.026
Epoch 1361: at batch 1: Training dataset Loss=0.213, Batch Time=0.026
Epoch 1371: at batch 1: Training dataset Loss=0.240, Batch Time=0.032
Epoch 1381: at batch 1: Training dataset Loss=0.226, Batch Time=0.028
Epoch 1391: at batch 1: Training dataset Loss=0.206, Batch Time=0.027
Epoch 1401: at batch 1: Training dataset Loss=0.256, Batch Time=0.022
Loss vector (slice for the first 20 images)
[[0.23431733]
 [0.13950998]
 [0.20004557]
 [0.085909  ]
 [0.27442223]
 [0.26302341]
 [0.47198287]
 [0.13368998]
 [0.26302341]
 [0.13368998]
 [0.13950998]
 [0.24125279]
 [0.44700301]
 [0.13368998]
 [0.33523357]
 [0.18502077]
 [0.23431733]
 [0.33483255]
 [0.13368998]
 [0.20004557]]
Epoch 1411: at batch 1: Training dataset Loss=0.226, Batch Time=0.019
Epoch 1421: at batch 1: Training dataset Loss=0.202, Batch Time=0.026
Epoch 1431: at batch 1: Training dataset Loss=0.229, Batch Time=0.029
Epoch 1441: at batch 1: Training dataset Loss=0.254, Batch Time=0.020
Epoch 1451: at batch 1: Training dataset Loss=0.246, Batch Time=0.023
Epoch 1461: at batch 1: Training dataset Loss=0.207, Batch Time=0.029
Epoch 1471: at batch 1: Training dataset Loss=0.181, Batch Time=0.025
Epoch 1481: at batch 1: Training dataset Loss=0.211, Batch Time=0.020
Epoch 1491: at batch 1: Training dataset Loss=0.247, Batch Time=0.025
Epoch 1501: at batch 1: Training dataset Loss=0.225, Batch Time=0.031
		Epoch 1501: Epoch time = 590.369, Avg epoch time=0.257, Total Time=0.393

Loss vector (slice for the first 20 images)
[[0.13879871]
 [0.12651575]
 [0.24284503]
 [0.13879871]
 [0.1819936 ]
 [0.45828173]
 [0.25183859]
 [0.24810137]
 [0.12651575]
 [0.13879871]
 [0.13879871]
 [0.12651575]
 [0.14758699]
 [0.19675817]
 [0.13879871]
 [0.30266303]
 [0.13879871]
 [0.13848487]
 [0.13879871]
 [0.25183859]]
Epoch 1511: at batch 1: Training dataset Loss=0.201, Batch Time=0.029
Epoch 1521: at batch 1: Training dataset Loss=0.210, Batch Time=0.029
Epoch 1531: at batch 1: Training dataset Loss=0.236, Batch Time=0.022
Epoch 1541: at batch 1: Training dataset Loss=0.190, Batch Time=0.030
Epoch 1551: at batch 1: Training dataset Loss=0.230, Batch Time=0.020
Epoch 1561: at batch 1: Training dataset Loss=0.226, Batch Time=0.021
Epoch 1571: at batch 1: Training dataset Loss=0.229, Batch Time=0.029
Epoch 1581: at batch 1: Training dataset Loss=0.205, Batch Time=0.026
Epoch 1591: at batch 1: Training dataset Loss=0.235, Batch Time=0.028
Epoch 1601: at batch 1: Training dataset Loss=0.224, Batch Time=0.031
Loss vector (slice for the first 20 images)
[[0.25133544]
 [0.17527366]
 [0.16130532]
 [0.22572938]
 [0.34388196]
 [0.24628291]
 [0.25682533]
 [0.18380547]
 [0.25682533]
 [0.25682533]
 [0.22572938]
 [0.24882272]
 [0.33391559]
 [0.33391559]
 [0.22572938]
 [0.28967714]
 [0.27222186]
 [0.21554631]
 [0.25133544]
 [0.17527366]]
Epoch 1611: at batch 1: Training dataset Loss=0.241, Batch Time=0.031
Epoch 1621: at batch 1: Training dataset Loss=0.209, Batch Time=0.028
Epoch 1631: at batch 1: Training dataset Loss=0.214, Batch Time=0.028
Epoch 1641: at batch 1: Training dataset Loss=0.211, Batch Time=0.019
Epoch 1651: at batch 1: Training dataset Loss=0.211, Batch Time=0.030
Epoch 1661: at batch 1: Training dataset Loss=0.220, Batch Time=0.027
Epoch 1671: at batch 1: Training dataset Loss=0.242, Batch Time=0.028
Epoch 1681: at batch 1: Training dataset Loss=0.238, Batch Time=0.031
Epoch 1691: at batch 1: Training dataset Loss=0.222, Batch Time=0.029
Epoch 1701: at batch 1: Training dataset Loss=0.261, Batch Time=0.021
Loss vector (slice for the first 20 images)
[[0.19141205]
 [0.23244049]
 [0.23244049]
 [0.24352473]
 [0.31423008]
 [0.25667921]
 [0.29091755]
 [0.247071  ]
 [0.29091755]
 [0.23176514]
 [0.2652781 ]
 [0.2652781 ]
 [0.42335826]
 [0.25667921]
 [0.27589211]
 [0.23176514]
 [0.2017895 ]
 [0.1710394 ]
 [0.29091755]
 [0.25667921]]
Epoch 1711: at batch 1: Training dataset Loss=0.231, Batch Time=0.021
Epoch 1721: at batch 1: Training dataset Loss=0.234, Batch Time=0.026
Epoch 1731: at batch 1: Training dataset Loss=0.245, Batch Time=0.029
Epoch 1741: at batch 1: Training dataset Loss=0.255, Batch Time=0.019
Epoch 1751: at batch 1: Training dataset Loss=0.223, Batch Time=0.021
Epoch 1761: at batch 1: Training dataset Loss=0.205, Batch Time=0.020
Epoch 1771: at batch 1: Training dataset Loss=0.235, Batch Time=0.032
Epoch 1781: at batch 1: Training dataset Loss=0.214, Batch Time=0.023
Epoch 1791: at batch 1: Training dataset Loss=0.243, Batch Time=0.023
Epoch 1801: at batch 1: Training dataset Loss=0.203, Batch Time=0.025
		Epoch 1801: Epoch time = 707.802, Avg epoch time=0.245, Total Time=0.393

Loss vector (slice for the first 20 images)
[[0.15850994]
 [0.15850994]
 [0.13080893]
 [0.13080893]
 [0.21437591]
 [0.13080893]
 [0.21248415]
 [0.21433729]
 [0.21433729]
 [0.13080893]
 [0.21084851]
 [0.21248415]
 [0.21219128]
 [0.21437591]
 [0.14182693]
 [0.21433729]
 [0.26357067]
 [0.21474171]
 [0.21433729]
 [0.33037212]]
Epoch 1811: at batch 1: Training dataset Loss=0.218, Batch Time=0.029
Epoch 1821: at batch 1: Training dataset Loss=0.208, Batch Time=0.032
Epoch 1831: at batch 1: Training dataset Loss=0.229, Batch Time=0.029
Epoch 1841: at batch 1: Training dataset Loss=0.215, Batch Time=0.025
Epoch 1851: at batch 1: Training dataset Loss=0.217, Batch Time=0.031
Epoch 1861: at batch 1: Training dataset Loss=0.251, Batch Time=0.022
Epoch 1871: at batch 1: Training dataset Loss=0.229, Batch Time=0.030
Epoch 1881: at batch 1: Training dataset Loss=0.215, Batch Time=0.027
Epoch 1891: at batch 1: Training dataset Loss=0.233, Batch Time=0.029
Epoch 1901: at batch 1: Training dataset Loss=0.234, Batch Time=0.023
Loss vector (slice for the first 20 images)
[[0.314794  ]
 [0.17888223]
 [0.24264865]
 [0.2510922 ]
 [0.24264865]
 [0.33795229]
 [0.16258124]
 [0.18417434]
 [0.314794  ]
 [0.26698795]
 [0.18417434]
 [0.24264865]
 [0.18417434]
 [0.24264865]
 [0.25194818]
 [0.33795229]
 [0.33795229]
 [0.24520662]
 [0.10744045]
 [0.18417434]]
Epoch 1911: at batch 1: Training dataset Loss=0.227, Batch Time=0.030
Epoch 1921: at batch 1: Training dataset Loss=0.234, Batch Time=0.025
Epoch 1931: at batch 1: Training dataset Loss=0.211, Batch Time=0.024
Epoch 1941: at batch 1: Training dataset Loss=0.221, Batch Time=0.027
Epoch 1951: at batch 1: Training dataset Loss=0.230, Batch Time=0.022
Epoch 1961: at batch 1: Training dataset Loss=0.213, Batch Time=0.022
Epoch 1971: at batch 1: Training dataset Loss=0.213, Batch Time=0.029
Epoch 1981: at batch 1: Training dataset Loss=0.211, Batch Time=0.027
Epoch 1991: at batch 1: Training dataset Loss=0.219, Batch Time=0.029
Epoch 2001: at batch 1: Training dataset Loss=0.233, Batch Time=0.030
Loss vector (slice for the first 20 images)
[[0.32963991]
 [0.20384777]
 [0.16571228]
 [0.17533727]
 [0.20787919]
 [0.16844781]
 [0.38078994]
 [0.20384777]
 [0.16571228]
 [0.20787919]
 [0.24457058]
 [0.15534385]
 [0.14710891]
 [0.25570264]
 [0.21031024]
 [0.3319326 ]
 [0.15534385]
 [0.32963991]
 [0.23020753]
 [0.3177284 ]]
Epoch 2011: at batch 1: Training dataset Loss=0.245, Batch Time=0.023
Epoch 2021: at batch 1: Training dataset Loss=0.219, Batch Time=0.026
Epoch 2031: at batch 1: Training dataset Loss=0.210, Batch Time=0.021
Epoch 2041: at batch 1: Training dataset Loss=0.229, Batch Time=0.023
Epoch 2051: at batch 1: Training dataset Loss=0.223, Batch Time=0.029
Epoch 2061: at batch 1: Training dataset Loss=0.247, Batch Time=0.021
Epoch 2071: at batch 1: Training dataset Loss=0.216, Batch Time=0.027
Epoch 2081: at batch 1: Training dataset Loss=0.210, Batch Time=0.022
Epoch 2091: at batch 1: Training dataset Loss=0.211, Batch Time=0.022
Epoch 2101: at batch 1: Training dataset Loss=0.214, Batch Time=0.023
		Epoch 2101: Epoch time = 825.418, Avg epoch time=0.233, Total Time=0.393

Loss vector (slice for the first 20 images)
[[0.29384732]
 [0.16300777]
 [0.16198754]
 [0.29384732]
 [0.27545542]
 [0.18621019]
 [0.27545542]
 [0.18362604]
 [0.15117301]
 [0.12210841]
 [0.16979113]
 [0.28043413]
 [0.2152707 ]
 [0.16979113]
 [0.18127471]
 [0.29384732]
 [0.18362604]
 [0.28344217]
 [0.21977738]
 [0.12210841]]
Epoch 2111: at batch 1: Training dataset Loss=0.203, Batch Time=0.022
Epoch 2121: at batch 1: Training dataset Loss=0.218, Batch Time=0.022
Epoch 2131: at batch 1: Training dataset Loss=0.225, Batch Time=0.031
Epoch 2141: at batch 1: Training dataset Loss=0.210, Batch Time=0.028
Epoch 2151: at batch 1: Training dataset Loss=0.207, Batch Time=0.024
Epoch 2161: at batch 1: Training dataset Loss=0.204, Batch Time=0.020
Epoch 2171: at batch 1: Training dataset Loss=0.255, Batch Time=0.022
Epoch 2181: at batch 1: Training dataset Loss=0.250, Batch Time=0.028
Epoch 2191: at batch 1: Training dataset Loss=0.232, Batch Time=0.026
Epoch 2201: at batch 1: Training dataset Loss=0.243, Batch Time=0.022
Loss vector (slice for the first 20 images)
[[0.29691404]
 [0.28752106]
 [0.22309336]
 [0.281775  ]
 [0.24867047]
 [0.29940075]
 [0.17649907]
 [0.17759341]
 [0.13390204]
 [0.13390204]
 [0.22065675]
 [0.29940075]
 [0.23198202]
 [0.22065675]
 [0.17759341]
 [0.17759341]
 [0.41929209]
 [0.29068232]
 [0.29691404]
 [0.1600586 ]]
Epoch 2211: at batch 1: Training dataset Loss=0.253, Batch Time=0.027
Epoch 2221: at batch 1: Training dataset Loss=0.226, Batch Time=0.029
Epoch 2231: at batch 1: Training dataset Loss=0.236, Batch Time=0.023
Epoch 2241: at batch 1: Training dataset Loss=0.203, Batch Time=0.024
Epoch 2251: at batch 1: Training dataset Loss=0.201, Batch Time=0.023
Epoch 2261: at batch 1: Training dataset Loss=0.217, Batch Time=0.026
Epoch 2271: at batch 1: Training dataset Loss=0.210, Batch Time=0.028
Epoch 2281: at batch 1: Training dataset Loss=0.228, Batch Time=0.030
Epoch 2291: at batch 1: Training dataset Loss=0.224, Batch Time=0.021
Epoch 2301: at batch 1: Training dataset Loss=0.211, Batch Time=0.027
Loss vector (slice for the first 20 images)
[[0.13059229]
 [0.39231798]
 [0.28242111]
 [0.15990721]
 [0.25755644]
 [0.16302463]
 [0.28242111]
 [0.13783607]
 [0.2199441 ]
 [0.1996955 ]
 [0.16302463]
 [0.15990721]
 [0.25755644]
 [0.28242111]
 [0.28242111]
 [0.26922947]
 [0.25834215]
 [0.39231798]
 [0.22152312]
 [0.19275178]]
Epoch 2311: at batch 1: Training dataset Loss=0.212, Batch Time=0.028
Epoch 2321: at batch 1: Training dataset Loss=0.241, Batch Time=0.026
Epoch 2331: at batch 1: Training dataset Loss=0.230, Batch Time=0.024
Epoch 2341: at batch 1: Training dataset Loss=0.214, Batch Time=0.024
Epoch 2351: at batch 1: Training dataset Loss=0.208, Batch Time=0.020
Epoch 2361: at batch 1: Training dataset Loss=0.197, Batch Time=0.029
Epoch 2371: at batch 1: Training dataset Loss=0.197, Batch Time=0.029
Epoch 2381: at batch 1: Training dataset Loss=0.199, Batch Time=0.025
Epoch 2391: at batch 1: Training dataset Loss=0.217, Batch Time=0.026
Epoch 2401: at batch 1: Training dataset Loss=0.208, Batch Time=0.022
		Epoch 2401: Epoch time = 943.134, Avg epoch time=0.243, Total Time=0.393

Loss vector (slice for the first 20 images)
[[0.32674891]
 [0.17579675]
 [0.25581163]
 [0.2140692 ]
 [0.24351573]
 [0.19028991]
 [0.32674891]
 [0.23303552]
 [0.24351573]
 [0.18092373]
 [0.20537251]
 [0.29222631]
 [0.29222631]
 [0.18589166]
 [0.25581163]
 [0.18471175]
 [0.24351573]
 [0.18471175]
 [0.24351573]
 [0.21561241]]
Epoch 2411: at batch 1: Training dataset Loss=0.196, Batch Time=0.028
Epoch 2421: at batch 1: Training dataset Loss=0.215, Batch Time=0.022
Epoch 2431: at batch 1: Training dataset Loss=0.243, Batch Time=0.028
Epoch 2441: at batch 1: Training dataset Loss=0.205, Batch Time=0.025
Epoch 2451: at batch 1: Training dataset Loss=0.199, Batch Time=0.025
Epoch 2461: at batch 1: Training dataset Loss=0.232, Batch Time=0.028
Epoch 2471: at batch 1: Training dataset Loss=0.221, Batch Time=0.029
Epoch 2481: at batch 1: Training dataset Loss=0.227, Batch Time=0.029
Epoch 2491: at batch 1: Training dataset Loss=0.218, Batch Time=0.030
Epoch 2501: at batch 1: Training dataset Loss=0.215, Batch Time=0.028
Loss vector (slice for the first 20 images)
[[0.1780034 ]
 [0.1780034 ]
 [0.27961531]
 [0.21213862]
 [0.24830674]
 [0.24830674]
 [0.23929611]
 [0.21855058]
 [0.21504599]
 [0.17749856]
 [0.21504599]
 [0.08101869]
 [0.21504599]
 [0.34982631]
 [0.1780034 ]
 [0.21855058]
 [0.19466515]
 [0.17749856]
 [0.20812866]
 [0.34238586]]
Epoch 2511: at batch 1: Training dataset Loss=0.234, Batch Time=0.029
Epoch 2521: at batch 1: Training dataset Loss=0.219, Batch Time=0.023
Epoch 2531: at batch 1: Training dataset Loss=0.218, Batch Time=0.024
Epoch 2541: at batch 1: Training dataset Loss=0.208, Batch Time=0.026
Epoch 2551: at batch 1: Training dataset Loss=0.204, Batch Time=0.029
Epoch 2561: at batch 1: Training dataset Loss=0.207, Batch Time=0.025
Epoch 2571: at batch 1: Training dataset Loss=0.238, Batch Time=0.023
Epoch 2581: at batch 1: Training dataset Loss=0.188, Batch Time=0.020
Epoch 2591: at batch 1: Training dataset Loss=0.193, Batch Time=0.023
Epoch 2601: at batch 1: Training dataset Loss=0.222, Batch Time=0.022
Loss vector (slice for the first 20 images)
[[0.12460385]
 [0.27674243]
 [0.28872514]
 [0.332717  ]
 [0.1696094 ]
 [0.332717  ]
 [0.332717  ]
 [0.25536519]
 [0.31908813]
 [0.28872514]
 [0.16591975]
 [0.18374239]
 [0.19520396]
 [0.19617312]
 [0.332717  ]
 [0.21972193]
 [0.19617312]
 [0.20432831]
 [0.21168101]
 [0.18374239]]
Epoch 2611: at batch 1: Training dataset Loss=0.206, Batch Time=0.032
Epoch 2621: at batch 1: Training dataset Loss=0.197, Batch Time=0.023
Epoch 2631: at batch 1: Training dataset Loss=0.238, Batch Time=0.025
Epoch 2641: at batch 1: Training dataset Loss=0.204, Batch Time=0.026
Epoch 2651: at batch 1: Training dataset Loss=0.205, Batch Time=0.030
Epoch 2661: at batch 1: Training dataset Loss=0.218, Batch Time=0.023
Epoch 2671: at batch 1: Training dataset Loss=0.211, Batch Time=0.023
Epoch 2681: at batch 1: Training dataset Loss=0.221, Batch Time=0.028
Epoch 2691: at batch 1: Training dataset Loss=0.226, Batch Time=0.026
Epoch 2701: at batch 1: Training dataset Loss=0.213, Batch Time=0.023
		Epoch 2701: Epoch time = 1061.391, Avg epoch time=0.247, Total Time=0.393

Loss vector (slice for the first 20 images)
[[0.14728776]
 [0.19849963]
 [0.18715805]
 [0.230858  ]
 [0.20212236]
 [0.20067659]
 [0.20600152]
 [0.19525278]
 [0.20067659]
 [0.1455234 ]
 [0.20875987]
 [0.24853854]
 [0.19098547]
 [0.35642821]
 [0.31255734]
 [0.19098547]
 [0.09304057]
 [0.18715805]
 [0.20067659]
 [0.21206433]]
Epoch 2711: at batch 1: Training dataset Loss=0.201, Batch Time=0.028
Epoch 2721: at batch 1: Training dataset Loss=0.204, Batch Time=0.020
Epoch 2731: at batch 1: Training dataset Loss=0.230, Batch Time=0.022
Epoch 2741: at batch 1: Training dataset Loss=0.221, Batch Time=0.026
Epoch 2751: at batch 1: Training dataset Loss=0.243, Batch Time=0.028
Epoch 2761: at batch 1: Training dataset Loss=0.221, Batch Time=0.026
Epoch 2771: at batch 1: Training dataset Loss=0.199, Batch Time=0.026
Epoch 2781: at batch 1: Training dataset Loss=0.208, Batch Time=0.022
Epoch 2791: at batch 1: Training dataset Loss=0.224, Batch Time=0.020
Epoch 2801: at batch 1: Training dataset Loss=0.190, Batch Time=0.032
Loss vector (slice for the first 20 images)
[[0.24047318]
 [0.15790617]
 [0.11371346]
 [0.20410207]
 [0.2298834 ]
 [0.2898559 ]
 [0.25361997]
 [0.16788137]
 [0.2298834 ]
 [0.24047318]
 [0.09891706]
 [0.17734385]
 [0.1801504 ]
 [0.1801504 ]
 [0.25361997]
 [0.2298834 ]
 [0.2298834 ]
 [0.16086316]
 [0.34076816]
 [0.23027405]]
Epoch 2811: at batch 1: Training dataset Loss=0.209, Batch Time=0.023
Epoch 2821: at batch 1: Training dataset Loss=0.222, Batch Time=0.030
Epoch 2831: at batch 1: Training dataset Loss=0.199, Batch Time=0.028
Epoch 2841: at batch 1: Training dataset Loss=0.213, Batch Time=0.022
Epoch 2851: at batch 1: Training dataset Loss=0.217, Batch Time=0.027
Epoch 2861: at batch 1: Training dataset Loss=0.231, Batch Time=0.022
Epoch 2871: at batch 1: Training dataset Loss=0.227, Batch Time=0.026
Epoch 2881: at batch 1: Training dataset Loss=0.193, Batch Time=0.025
Epoch 2891: at batch 1: Training dataset Loss=0.201, Batch Time=0.029
Epoch 2901: at batch 1: Training dataset Loss=0.191, Batch Time=0.028
Loss vector (slice for the first 20 images)
[[0.22136445]
 [0.28402799]
 [0.15565872]
 [0.22130163]
 [0.18251567]
 [0.33786869]
 [0.33710784]
 [0.18251567]
 [0.22130163]
 [0.12542391]
 [0.12542391]
 [0.33710784]
 [0.26222947]
 [0.26029238]
 [0.21891636]
 [0.33710784]
 [0.33786869]
 [0.23139875]
 [0.28712791]
 [0.19343114]]
Epoch 2911: at batch 1: Training dataset Loss=0.212, Batch Time=0.030
Epoch 2921: at batch 1: Training dataset Loss=0.209, Batch Time=0.025
Epoch 2931: at batch 1: Training dataset Loss=0.201, Batch Time=0.022
Epoch 2941: at batch 1: Training dataset Loss=0.209, Batch Time=0.028
Epoch 2951: at batch 1: Training dataset Loss=0.228, Batch Time=0.022
Epoch 2961: at batch 1: Training dataset Loss=0.228, Batch Time=0.026
Epoch 2971: at batch 1: Training dataset Loss=0.232, Batch Time=0.021
Epoch 2981: at batch 1: Training dataset Loss=0.204, Batch Time=0.028
Epoch 2991: at batch 1: Training dataset Loss=0.204, Batch Time=0.031
Epoch 3001: at batch 1: Training dataset Loss=0.219, Batch Time=0.023
		Epoch 3001: Epoch time = 1179.214, Avg epoch time=0.253, Total Time=0.393

Loss vector (slice for the first 20 images)
[[0.25713304]
 [0.20958817]
 [0.25430706]
 [0.214761  ]
 [0.28948033]
 [0.15639667]
 [0.18902119]
 [0.18902119]
 [0.18902119]
 [0.1551113 ]
 [0.25430706]
 [0.18902119]
 [0.23653626]
 [0.15639667]
 [0.18902119]
 [0.19116974]
 [0.1706447 ]
 [0.30861706]
 [0.15639667]
 [0.13231553]]
Epoch 3011: at batch 1: Training dataset Loss=0.205, Batch Time=0.026
Epoch 3021: at batch 1: Training dataset Loss=0.226, Batch Time=0.020
Epoch 3031: at batch 1: Training dataset Loss=0.231, Batch Time=0.023
Epoch 3041: at batch 1: Training dataset Loss=0.250, Batch Time=0.025
Epoch 3051: at batch 1: Training dataset Loss=0.214, Batch Time=0.024
Epoch 3061: at batch 1: Training dataset Loss=0.208, Batch Time=0.023
Epoch 3071: at batch 1: Training dataset Loss=0.208, Batch Time=0.025
Epoch 3081: at batch 1: Training dataset Loss=0.199, Batch Time=0.032
Epoch 3091: at batch 1: Training dataset Loss=0.199, Batch Time=0.021
Epoch 3101: at batch 1: Training dataset Loss=0.214, Batch Time=0.029
Loss vector (slice for the first 20 images)
[[0.17864376]
 [0.14716117]
 [0.09902526]
 [0.09902526]
 [0.26308924]
 [0.18274811]
 [0.27891093]
 [0.1424585 ]
 [0.14716117]
 [0.15866463]
 [0.22452851]
 [0.22424576]
 [0.18524118]
 [0.17940722]
 [0.17864376]
 [0.15866463]
 [0.23508322]
 [0.22424576]
 [0.1424585 ]
 [0.25215855]]
Epoch 3111: at batch 1: Training dataset Loss=0.201, Batch Time=0.020
Epoch 3121: at batch 1: Training dataset Loss=0.196, Batch Time=0.031
Epoch 3131: at batch 1: Training dataset Loss=0.200, Batch Time=0.022
Epoch 3141: at batch 1: Training dataset Loss=0.236, Batch Time=0.028
Epoch 3151: at batch 1: Training dataset Loss=0.194, Batch Time=0.023
Epoch 3161: at batch 1: Training dataset Loss=0.217, Batch Time=0.031
Epoch 3171: at batch 1: Training dataset Loss=0.230, Batch Time=0.022
Epoch 3181: at batch 1: Training dataset Loss=0.220, Batch Time=0.026
Epoch 3191: at batch 1: Training dataset Loss=0.212, Batch Time=0.024
Epoch 3201: at batch 1: Training dataset Loss=0.220, Batch Time=0.021
Loss vector (slice for the first 20 images)
[[0.22120897]
 [0.19548367]
 [0.14845711]
 [0.14845711]
 [0.2153122 ]
 [0.24750394]
 [0.20867485]
 [0.18083172]
 [0.20867485]
 [0.19463381]
 [0.2153122 ]
 [0.2153122 ]
 [0.18083172]
 [0.12559536]
 [0.2153122 ]
 [0.21681984]
 [0.24141538]
 [0.14845711]
 [0.23989305]
 [0.20532979]]
Epoch 3211: at batch 1: Training dataset Loss=0.194, Batch Time=0.028
Epoch 3221: at batch 1: Training dataset Loss=0.204, Batch Time=0.025
Epoch 3231: at batch 1: Training dataset Loss=0.198, Batch Time=0.029
Epoch 3241: at batch 1: Training dataset Loss=0.198, Batch Time=0.028
Epoch 3251: at batch 1: Training dataset Loss=0.210, Batch Time=0.023
Epoch 3261: at batch 1: Training dataset Loss=0.210, Batch Time=0.031
Epoch 3271: at batch 1: Training dataset Loss=0.211, Batch Time=0.031
Epoch 3281: at batch 1: Training dataset Loss=0.208, Batch Time=0.025
Epoch 3291: at batch 1: Training dataset Loss=0.198, Batch Time=0.026
Epoch 3301: at batch 1: Training dataset Loss=0.221, Batch Time=0.022
		Epoch 3301: Epoch time = 1296.878, Avg epoch time=0.246, Total Time=0.393

Loss vector (slice for the first 20 images)
[[0.16638494]
 [0.11505955]
 [0.13689792]
 [0.36497036]
 [0.26289684]
 [0.16416174]
 [0.23007295]
 [0.15665436]
 [0.22532642]
 [0.36497036]
 [0.26289684]
 [0.17730649]
 [0.1709488 ]
 [0.22532642]
 [0.11505955]
 [0.28547269]
 [0.23793653]
 [0.15665436]
 [0.24141213]
 [0.13689792]]
Epoch 3311: at batch 1: Training dataset Loss=0.229, Batch Time=0.028
Epoch 3321: at batch 1: Training dataset Loss=0.211, Batch Time=0.025
Epoch 3331: at batch 1: Training dataset Loss=0.213, Batch Time=0.031
Epoch 3341: at batch 1: Training dataset Loss=0.219, Batch Time=0.025
Epoch 3351: at batch 1: Training dataset Loss=0.186, Batch Time=0.023
Epoch 3361: at batch 1: Training dataset Loss=0.197, Batch Time=0.023
Epoch 3371: at batch 1: Training dataset Loss=0.223, Batch Time=0.029
Epoch 3381: at batch 1: Training dataset Loss=0.217, Batch Time=0.021
Epoch 3391: at batch 1: Training dataset Loss=0.202, Batch Time=0.031
Epoch 3401: at batch 1: Training dataset Loss=0.214, Batch Time=0.023
Loss vector (slice for the first 20 images)
[[0.16429274]
 [0.16183297]
 [0.21382448]
 [0.26047373]
 [0.15022925]
 [0.22398193]
 [0.26761961]
 [0.22476642]
 [0.15806292]
 [0.1707163 ]
 [0.25960785]
 [0.27749535]
 [0.21382448]
 [0.13308021]
 [0.1707163 ]
 [0.28197491]
 [0.25960785]
 [0.13308021]
 [0.14476398]
 [0.141123  ]]
Epoch 3411: at batch 1: Training dataset Loss=0.215, Batch Time=0.022
Epoch 3421: at batch 1: Training dataset Loss=0.217, Batch Time=0.026
Epoch 3431: at batch 1: Training dataset Loss=0.217, Batch Time=0.023
Epoch 3441: at batch 1: Training dataset Loss=0.239, Batch Time=0.027
Epoch 3451: at batch 1: Training dataset Loss=0.183, Batch Time=0.025
Epoch 3461: at batch 1: Training dataset Loss=0.201, Batch Time=0.029
Epoch 3471: at batch 1: Training dataset Loss=0.224, Batch Time=0.031
Epoch 3481: at batch 1: Training dataset Loss=0.193, Batch Time=0.021
Epoch 3491: at batch 1: Training dataset Loss=0.211, Batch Time=0.028
Epoch 3501: at batch 1: Training dataset Loss=0.216, Batch Time=0.025
Loss vector (slice for the first 20 images)
[[0.18054631]
 [0.21923488]
 [0.14785367]
 [0.16209944]
 [0.22719982]
 [0.18198183]
 [0.21923488]
 [0.11885694]
 [0.2228072 ]
 [0.23969223]
 [0.27935413]
 [0.21923488]
 [0.16683935]
 [0.14785367]
 [0.22719982]
 [0.20258045]
 [0.15327153]
 [0.16209944]
 [0.15237084]
 [0.15327153]]
Epoch 3511: at batch 1: Training dataset Loss=0.213, Batch Time=0.028
Epoch 3521: at batch 1: Training dataset Loss=0.216, Batch Time=0.020
Epoch 3531: at batch 1: Training dataset Loss=0.182, Batch Time=0.025
Epoch 3541: at batch 1: Training dataset Loss=0.231, Batch Time=0.026
Epoch 3551: at batch 1: Training dataset Loss=0.199, Batch Time=0.023
Epoch 3561: at batch 1: Training dataset Loss=0.201, Batch Time=0.027
Epoch 3571: at batch 1: Training dataset Loss=0.207, Batch Time=0.027
Epoch 3581: at batch 1: Training dataset Loss=0.200, Batch Time=0.028
Epoch 3591: at batch 1: Training dataset Loss=0.184, Batch Time=0.028
Epoch 3601: at batch 1: Training dataset Loss=0.199, Batch Time=0.025
		Epoch 3601: Epoch time = 1415.041, Avg epoch time=0.250, Total Time=0.393

Loss vector (slice for the first 20 images)
[[0.12323528]
 [0.13977242]
 [0.15062152]
 [0.09566698]
 [0.2213107 ]
 [0.12476057]
 [0.12466636]
 [0.12476057]
 [0.19595298]
 [0.15062152]
 [0.09566698]
 [0.12276269]
 [0.212955  ]
 [0.22767495]
 [0.13977242]
 [0.16662131]
 [0.15062152]
 [0.3793363 ]
 [0.13977242]
 [0.12476057]]
Epoch 3611: at batch 1: Training dataset Loss=0.195, Batch Time=0.024
Epoch 3621: at batch 1: Training dataset Loss=0.190, Batch Time=0.021
Epoch 3631: at batch 1: Training dataset Loss=0.232, Batch Time=0.022
Epoch 3641: at batch 1: Training dataset Loss=0.204, Batch Time=0.031
Epoch 3651: at batch 1: Training dataset Loss=0.208, Batch Time=0.027
Epoch 3661: at batch 1: Training dataset Loss=0.215, Batch Time=0.028
Epoch 3671: at batch 1: Training dataset Loss=0.196, Batch Time=0.023
Epoch 3681: at batch 1: Training dataset Loss=0.239, Batch Time=0.031
Epoch 3691: at batch 1: Training dataset Loss=0.225, Batch Time=0.029
Epoch 3701: at batch 1: Training dataset Loss=0.191, Batch Time=0.023
Loss vector (slice for the first 20 images)
[[0.16165876]
 [0.14719167]
 [0.22481726]
 [0.22639182]
 [0.37300694]
 [0.14719167]
 [0.2912575 ]
 [0.16220678]
 [0.1339021 ]
 [0.20997474]
 [0.20997474]
 [0.14882015]
 [0.35427007]
 [0.32725015]
 [0.14719167]
 [0.37612677]
 [0.11794824]
 [0.11863202]
 [0.35427007]
 [0.32214803]]
Epoch 3711: at batch 1: Training dataset Loss=0.221, Batch Time=0.020
Epoch 3721: at batch 1: Training dataset Loss=0.195, Batch Time=0.023
Epoch 3731: at batch 1: Training dataset Loss=0.213, Batch Time=0.026
Epoch 3741: at batch 1: Training dataset Loss=0.203, Batch Time=0.028
Epoch 3751: at batch 1: Training dataset Loss=0.199, Batch Time=0.029
Epoch 3761: at batch 1: Training dataset Loss=0.215, Batch Time=0.031
Epoch 3771: at batch 1: Training dataset Loss=0.211, Batch Time=0.029
Epoch 3781: at batch 1: Training dataset Loss=0.208, Batch Time=0.022
Epoch 3791: at batch 1: Training dataset Loss=0.201, Batch Time=0.023
Epoch 3801: at batch 1: Training dataset Loss=0.201, Batch Time=0.030
Loss vector (slice for the first 20 images)
[[0.12719613]
 [0.12719613]
 [0.20203918]
 [0.18527274]
 [0.20312423]
 [0.16481307]
 [0.20140699]
 [0.20313986]
 [0.2063407 ]
 [0.16481307]
 [0.20203918]
 [0.23335424]
 [0.13912556]
 [0.12719613]
 [0.18527274]
 [0.13912556]
 [0.16481307]
 [0.28409031]
 [0.2023399 ]
 [0.20203918]]
Epoch 3811: at batch 1: Training dataset Loss=0.210, Batch Time=0.030
Epoch 3821: at batch 1: Training dataset Loss=0.220, Batch Time=0.029
Epoch 3831: at batch 1: Training dataset Loss=0.206, Batch Time=0.028
Epoch 3841: at batch 1: Training dataset Loss=0.206, Batch Time=0.021
Epoch 3851: at batch 1: Training dataset Loss=0.228, Batch Time=0.030
Epoch 3861: at batch 1: Training dataset Loss=0.194, Batch Time=0.029
Epoch 3871: at batch 1: Training dataset Loss=0.215, Batch Time=0.025
Epoch 3881: at batch 1: Training dataset Loss=0.196, Batch Time=0.020
Epoch 3891: at batch 1: Training dataset Loss=0.198, Batch Time=0.023
Epoch 3901: at batch 1: Training dataset Loss=0.211, Batch Time=0.020
		Epoch 3901: Epoch time = 1532.665, Avg epoch time=0.220, Total Time=0.393

Loss vector (slice for the first 20 images)
[[0.25856015]
 [0.236985  ]
 [0.2266345 ]
 [0.17175193]
 [0.22231448]
 [0.22231448]
 [0.25856015]
 [0.2266345 ]
 [0.21430549]
 [0.19725476]
 [0.21430549]
 [0.12309898]
 [0.13729897]
 [0.13094546]
 [0.22231448]
 [0.19305408]
 [0.30279082]
 [0.22231448]
 [0.13094546]
 [0.21430549]]
Epoch 3911: at batch 1: Training dataset Loss=0.192, Batch Time=0.027
Epoch 3921: at batch 1: Training dataset Loss=0.208, Batch Time=0.029
Epoch 3931: at batch 1: Training dataset Loss=0.200, Batch Time=0.030
Epoch 3941: at batch 1: Training dataset Loss=0.220, Batch Time=0.029
Epoch 3951: at batch 1: Training dataset Loss=0.203, Batch Time=0.022
Epoch 3961: at batch 1: Training dataset Loss=0.207, Batch Time=0.023
Epoch 3971: at batch 1: Training dataset Loss=0.217, Batch Time=0.025
Epoch 3981: at batch 1: Training dataset Loss=0.226, Batch Time=0.024
Epoch 3991: at batch 1: Training dataset Loss=0.217, Batch Time=0.028

















(base) [ir967@log-3 gt_Sony_CNN5_FC2_no_log_jitter]$ vi 4000/intermediate_results.txt 
(base) [ir967@log-3 gt_Sony_CNN5_FC2_no_log_jitter]$ cd ..
(base) [ir967@log-3 Learning-to-See-in-the-Dark]$ vi CNN5_FC2_no_log.py 
(base) [ir967@log-3 Learning-to-See-in-the-Dark]$ srun -t0:30:00 --mem=15640MB --gres=gpu:1 --pty /bin/bash
srun: job 403475 queued and waiting for resources
srun: job 403475 has been allocated resources
(base) [ir967@gr014 Learning-to-See-in-the-Dark]$ conda activate sid2
(sid2) [ir967@gr014 Learning-to-See-in-the-Dark]$ python CNN5_FC2_no_log.py 




Current date and time : 
2020-12-13 01:56:13
Found 161 images to train with

Training on 161 images only

2020-12-13 01:56:13.333829: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-13 01:56:13.471740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-13 01:56:13.471775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-13 01:56:13.763066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-13 01:56:13.763110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-13 01:56:13.763137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-13 01:56:13.763254: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)

Loaded ./gt_Sony_CNN5_FC2_no_log_jitter/model.ckpt

last epoch of previous run: 4000
rawpy read the 0th file at location: ./dataset/Sony/long/00018_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 0.00211, 0.00030, 300.00000, 242663
min, max, mean, gamma, argmax: 0.00001, 0.00333, 0.00082, 300.00000, 3485919
min, max, mean, gamma, argmax: 0.00000, 0.00400, 0.00060, 250.00000, 1
min, max, mean, gamma, argmax: 0.00000, 0.00401, 0.00021, 100.00000, 7350917
min, max, mean, gamma, argmax: 0.00000, 0.00125, 0.00015, 250.00000, 8226691
min, max, mean, gamma, argmax: 0.00000, 0.01000, 0.00040, 100.00000, 1177951
min, max, mean, gamma, argmax: 0.00000, 0.00333, 0.00032, 300.00000, 803131
min, max, mean, gamma, argmax: 0.00000, 0.01000, 0.00128, 100.00000, 3700
min, max, mean, gamma, argmax: 0.00001, 0.00333, 0.00039, 300.00000, 2882267
min, max, mean, gamma, argmax: 0.00000, 0.00333, 0.00019, 300.00000, 5540515
rawpy read the 10th file at location: ./dataset/Sony/long/00038_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 0.01000, 0.00078, 100.00000, 2598691
min, max, mean, gamma, argmax: 0.00000, 0.01000, 0.00055, 100.00000, 4653941
min, max, mean, gamma, argmax: 0.00000, 0.01000, 0.00044, 100.00000, 6102869
min, max, mean, gamma, argmax: 0.00000, 0.00333, 0.00011, 300.00000, 1458187
min, max, mean, gamma, argmax: 0.00000, 0.00400, 0.00009, 250.00000, 3426263
min, max, mean, gamma, argmax: 0.00000, 0.00333, 0.00027, 300.00000, 4586373
min, max, mean, gamma, argmax: 0.00000, 0.00400, 0.00017, 250.00000, 909101
min, max, mean, gamma, argmax: 0.00000, 0.00047, 0.00006, 250.00000, 2159003
min, max, mean, gamma, argmax: 0.00000, 0.01000, 0.00081, 100.00000, 3630407
min, max, mean, gamma, argmax: 0.00000, 0.00333, 0.00031, 300.00000, 6735543
rawpy read the 20th file at location: ./dataset/Sony/long/00072_00_30s.ARW
rawpy read the 30th file at location: ./dataset/Sony/long/00039_00_10s.ARW
rawpy read the 40th file at location: ./dataset/Sony/long/00200_00_10s.ARW
rawpy read the 50th file at location: ./dataset/Sony/long/00024_00_10s.ARW
rawpy read the 60th file at location: ./dataset/Sony/long/00084_00_30s.ARW
rawpy read the 70th file at location: ./dataset/Sony/long/00164_00_30s.ARW
rawpy read the 80th file at location: ./dataset/Sony/long/00128_00_30s.ARW
rawpy read the 90th file at location: ./dataset/Sony/long/00057_00_10s.ARW
rawpy read the 100th file at location: ./dataset/Sony/long/00059_00_10s.ARW
rawpy read the 110th file at location: ./dataset/Sony/long/00026_00_10s.ARW
rawpy read the 120th file at location: ./dataset/Sony/long/00012_00_10s.ARW
rawpy read the 130th file at location: ./dataset/Sony/long/00090_00_30s.ARW
rawpy read the 140th file at location: ./dataset/Sony/long/00114_00_30s.ARW
rawpy read the 150th file at location: ./dataset/Sony/long/00156_00_30s.ARW
rawpy read the 160th file at location: ./dataset/Sony/long/00219_00_10s.ARW
161 images loaded to CPU RAM in Time=29.780 seconds.

Moved images data to a numpy array.



BATCH_SIZE 16 ,final_epoch 8001 ,no_of_batches 10 ,ps 128 ,result_dir ./gt_Sony_CNN5_FC2_no_log_jitter/ ,len(train_ids) 161
Scaling the log regression labels now.

Epoch 4001: at batch 1: Training dataset Loss=0.247, Batch Time=1.540
		Epoch 4001: Epoch time = 1.810, Avg epoch time=1.809, Total Time=0.905

Loss vector (slice for the first 20 images)
[[0.26373243]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.18600264]
 [0.        ]
 [0.2468327 ]
 [0.        ]
 [0.16177985]
 [0.16177985]
 [0.        ]
 [0.16177985]
 [0.23807368]
 [0.24412535]
 [0.        ]
 [0.21622339]
 [0.        ]
 [0.18600264]
 [0.21622339]
 [0.19065081]]
Epoch 4002: at batch 1: Training dataset Loss=0.235, Batch Time=0.028
Loss vector (slice for the first 20 images)
[[0.3320044 ]
 [0.22285622]
 [0.21305229]
 [0.        ]
 [0.22285622]
 [0.27853262]
 [0.26343083]
 [0.        ]
 [0.3320044 ]
 [0.17967999]
 [0.        ]
 [0.30638599]
 [0.23807368]
 [0.24412535]
 [0.17967999]
 [0.21622339]
 [0.3320044 ]
 [0.30638599]
 [0.17967999]
 [0.3320044 ]]
Epoch 4003: at batch 1: Training dataset Loss=0.236, Batch Time=0.029
Loss vector (slice for the first 20 images)
[[0.3320044 ]
 [0.27576911]
 [0.28938243]
 [0.27576911]
 [0.28735375]
 [0.27853262]
 [0.21084735]
 [0.        ]
 [0.3320044 ]
 [0.30291894]
 [0.        ]
 [0.30638599]
 [0.28938243]
 [0.28735375]
 [0.2815415 ]
 [0.21622339]
 [0.3320044 ]
 [0.31062889]
 [0.2815415 ]
 [0.3320044 ]]
Epoch 4004: at batch 1: Training dataset Loss=0.256, Batch Time=0.034
Loss vector (slice for the first 20 images)
[[0.3320044 ]
 [0.24489881]
 [0.28938243]
 [0.27576911]
 [0.28735375]
 [0.11377752]
 [0.21084735]
 [0.        ]
 [0.18096267]
 [0.23381907]
 [0.11377752]
 [0.27479136]
 [0.15344673]
 [0.28735375]
 [0.15344673]
 [0.37699306]
 [0.27479136]
 [0.31062889]
 [0.11377752]
 [0.3320044 ]]
Epoch 4005: at batch 1: Training dataset Loss=0.240, Batch Time=0.030
Epoch 4006: at batch 1: Training dataset Loss=0.233, Batch Time=0.032
Epoch 4007: at batch 1: Training dataset Loss=0.233, Batch Time=0.028
Epoch 4008: at batch 1: Training dataset Loss=0.224, Batch Time=0.030
Epoch 4009: at batch 1: Training dataset Loss=0.223, Batch Time=0.024
Epoch 4011: at batch 1: Training dataset Loss=0.248, Batch Time=0.028
Epoch 4021: at batch 1: Training dataset Loss=0.238, Batch Time=0.029
Epoch 4031: at batch 1: Training dataset Loss=0.212, Batch Time=0.031
Epoch 4041: at batch 1: Training dataset Loss=0.230, Batch Time=0.028
Epoch 4051: at batch 1: Training dataset Loss=0.217, Batch Time=0.035
Epoch 4061: at batch 1: Training dataset Loss=0.230, Batch Time=0.025
Epoch 4071: at batch 1: Training dataset Loss=0.234, Batch Time=0.027
Epoch 4081: at batch 1: Training dataset Loss=0.252, Batch Time=0.027
Epoch 4091: at batch 1: Training dataset Loss=0.227, Batch Time=0.029
Epoch 4101: at batch 1: Training dataset Loss=0.236, Batch Time=0.029
Loss vector (slice for the first 20 images)
[[0.30528653]
 [0.31485754]
 [0.2269287 ]
 [0.20042016]
 [0.33796096]
 [0.14863753]
 [0.2791748 ]
 [0.20042016]
 [0.19769993]
 [0.24062601]
 [0.24654406]
 [0.20803583]
 [0.24393958]
 [0.27385789]
 [0.31485754]
 [0.21216911]
 [0.14863753]
 [0.20042016]
 [0.27385789]
 [0.30528653]]
Epoch 4111: at batch 1: Training dataset Loss=0.233, Batch Time=0.029
Epoch 4121: at batch 1: Training dataset Loss=0.250, Batch Time=0.030
Epoch 4131: at batch 1: Training dataset Loss=0.240, Batch Time=0.032
Epoch 4141: at batch 1: Training dataset Loss=0.192, Batch Time=0.033
Epoch 4151: at batch 1: Training dataset Loss=0.222, Batch Time=0.029
Epoch 4161: at batch 1: Training dataset Loss=0.228, Batch Time=0.025
Epoch 4171: at batch 1: Training dataset Loss=0.225, Batch Time=0.027
Epoch 4181: at batch 1: Training dataset Loss=0.220, Batch Time=0.028
Epoch 4191: at batch 1: Training dataset Loss=0.214, Batch Time=0.027
Epoch 4201: at batch 1: Training dataset Loss=0.243, Batch Time=0.035
Loss vector (slice for the first 20 images)
[[0.31476194]
 [0.30705732]
 [0.22339314]
 [0.21151805]
 [0.15908483]
 [0.20622215]
 [0.17333448]
 [0.15908483]
 [0.27975264]
 [0.16988093]
 [0.16988093]
 [0.32258385]
 [0.21151805]
 [0.21335049]
 [0.21151805]
 [0.35957661]
 [0.30705732]
 [0.23412405]
 [0.24721608]
 [0.19809484]]
Epoch 4211: at batch 1: Training dataset Loss=0.228, Batch Time=0.027
Epoch 4221: at batch 1: Training dataset Loss=0.238, Batch Time=0.028
Epoch 4231: at batch 1: Training dataset Loss=0.249, Batch Time=0.030
Epoch 4241: at batch 1: Training dataset Loss=0.221, Batch Time=0.033
Epoch 4251: at batch 1: Training dataset Loss=0.207, Batch Time=0.027
Epoch 4261: at batch 1: Training dataset Loss=0.241, Batch Time=0.026
Epoch 4271: at batch 1: Training dataset Loss=0.224, Batch Time=0.024
Epoch 4281: at batch 1: Training dataset Loss=0.221, Batch Time=0.032
Epoch 4291: at batch 1: Training dataset Loss=0.220, Batch Time=0.024
Epoch 4301: at batch 1: Training dataset Loss=0.235, Batch Time=0.035
		Epoch 4301: Epoch time = 125.296, Avg epoch time=0.285, Total Time=0.415

Loss vector (slice for the first 20 images)
[[0.20512116]
 [0.27741653]
 [0.12849396]
 [0.1861413 ]
 [0.28580153]
 [0.1861413 ]
 [0.39512828]
 [0.21643659]
 [0.29742837]
 [0.1861413 ]
 [0.26484886]
 [0.20512116]
 [0.1861413 ]
 [0.29742837]
 [0.39512828]
 [0.27741653]
 [0.20899357]
 [0.30246735]
 [0.23995365]
 [0.27741653]]
Epoch 4311: at batch 1: Training dataset Loss=0.227, Batch Time=0.028
Epoch 4321: at batch 1: Training dataset Loss=0.252, Batch Time=0.034
Epoch 4331: at batch 1: Training dataset Loss=0.227, Batch Time=0.028
Epoch 4341: at batch 1: Training dataset Loss=0.234, Batch Time=0.028
Epoch 4351: at batch 1: Training dataset Loss=0.234, Batch Time=0.033
Epoch 4361: at batch 1: Training dataset Loss=0.227, Batch Time=0.029
Epoch 4371: at batch 1: Training dataset Loss=0.246, Batch Time=0.026
Epoch 4381: at batch 1: Training dataset Loss=0.245, Batch Time=0.029
Epoch 4391: at batch 1: Training dataset Loss=0.234, Batch Time=0.025
Epoch 4401: at batch 1: Training dataset Loss=0.226, Batch Time=0.024
Loss vector (slice for the first 20 images)
[[0.34042883]
 [0.23817217]
 [0.15657693]
 [0.25986925]
 [0.19015978]
 [0.16987304]
 [0.17955582]
 [0.16987304]
 [0.19015978]
 [0.16483061]
 [0.16987304]
 [0.19015978]
 [0.19064161]
 [0.19064161]
 [0.16198498]
 [0.24683425]
 [0.23817217]
 [0.15657693]
 [0.23517153]
 [0.2268935 ]]
Epoch 4411: at batch 1: Training dataset Loss=0.231, Batch Time=0.029
Epoch 4421: at batch 1: Training dataset Loss=0.228, Batch Time=0.026
Epoch 4431: at batch 1: Training dataset Loss=0.259, Batch Time=0.024
Epoch 4441: at batch 1: Training dataset Loss=0.245, Batch Time=0.023
Epoch 4451: at batch 1: Training dataset Loss=0.253, Batch Time=0.034
Epoch 4461: at batch 1: Training dataset Loss=0.238, Batch Time=0.027
Epoch 4471: at batch 1: Training dataset Loss=0.230, Batch Time=0.032
Epoch 4481: at batch 1: Training dataset Loss=0.260, Batch Time=0.035
Epoch 4491: at batch 1: Training dataset Loss=0.230, Batch Time=0.033
Epoch 4501: at batch 1: Training dataset Loss=0.226, Batch Time=0.025
Loss vector (slice for the first 20 images)
[[0.35557535]
 [0.17458263]
 [0.26536423]
 [0.26840025]
 [0.29643589]
 [0.23409273]
 [0.26840025]
 [0.26840025]
 [0.26840025]
 [0.35557535]
 [0.15221971]
 [0.20936853]
 [0.32061082]
 [0.29643589]
 [0.27129227]
 [0.22743845]
 [0.31468907]
 [0.25750464]
 [0.2483089 ]
 [0.29643589]]
Epoch 4511: at batch 1: Training dataset Loss=0.235, Batch Time=0.033
Epoch 4521: at batch 1: Training dataset Loss=0.225, Batch Time=0.033
Epoch 4531: at batch 1: Training dataset Loss=0.242, Batch Time=0.032
Epoch 4541: at batch 1: Training dataset Loss=0.216, Batch Time=0.032
Epoch 4551: at batch 1: Training dataset Loss=0.214, Batch Time=0.033
Epoch 4561: at batch 1: Training dataset Loss=0.225, Batch Time=0.034
Epoch 4571: at batch 1: Training dataset Loss=0.236, Batch Time=0.026
Epoch 4581: at batch 1: Training dataset Loss=0.238, Batch Time=0.026
Epoch 4591: at batch 1: Training dataset Loss=0.228, Batch Time=0.035
Epoch 4601: at batch 1: Training dataset Loss=0.244, Batch Time=0.029
		Epoch 4601: Epoch time = 249.453, Avg epoch time=0.273, Total Time=0.414

Loss vector (slice for the first 20 images)
[[0.33367759]
 [0.31526798]
 [0.19339892]
 [0.31008554]
 [0.26429579]
 [0.246277  ]
 [0.21832247]
 [0.15526634]
 [0.25101569]
 [0.3185026 ]
 [0.26429579]
 [0.18387905]
 [0.21907723]
 [0.31008554]
 [0.31008554]
 [0.24550557]
 [0.31008554]
 [0.25101569]
 [0.34030867]
 [0.24550557]]
Epoch 4611: at batch 1: Training dataset Loss=0.232, Batch Time=0.033
Epoch 4621: at batch 1: Training dataset Loss=0.208, Batch Time=0.027
Epoch 4631: at batch 1: Training dataset Loss=0.229, Batch Time=0.027
Epoch 4641: at batch 1: Training dataset Loss=0.205, Batch Time=0.030
Epoch 4651: at batch 1: Training dataset Loss=0.224, Batch Time=0.033
Epoch 4661: at batch 1: Training dataset Loss=0.236, Batch Time=0.030
Epoch 4671: at batch 1: Training dataset Loss=0.225, Batch Time=0.032
Epoch 4681: at batch 1: Training dataset Loss=0.213, Batch Time=0.027
Epoch 4691: at batch 1: Training dataset Loss=0.254, Batch Time=0.027
Epoch 4701: at batch 1: Training dataset Loss=0.237, Batch Time=0.032
Loss vector (slice for the first 20 images)
[[0.17955643]
 [0.19935396]
 [0.24375233]
 [0.33288193]
 [0.24375233]
 [0.19935396]
 [0.17163569]
 [0.25426596]
 [0.26434425]
 [0.25272793]
 [0.23490492]
 [0.1599423 ]
 [0.23444577]
 [0.22581246]
 [0.22581246]
 [0.18776602]
 [0.1599423 ]
 [0.15520518]
 [0.17955643]
 [0.32348067]]
Epoch 4711: at batch 1: Training dataset Loss=0.244, Batch Time=0.026
Epoch 4721: at batch 1: Training dataset Loss=0.252, Batch Time=0.030
Epoch 4731: at batch 1: Training dataset Loss=0.243, Batch Time=0.033
Epoch 4741: at batch 1: Training dataset Loss=0.219, Batch Time=0.027
Epoch 4751: at batch 1: Training dataset Loss=0.224, Batch Time=0.027
Epoch 4761: at batch 1: Training dataset Loss=0.233, Batch Time=0.035
Epoch 4771: at batch 1: Training dataset Loss=0.214, Batch Time=0.031
Epoch 4781: at batch 1: Training dataset Loss=0.228, Batch Time=0.030
Epoch 4791: at batch 1: Training dataset Loss=0.244, Batch Time=0.030
Epoch 4801: at batch 1: Training dataset Loss=0.202, Batch Time=0.027
Loss vector (slice for the first 20 images)
[[0.28116393]
 [0.2539221 ]
 [0.17791113]
 [0.14195803]
 [0.28116393]
 [0.213512  ]
 [0.17000309]
 [0.19734864]
 [0.28116393]
 [0.36654651]
 [0.21804199]
 [0.16110444]
 [0.28116393]
 [0.14448996]
 [0.1428887 ]
 [0.14195803]
 [0.12590092]
 [0.20923525]
 [0.17732567]
 [0.14448996]]
Epoch 4811: at batch 1: Training dataset Loss=0.221, Batch Time=0.028
Epoch 4821: at batch 1: Training dataset Loss=0.229, Batch Time=0.024
Epoch 4831: at batch 1: Training dataset Loss=0.242, Batch Time=0.030
Epoch 4841: at batch 1: Training dataset Loss=0.230, Batch Time=0.034
Epoch 4851: at batch 1: Training dataset Loss=0.226, Batch Time=0.025
Epoch 4861: at batch 1: Training dataset Loss=0.203, Batch Time=0.031
Epoch 4871: at batch 1: Training dataset Loss=0.212, Batch Time=0.030
Epoch 4881: at batch 1: Training dataset Loss=0.222, Batch Time=0.027
Epoch 4891: at batch 1: Training dataset Loss=0.229, Batch Time=0.033
Epoch 4901: at batch 1: Training dataset Loss=0.252, Batch Time=0.025
		Epoch 4901: Epoch time = 372.486, Avg epoch time=0.262, Total Time=0.413

Loss vector (slice for the first 20 images)
[[0.1377573 ]
 [0.34566635]
 [0.24229716]
 [0.28367895]
 [0.28367895]
 [0.1377573 ]
 [0.23207299]
 [0.24229716]
 [0.2442821 ]
 [0.34455538]
 [0.33234489]
 [0.31782156]
 [0.16545302]
 [0.29874468]
 [0.34015447]
 [0.26638278]
 [0.31782156]
 [0.29874468]
 [0.20999007]
 [0.2442821 ]]
Epoch 4911: at batch 1: Training dataset Loss=0.250, Batch Time=0.029
Epoch 4921: at batch 1: Training dataset Loss=0.253, Batch Time=0.035
Epoch 4931: at batch 1: Training dataset Loss=0.230, Batch Time=0.026
Epoch 4941: at batch 1: Training dataset Loss=0.211, Batch Time=0.034
Epoch 4951: at batch 1: Training dataset Loss=0.228, Batch Time=0.033
Epoch 4961: at batch 1: Training dataset Loss=0.237, Batch Time=0.030
Epoch 4971: at batch 1: Training dataset Loss=0.235, Batch Time=0.032
Epoch 4981: at batch 1: Training dataset Loss=0.222, Batch Time=0.025
Epoch 4991: at batch 1: Training dataset Loss=0.235, Batch Time=0.027
Epoch 5001: at batch 1: Training dataset Loss=0.212, Batch Time=0.026
Loss vector (slice for the first 20 images)
[[0.4181962 ]
 [0.41070464]
 [0.1355281 ]
 [0.13125978]
 [0.15990944]
 [0.21200421]
 [0.2416653 ]
 [0.13125978]
 [0.20276825]
 [0.4181962 ]
 [0.18242794]
 [0.13785575]
 [0.1420854 ]
 [0.17961618]
 [0.16465938]
 [0.23076983]
 [0.2326697 ]
 [0.25959122]
 [0.20276825]
 [0.1420854 ]]
Epoch 5011: at batch 1: Training dataset Loss=0.234, Batch Time=0.032
Epoch 5021: at batch 1: Training dataset Loss=0.231, Batch Time=0.026
Epoch 5031: at batch 1: Training dataset Loss=0.223, Batch Time=0.031
Epoch 5041: at batch 1: Training dataset Loss=0.207, Batch Time=0.030
Epoch 5051: at batch 1: Training dataset Loss=0.235, Batch Time=0.036
Epoch 5061: at batch 1: Training dataset Loss=0.235, Batch Time=0.024
Epoch 5071: at batch 1: Training dataset Loss=0.239, Batch Time=0.025
Epoch 5081: at batch 1: Training dataset Loss=0.226, Batch Time=0.035
Epoch 5091: at batch 1: Training dataset Loss=0.209, Batch Time=0.026
Epoch 5101: at batch 1: Training dataset Loss=0.208, Batch Time=0.027
Loss vector (slice for the first 20 images)
[[0.17068173]
 [0.21032405]
 [0.27304828]
 [0.15017967]
 [0.19915485]
 [0.32139051]
 [0.32139051]
 [0.18259756]
 [0.23442513]
 [0.17068173]
 [0.18259756]
 [0.2031059 ]
 [0.12469597]
 [0.12469597]
 [0.17072059]
 [0.24944277]
 [0.19423133]
 [0.17068173]
 [0.12469597]
 [0.19158129]]
Epoch 5111: at batch 1: Training dataset Loss=0.210, Batch Time=0.030
Epoch 5121: at batch 1: Training dataset Loss=0.228, Batch Time=0.030
Epoch 5131: at batch 1: Training dataset Loss=0.253, Batch Time=0.033
Epoch 5141: at batch 1: Training dataset Loss=0.238, Batch Time=0.026
Epoch 5151: at batch 1: Training dataset Loss=0.242, Batch Time=0.028
Epoch 5161: at batch 1: Training dataset Loss=0.215, Batch Time=0.030
Epoch 5171: at batch 1: Training dataset Loss=0.195, Batch Time=0.025
Epoch 5181: at batch 1: Training dataset Loss=0.237, Batch Time=0.026
Epoch 5191: at batch 1: Training dataset Loss=0.187, Batch Time=0.026
Epoch 5201: at batch 1: Training dataset Loss=0.242, Batch Time=0.035
		Epoch 5201: Epoch time = 495.823, Avg epoch time=0.301, Total Time=0.412

Loss vector (slice for the first 20 images)
[[0.2095625 ]
 [0.22446993]
 [0.21290623]
 [0.14052716]
 [0.20387894]
 [0.19678789]
 [0.25102574]
 [0.17148106]
 [0.26365677]
 [0.16733471]
 [0.26108676]
 [0.38662684]
 [0.11751889]
 [0.27574635]
 [0.19678789]
 [0.38662684]
 [0.33071956]
 [0.27574635]
 [0.26729146]
 [0.16731705]]
Epoch 5211: at batch 1: Training dataset Loss=0.221, Batch Time=0.032
Epoch 5221: at batch 1: Training dataset Loss=0.214, Batch Time=0.028
Epoch 5231: at batch 1: Training dataset Loss=0.213, Batch Time=0.030
Epoch 5241: at batch 1: Training dataset Loss=0.241, Batch Time=0.033
Epoch 5251: at batch 1: Training dataset Loss=0.230, Batch Time=0.026
Epoch 5261: at batch 1: Training dataset Loss=0.254, Batch Time=0.024
Epoch 5271: at batch 1: Training dataset Loss=0.223, Batch Time=0.029
Epoch 5281: at batch 1: Training dataset Loss=0.214, Batch Time=0.024
Epoch 5291: at batch 1: Training dataset Loss=0.222, Batch Time=0.033
Epoch 5301: at batch 1: Training dataset Loss=0.212, Batch Time=0.029
Loss vector (slice for the first 20 images)
[[0.18523508]
 [0.25126275]
 [0.2377525 ]
 [0.22312434]
 [0.20658138]
 [0.30683982]
 [0.25126275]
 [0.25656176]
 [0.24302098]
 [0.28730989]
 [0.19612962]
 [0.28730989]
 [0.25656176]
 [0.3005932 ]
 [0.22312434]
 [0.24732223]
 [0.24424955]
 [0.17822054]
 [0.2377525 ]
 [0.30683982]]
Epoch 5311: at batch 1: Training dataset Loss=0.215, Batch Time=0.030
Epoch 5321: at batch 1: Training dataset Loss=0.234, Batch Time=0.030
Epoch 5331: at batch 1: Training dataset Loss=0.218, Batch Time=0.032
Epoch 5341: at batch 1: Training dataset Loss=0.202, Batch Time=0.030
Epoch 5351: at batch 1: Training dataset Loss=0.227, Batch Time=0.032
Epoch 5361: at batch 1: Training dataset Loss=0.232, Batch Time=0.031
Epoch 5371: at batch 1: Training dataset Loss=0.221, Batch Time=0.031
Epoch 5381: at batch 1: Training dataset Loss=0.241, Batch Time=0.029
Epoch 5391: at batch 1: Training dataset Loss=0.201, Batch Time=0.035
Epoch 5401: at batch 1: Training dataset Loss=0.229, Batch Time=0.027
Loss vector (slice for the first 20 images)
[[0.22286648]
 [0.24099635]
 [0.1344655 ]
 [0.22286648]
 [0.09226775]
 [0.29234737]
 [0.36481568]
 [0.22286648]
 [0.18512997]
 [0.27749479]
 [0.18153811]
 [0.27749479]
 [0.18506786]
 [0.36481568]
 [0.28508213]
 [0.29992276]
 [0.18579453]
 [0.17272449]
 [0.15020861]
 [0.18153811]]
Epoch 5411: at batch 1: Training dataset Loss=0.228, Batch Time=0.036
Epoch 5421: at batch 1: Training dataset Loss=0.196, Batch Time=0.030
Epoch 5431: at batch 1: Training dataset Loss=0.228, Batch Time=0.028
Epoch 5441: at batch 1: Training dataset Loss=0.237, Batch Time=0.030
Epoch 5451: at batch 1: Training dataset Loss=0.205, Batch Time=0.024
Epoch 5461: at batch 1: Training dataset Loss=0.232, Batch Time=0.026
Epoch 5471: at batch 1: Training dataset Loss=0.204, Batch Time=0.024
Epoch 5481: at batch 1: Training dataset Loss=0.230, Batch Time=0.024
Epoch 5491: at batch 1: Training dataset Loss=0.220, Batch Time=0.029
Epoch 5501: at batch 1: Training dataset Loss=0.231, Batch Time=0.033
		Epoch 5501: Epoch time = 619.252, Avg epoch time=0.289, Total Time=0.412

Loss vector (slice for the first 20 images)
[[0.16762176]
 [0.19715828]
 [0.1700622 ]
 [0.16292249]
 [0.21545339]
 [0.16487439]
 [0.31212318]
 [0.25117457]
 [0.15242499]
 [0.19677997]
 [0.22235523]
 [0.19715828]
 [0.31585947]
 [0.25117457]
 [0.15958145]
 [0.25100338]
 [0.26105273]
 [0.30902219]
 [0.15958145]
 [0.1700622 ]]
Epoch 5511: at batch 1: Training dataset Loss=0.216, Batch Time=0.027
Epoch 5521: at batch 1: Training dataset Loss=0.207, Batch Time=0.033
Epoch 5531: at batch 1: Training dataset Loss=0.233, Batch Time=0.031
Epoch 5541: at batch 1: Training dataset Loss=0.216, Batch Time=0.026
Epoch 5551: at batch 1: Training dataset Loss=0.217, Batch Time=0.035
Epoch 5561: at batch 1: Training dataset Loss=0.213, Batch Time=0.024
Epoch 5571: at batch 1: Training dataset Loss=0.210, Batch Time=0.026
Epoch 5581: at batch 1: Training dataset Loss=0.214, Batch Time=0.033
Epoch 5591: at batch 1: Training dataset Loss=0.228, Batch Time=0.026
Epoch 5601: at batch 1: Training dataset Loss=0.226, Batch Time=0.029
Loss vector (slice for the first 20 images)
[[0.17008696]
 [0.25766861]
 [0.25292051]
 [0.2547558 ]
 [0.23929386]
 [0.25292051]
 [0.14809829]
 [0.2547558 ]
 [0.22034079]
 [0.16179362]
 [0.18118006]
 [0.34642893]
 [0.14985272]
 [0.34642893]
 [0.14809829]
 [0.2547558 ]
 [0.19524592]
 [0.20288953]
 [0.18962365]
 [0.22161636]]
Epoch 5611: at batch 1: Training dataset Loss=0.212, Batch Time=0.030
Epoch 5621: at batch 1: Training dataset Loss=0.202, Batch Time=0.029
Epoch 5631: at batch 1: Training dataset Loss=0.237, Batch Time=0.035
Epoch 5641: at batch 1: Training dataset Loss=0.238, Batch Time=0.036
Epoch 5651: at batch 1: Training dataset Loss=0.208, Batch Time=0.033
Epoch 5661: at batch 1: Training dataset Loss=0.217, Batch Time=0.029
Epoch 5671: at batch 1: Training dataset Loss=0.230, Batch Time=0.028
Epoch 5681: at batch 1: Training dataset Loss=0.220, Batch Time=0.027
Epoch 5691: at batch 1: Training dataset Loss=0.208, Batch Time=0.028
Epoch 5701: at batch 1: Training dataset Loss=0.222, Batch Time=0.035
Loss vector (slice for the first 20 images)
[[0.32036537]
 [0.26937512]
 [0.33243492]
 [0.18630695]
 [0.33243492]
 [0.19178919]
 [0.18712819]
 [0.20405143]
 [0.26937512]
 [0.4536269 ]
 [0.22396344]
 [0.4536269 ]
 [0.16068703]
 [0.19730762]
 [0.18630695]
 [0.33243492]
 [0.14418471]
 [0.26937512]
 [0.22396344]
 [0.22163029]]
Epoch 5711: at batch 1: Training dataset Loss=0.229, Batch Time=0.032
Epoch 5721: at batch 1: Training dataset Loss=0.200, Batch Time=0.028
Epoch 5731: at batch 1: Training dataset Loss=0.197, Batch Time=0.026
Epoch 5741: at batch 1: Training dataset Loss=0.227, Batch Time=0.034
Epoch 5751: at batch 1: Training dataset Loss=0.220, Batch Time=0.032
Epoch 5761: at batch 1: Training dataset Loss=0.223, Batch Time=0.024
Epoch 5771: at batch 1: Training dataset Loss=0.231, Batch Time=0.026
Epoch 5781: at batch 1: Training dataset Loss=0.208, Batch Time=0.029
Epoch 5791: at batch 1: Training dataset Loss=0.213, Batch Time=0.026
Epoch 5801: at batch 1: Training dataset Loss=0.223, Batch Time=0.027
		Epoch 5801: Epoch time = 742.594, Avg epoch time=0.265, Total Time=0.412

Loss vector (slice for the first 20 images)
[[0.2881659 ]
 [0.2542778 ]
 [0.2356437 ]
 [0.2542778 ]
 [0.2542778 ]
 [0.2542778 ]
 [0.2542778 ]
 [0.2542778 ]
 [0.30960214]
 [0.26507241]
 [0.22333403]
 [0.13456115]
 [0.2356437 ]
 [0.30960214]
 [0.19934973]
 [0.24349046]
 [0.24282677]
 [0.26507241]
 [0.2356437 ]
 [0.20792493]]
Epoch 5811: at batch 1: Training dataset Loss=0.224, Batch Time=0.026
Epoch 5821: at batch 1: Training dataset Loss=0.261, Batch Time=0.029
Epoch 5831: at batch 1: Training dataset Loss=0.231, Batch Time=0.025
Epoch 5841: at batch 1: Training dataset Loss=0.208, Batch Time=0.033
Epoch 5851: at batch 1: Training dataset Loss=0.231, Batch Time=0.029
Epoch 5861: at batch 1: Training dataset Loss=0.218, Batch Time=0.029
Epoch 5871: at batch 1: Training dataset Loss=0.207, Batch Time=0.032
Epoch 5881: at batch 1: Training dataset Loss=0.232, Batch Time=0.026
Epoch 5891: at batch 1: Training dataset Loss=0.216, Batch Time=0.027
Epoch 5901: at batch 1: Training dataset Loss=0.233, Batch Time=0.036
Loss vector (slice for the first 20 images)
[[0.3172583 ]
 [0.3354041 ]
 [0.34094602]
 [0.39538011]
 [0.3172583 ]
 [0.19450589]
 [0.28004616]
 [0.21910597]
 [0.17464262]
 [0.34094602]
 [0.17464262]
 [0.14524026]
 [0.28004616]
 [0.2124574 ]
 [0.12366045]
 [0.30821872]
 [0.22061427]
 [0.34094602]
 [0.26995113]
 [0.28004616]]
Epoch 5911: at batch 1: Training dataset Loss=0.212, Batch Time=0.032
Epoch 5921: at batch 1: Training dataset Loss=0.225, Batch Time=0.030
Epoch 5931: at batch 1: Training dataset Loss=0.232, Batch Time=0.035
Epoch 5941: at batch 1: Training dataset Loss=0.225, Batch Time=0.030
Epoch 5951: at batch 1: Training dataset Loss=0.237, Batch Time=0.032
Epoch 5961: at batch 1: Training dataset Loss=0.207, Batch Time=0.027
Epoch 5971: at batch 1: Training dataset Loss=0.220, Batch Time=0.035
Epoch 5981: at batch 1: Training dataset Loss=0.239, Batch Time=0.027
Epoch 5991: at batch 1: Training dataset Loss=0.213, Batch Time=0.026
Epoch 6001: at batch 1: Training dataset Loss=0.218, Batch Time=0.029
Loss vector (slice for the first 20 images)
[[0.18554312]
 [0.1637826 ]
 [0.1637826 ]
 [0.21429819]
 [0.25400916]
 [0.19512483]
 [0.25400916]
 [0.14647382]
 [0.30677307]
 [0.21429819]
 [0.21429819]
 [0.21404698]
 [0.21429819]
 [0.26696816]
 [0.12854049]
 [0.1637826 ]
 [0.28036994]
 [0.12854049]
 [0.17578456]
 [0.40749007]]
Epoch 6011: at batch 1: Training dataset Loss=0.241, Batch Time=0.024
Epoch 6021: at batch 1: Training dataset Loss=0.210, Batch Time=0.029
Epoch 6031: at batch 1: Training dataset Loss=0.221, Batch Time=0.032
Epoch 6041: at batch 1: Training dataset Loss=0.215, Batch Time=0.032
Epoch 6051: at batch 1: Training dataset Loss=0.237, Batch Time=0.035
Epoch 6061: at batch 1: Training dataset Loss=0.235, Batch Time=0.032
Epoch 6071: at batch 1: Training dataset Loss=0.217, Batch Time=0.028
Epoch 6081: at batch 1: Training dataset Loss=0.216, Batch Time=0.035
Epoch 6091: at batch 1: Training dataset Loss=0.210, Batch Time=0.032
Epoch 6101: at batch 1: Training dataset Loss=0.198, Batch Time=0.026
		Epoch 6101: Epoch time = 865.976, Avg epoch time=0.273, Total Time=0.412

Loss vector (slice for the first 20 images)
[[0.16838627]
 [0.30837047]
 [0.21180409]
 [0.18125644]
 [0.24570847]
 [0.23472556]
 [0.24392691]
 [0.20371273]
 [0.33963415]
 [0.23665047]
 [0.30837047]
 [0.1364809 ]
 [0.21667853]
 [0.12353674]
 [0.30837047]
 [0.38313389]
 [0.26823339]
 [0.23472556]
 [0.26823339]
 [0.30837047]]
Epoch 6111: at batch 1: Training dataset Loss=0.203, Batch Time=0.027
Epoch 6121: at batch 1: Training dataset Loss=0.224, Batch Time=0.035
Epoch 6131: at batch 1: Training dataset Loss=0.204, Batch Time=0.032
Epoch 6141: at batch 1: Training dataset Loss=0.219, Batch Time=0.032
Epoch 6151: at batch 1: Training dataset Loss=0.230, Batch Time=0.032
Epoch 6161: at batch 1: Training dataset Loss=0.220, Batch Time=0.029
Epoch 6171: at batch 1: Training dataset Loss=0.204, Batch Time=0.025
Epoch 6181: at batch 1: Training dataset Loss=0.211, Batch Time=0.030
Epoch 6191: at batch 1: Training dataset Loss=0.237, Batch Time=0.033
Epoch 6201: at batch 1: Training dataset Loss=0.216, Batch Time=0.025
Loss vector (slice for the first 20 images)
[[0.24615207]
 [0.37551874]
 [0.14344189]
 [0.16612342]
 [0.16381527]
 [0.14961946]
 [0.37551874]
 [0.14961946]
 [0.23963663]
 [0.16612342]
 [0.17580378]
 [0.15455496]
 [0.16503   ]
 [0.14961946]
 [0.26569241]
 [0.16847925]
 [0.24615207]
 [0.17598747]
 [0.19998422]
 [0.23358217]]
Epoch 6211: at batch 1: Training dataset Loss=0.199, Batch Time=0.034
Epoch 6221: at batch 1: Training dataset Loss=0.231, Batch Time=0.032
Epoch 6231: at batch 1: Training dataset Loss=0.208, Batch Time=0.029
Epoch 6241: at batch 1: Training dataset Loss=0.228, Batch Time=0.033
Epoch 6251: at batch 1: Training dataset Loss=0.245, Batch Time=0.036
Epoch 6261: at batch 1: Training dataset Loss=0.227, Batch Time=0.026
Epoch 6271: at batch 1: Training dataset Loss=0.226, Batch Time=0.030
Epoch 6281: at batch 1: Training dataset Loss=0.231, Batch Time=0.032
Epoch 6291: at batch 1: Training dataset Loss=0.236, Batch Time=0.027
Epoch 6301: at batch 1: Training dataset Loss=0.202, Batch Time=0.028
Loss vector (slice for the first 20 images)
[[0.17158663]
 [0.21175182]
 [0.22247897]
 [0.29594737]
 [0.22247897]
 [0.24415523]
 [0.28019384]
 [0.15646225]
 [0.22343427]
 [0.22247897]
 [0.13749793]
 [0.28019384]
 [0.17158663]
 [0.16282293]
 [0.27698216]
 [0.22928385]
 [0.13749793]
 [0.22928385]
 [0.22247897]
 [0.28019384]]
Epoch 6311: at batch 1: Training dataset Loss=0.215, Batch Time=0.028
Epoch 6321: at batch 1: Training dataset Loss=0.193, Batch Time=0.033
Epoch 6331: at batch 1: Training dataset Loss=0.233, Batch Time=0.027
Epoch 6341: at batch 1: Training dataset Loss=0.232, Batch Time=0.030
Epoch 6351: at batch 1: Training dataset Loss=0.212, Batch Time=0.033
Epoch 6361: at batch 1: Training dataset Loss=0.235, Batch Time=0.033
Epoch 6371: at batch 1: Training dataset Loss=0.203, Batch Time=0.026
Epoch 6381: at batch 1: Training dataset Loss=0.239, Batch Time=0.032
Epoch 6391: at batch 1: Training dataset Loss=0.221, Batch Time=0.032
Epoch 6401: at batch 1: Training dataset Loss=0.241, Batch Time=0.032
		Epoch 6401: Epoch time = 989.398, Avg epoch time=0.288, Total Time=0.412

Loss vector (slice for the first 20 images)
[[0.21899056]
 [0.34114626]
 [0.24813122]
 [0.18726972]
 [0.1878981 ]
 [0.18271688]
 [0.15655752]
 [0.17609431]
 [0.34114626]
 [0.34114626]
 [0.18271688]
 [0.18613163]
 [0.15655752]
 [0.19375694]
 [0.3791413 ]
 [0.19375694]
 [0.38718435]
 [0.19375694]
 [0.17609431]
 [0.22824346]]
Epoch 6411: at batch 1: Training dataset Loss=0.253, Batch Time=0.030
Epoch 6421: at batch 1: Training dataset Loss=0.207, Batch Time=0.025
Epoch 6431: at batch 1: Training dataset Loss=0.202, Batch Time=0.033
Epoch 6441: at batch 1: Training dataset Loss=0.215, Batch Time=0.026
Epoch 6451: at batch 1: Training dataset Loss=0.221, Batch Time=0.034
Epoch 6461: at batch 1: Training dataset Loss=0.210, Batch Time=0.031
Epoch 6471: at batch 1: Training dataset Loss=0.204, Batch Time=0.031
Epoch 6481: at batch 1: Training dataset Loss=0.219, Batch Time=0.032
Epoch 6491: at batch 1: Training dataset Loss=0.234, Batch Time=0.029
Epoch 6501: at batch 1: Training dataset Loss=0.206, Batch Time=0.034
Loss vector (slice for the first 20 images)
[[0.22037937]
 [0.29682142]
 [0.16606632]
 [0.19451083]
 [0.22037937]
 [0.22037937]
 [0.26615515]
 [0.28179321]
 [0.22437744]
 [0.19451083]
 [0.16606632]
 [0.1837403 ]
 [0.19636489]
 [0.17590451]
 [0.22037937]
 [0.19563252]
 [0.14957654]
 [0.1837403 ]
 [0.28179321]
 [0.28179321]]
Epoch 6511: at batch 1: Training dataset Loss=0.203, Batch Time=0.030
Epoch 6521: at batch 1: Training dataset Loss=0.217, Batch Time=0.026
Epoch 6531: at batch 1: Training dataset Loss=0.210, Batch Time=0.027
Epoch 6541: at batch 1: Training dataset Loss=0.214, Batch Time=0.030
Epoch 6551: at batch 1: Training dataset Loss=0.240, Batch Time=0.032
Epoch 6561: at batch 1: Training dataset Loss=0.221, Batch Time=0.035
Epoch 6571: at batch 1: Training dataset Loss=0.256, Batch Time=0.027
Epoch 6581: at batch 1: Training dataset Loss=0.214, Batch Time=0.031
Epoch 6591: at batch 1: Training dataset Loss=0.227, Batch Time=0.031
Epoch 6601: at batch 1: Training dataset Loss=0.184, Batch Time=0.035
Loss vector (slice for the first 20 images)
[[0.17008686]
 [0.30643278]
 [0.17758222]
 [0.1311534 ]
 [0.16962716]
 [0.20313533]
 [0.35780239]
 [0.21824831]
 [0.22211179]
 [0.22211179]
 [0.22878928]
 [0.23387113]
 [0.19335628]
 [0.21824831]
 [0.21824831]
 [0.2636537 ]
 [0.19470792]
 [0.17372456]
 [0.16746853]
 [0.21772456]]
Epoch 6611: at batch 1: Training dataset Loss=0.228, Batch Time=0.031
Epoch 6621: at batch 1: Training dataset Loss=0.216, Batch Time=0.030
Epoch 6631: at batch 1: Training dataset Loss=0.254, Batch Time=0.032
Epoch 6641: at batch 1: Training dataset Loss=0.223, Batch Time=0.036
Epoch 6651: at batch 1: Training dataset Loss=0.237, Batch Time=0.027
Epoch 6661: at batch 1: Training dataset Loss=0.224, Batch Time=0.029
Epoch 6671: at batch 1: Training dataset Loss=0.210, Batch Time=0.035
Epoch 6681: at batch 1: Training dataset Loss=0.213, Batch Time=0.034
Epoch 6691: at batch 1: Training dataset Loss=0.199, Batch Time=0.031
Epoch 6701: at batch 1: Training dataset Loss=0.206, Batch Time=0.025
		Epoch 6701: Epoch time = 1112.849, Avg epoch time=0.296, Total Time=0.412

Loss vector (slice for the first 20 images)
[[0.24002962]
 [0.33763152]
 [0.32768854]
 [0.33763152]
 [0.23360236]
 [0.12559196]
 [0.33763152]
 [0.22242257]
 [0.31613174]
 [0.18165478]
 [0.17664406]
 [0.22963062]
 [0.20270227]
 [0.12559196]
 [0.17331633]
 [0.20534745]
 [0.20534745]
 [0.17714247]
 [0.20534745]
 [0.19325134]]
Epoch 6711: at batch 1: Training dataset Loss=0.224, Batch Time=0.029
Epoch 6721: at batch 1: Training dataset Loss=0.232, Batch Time=0.029
Epoch 6731: at batch 1: Training dataset Loss=0.241, Batch Time=0.035
Epoch 6741: at batch 1: Training dataset Loss=0.206, Batch Time=0.026
Epoch 6751: at batch 1: Training dataset Loss=0.230, Batch Time=0.034
Epoch 6761: at batch 1: Training dataset Loss=0.247, Batch Time=0.027
Epoch 6771: at batch 1: Training dataset Loss=0.246, Batch Time=0.028
Epoch 6781: at batch 1: Training dataset Loss=0.220, Batch Time=0.033
Epoch 6791: at batch 1: Training dataset Loss=0.238, Batch Time=0.034
Epoch 6801: at batch 1: Training dataset Loss=0.232, Batch Time=0.035
Loss vector (slice for the first 20 images)
[[0.18647346]
 [0.26520213]
 [0.15446922]
 [0.25262204]
 [0.29770875]
 [0.27398524]
 [0.2042224 ]
 [0.11279878]
 [0.39694294]
 [0.18572301]
 [0.16614312]
 [0.25262204]
 [0.17989379]
 [0.28057349]
 [0.15024501]
 [0.20591292]
 [0.29645556]
 [0.2568112 ]
 [0.29645556]
 [0.26520213]]
Epoch 6811: at batch 1: Training dataset Loss=0.226, Batch Time=0.030
Epoch 6821: at batch 1: Training dataset Loss=0.206, Batch Time=0.035
Epoch 6831: at batch 1: Training dataset Loss=0.220, Batch Time=0.028
Epoch 6841: at batch 1: Training dataset Loss=0.215, Batch Time=0.027
Epoch 6851: at batch 1: Training dataset Loss=0.228, Batch Time=0.036
Epoch 6861: at batch 1: Training dataset Loss=0.226, Batch Time=0.034
Epoch 6871: at batch 1: Training dataset Loss=0.219, Batch Time=0.026
Epoch 6881: at batch 1: Training dataset Loss=0.217, Batch Time=0.032
Epoch 6891: at batch 1: Training dataset Loss=0.215, Batch Time=0.023
Epoch 6901: at batch 1: Training dataset Loss=0.254, Batch Time=0.031
Loss vector (slice for the first 20 images)
[[0.32936895]
 [0.23367631]
 [0.25271088]
 [0.21973357]
 [0.1943219 ]
 [0.17732304]
 [0.35421434]
 [0.21681194]
 [0.20658621]
 [0.28406841]
 [0.20858219]
 [0.19819653]
 [0.20658621]
 [0.20858219]
 [0.18205625]
 [0.35612175]
 [0.17732304]
 [0.19819653]
 [0.20423608]
 [0.20423608]]
Epoch 6911: at batch 1: Training dataset Loss=0.227, Batch Time=0.029
Epoch 6921: at batch 1: Training dataset Loss=0.217, Batch Time=0.029
Epoch 6931: at batch 1: Training dataset Loss=0.210, Batch Time=0.029
Epoch 6941: at batch 1: Training dataset Loss=0.217, Batch Time=0.025
Epoch 6951: at batch 1: Training dataset Loss=0.229, Batch Time=0.028
Epoch 6961: at batch 1: Training dataset Loss=0.234, Batch Time=0.032
Epoch 6971: at batch 1: Training dataset Loss=0.232, Batch Time=0.029
Epoch 6981: at batch 1: Training dataset Loss=0.203, Batch Time=0.028
Epoch 6991: at batch 1: Training dataset Loss=0.198, Batch Time=0.032
Epoch 7001: at batch 1: Training dataset Loss=0.232, Batch Time=0.029
		Epoch 7001: Epoch time = 1236.333, Avg epoch time=0.290, Total Time=0.412

Loss vector (slice for the first 20 images)
[[0.1749904 ]
 [0.30366072]
 [0.24206991]
 [0.1349307 ]
 [0.13985696]
 [0.16710505]
 [0.24206991]
 [0.13985696]
 [0.34605068]
 [0.25847667]
 [0.30936712]
 [0.18066409]
 [0.25847667]
 [0.29945391]
 [0.30936712]
 [0.27607054]
 [0.14003994]
 [0.30366072]
 [0.27607054]
 [0.21885946]]
Epoch 7011: at batch 1: Training dataset Loss=0.245, Batch Time=0.036
Epoch 7021: at batch 1: Training dataset Loss=0.249, Batch Time=0.032
Epoch 7031: at batch 1: Training dataset Loss=0.227, Batch Time=0.026
Epoch 7041: at batch 1: Training dataset Loss=0.214, Batch Time=0.026
Epoch 7051: at batch 1: Training dataset Loss=0.221, Batch Time=0.026
Epoch 7061: at batch 1: Training dataset Loss=0.247, Batch Time=0.032
Epoch 7071: at batch 1: Training dataset Loss=0.224, Batch Time=0.031
Epoch 7081: at batch 1: Training dataset Loss=0.218, Batch Time=0.034
Epoch 7091: at batch 1: Training dataset Loss=0.228, Batch Time=0.035
Epoch 7101: at batch 1: Training dataset Loss=0.217, Batch Time=0.032
Loss vector (slice for the first 20 images)
[[0.20294249]
 [0.31456852]
 [0.17911559]
 [0.31456852]
 [0.29789865]
 [0.1913909 ]
 [0.24628247]
 [0.18598723]
 [0.17631057]
 [0.13159367]
 [0.25810304]
 [0.25810304]
 [0.17911559]
 [0.1906095 ]
 [0.13116521]
 [0.31456852]
 [0.13116521]
 [0.34122097]
 [0.26206535]
 [0.26206535]]
Epoch 7111: at batch 1: Training dataset Loss=0.228, Batch Time=0.027
Epoch 7121: at batch 1: Training dataset Loss=0.225, Batch Time=0.028
Epoch 7131: at batch 1: Training dataset Loss=0.219, Batch Time=0.035
Epoch 7141: at batch 1: Training dataset Loss=0.220, Batch Time=0.035
Epoch 7151: at batch 1: Training dataset Loss=0.222, Batch Time=0.031
Epoch 7161: at batch 1: Training dataset Loss=0.257, Batch Time=0.027
Epoch 7171: at batch 1: Training dataset Loss=0.237, Batch Time=0.024
Epoch 7181: at batch 1: Training dataset Loss=0.247, Batch Time=0.032
Epoch 7191: at batch 1: Training dataset Loss=0.218, Batch Time=0.029
Epoch 7201: at batch 1: Training dataset Loss=0.222, Batch Time=0.031
Loss vector (slice for the first 20 images)
[[0.18032463]
 [0.25629145]
 [0.14855805]
 [0.34038085]
 [0.25648132]
 [0.16149844]
 [0.2227422 ]
 [0.16149844]
 [0.19161034]
 [0.22074035]
 [0.20166045]
 [0.25785217]
 [0.25646099]
 [0.25892049]
 [0.17153649]
 [0.25769243]
 [0.12741664]
 [0.20166045]
 [0.19161034]
 [0.2227422 ]]
Epoch 7211: at batch 1: Training dataset Loss=0.203, Batch Time=0.030
Epoch 7221: at batch 1: Training dataset Loss=0.208, Batch Time=0.032
Epoch 7231: at batch 1: Training dataset Loss=0.206, Batch Time=0.027
Epoch 7241: at batch 1: Training dataset Loss=0.233, Batch Time=0.033
Epoch 7251: at batch 1: Training dataset Loss=0.218, Batch Time=0.028
Epoch 7261: at batch 1: Training dataset Loss=0.213, Batch Time=0.036
Epoch 7271: at batch 1: Training dataset Loss=0.222, Batch Time=0.033
Epoch 7281: at batch 1: Training dataset Loss=0.222, Batch Time=0.028
Epoch 7291: at batch 1: Training dataset Loss=0.221, Batch Time=0.032
Epoch 7301: at batch 1: Training dataset Loss=0.225, Batch Time=0.026
		Epoch 7301: Epoch time = 1359.895, Avg epoch time=0.277, Total Time=0.412

Loss vector (slice for the first 20 images)
[[0.22606662]
 [0.30805033]
 [0.30805033]
 [0.23982772]
 [0.23982772]
 [0.15008277]
 [0.3091138 ]
 [0.23982772]
 [0.21231312]
 [0.23982772]
 [0.22589082]
 [0.21231312]
 [0.20336872]
 [0.23982772]
 [0.17385381]
 [0.37488019]
 [0.17385381]
 [0.26574224]
 [0.14971578]
 [0.20499903]]
Epoch 7311: at batch 1: Training dataset Loss=0.207, Batch Time=0.031
Epoch 7321: at batch 1: Training dataset Loss=0.223, Batch Time=0.027
Epoch 7331: at batch 1: Training dataset Loss=0.234, Batch Time=0.027
Epoch 7341: at batch 1: Training dataset Loss=0.229, Batch Time=0.029
Epoch 7351: at batch 1: Training dataset Loss=0.212, Batch Time=0.033
Epoch 7361: at batch 1: Training dataset Loss=0.240, Batch Time=0.028
Epoch 7371: at batch 1: Training dataset Loss=0.225, Batch Time=0.029
Epoch 7381: at batch 1: Training dataset Loss=0.230, Batch Time=0.029
Epoch 7391: at batch 1: Training dataset Loss=0.214, Batch Time=0.032
Epoch 7401: at batch 1: Training dataset Loss=0.236, Batch Time=0.025
Loss vector (slice for the first 20 images)
[[0.36684629]
 [0.27413443]
 [0.19919786]
 [0.20399532]
 [0.25646475]
 [0.27134863]
 [0.27014631]
 [0.26219893]
 [0.21569481]
 [0.19956005]
 [0.20399532]
 [0.27014631]
 [0.27413443]
 [0.27134863]
 [0.27413443]
 [0.25646475]
 [0.30964354]
 [0.20384178]
 [0.19956005]
 [0.20384178]]
Epoch 7411: at batch 1: Training dataset Loss=0.213, Batch Time=0.026
Epoch 7421: at batch 1: Training dataset Loss=0.219, Batch Time=0.026
Epoch 7431: at batch 1: Training dataset Loss=0.222, Batch Time=0.032
Epoch 7441: at batch 1: Training dataset Loss=0.211, Batch Time=0.029
Epoch 7451: at batch 1: Training dataset Loss=0.216, Batch Time=0.027
Epoch 7461: at batch 1: Training dataset Loss=0.239, Batch Time=0.026
Epoch 7471: at batch 1: Training dataset Loss=0.223, Batch Time=0.025
Epoch 7481: at batch 1: Training dataset Loss=0.223, Batch Time=0.033
Epoch 7491: at batch 1: Training dataset Loss=0.237, Batch Time=0.032
Epoch 7501: at batch 1: Training dataset Loss=0.235, Batch Time=0.025
Loss vector (slice for the first 20 images)
[[0.17002335]
 [0.23259981]
 [0.22631556]
 [0.24212202]
 [0.25961071]
 [0.14212963]
 [0.24092606]
 [0.20595895]
 [0.35593414]
 [0.29627985]
 [0.18847823]
 [0.24212202]
 [0.35593414]
 [0.2468925 ]
 [0.17269957]
 [0.22631556]
 [0.29627985]
 [0.35593414]
 [0.17002335]
 [0.29627985]]
Epoch 7511: at batch 1: Training dataset Loss=0.221, Batch Time=0.029
Epoch 7521: at batch 1: Training dataset Loss=0.228, Batch Time=0.026
Epoch 7531: at batch 1: Training dataset Loss=0.237, Batch Time=0.031
Epoch 7541: at batch 1: Training dataset Loss=0.231, Batch Time=0.026
Epoch 7551: at batch 1: Training dataset Loss=0.222, Batch Time=0.027
Epoch 7561: at batch 1: Training dataset Loss=0.223, Batch Time=0.035
Epoch 7571: at batch 1: Training dataset Loss=0.210, Batch Time=0.029
Epoch 7581: at batch 1: Training dataset Loss=0.219, Batch Time=0.029
Epoch 7591: at batch 1: Training dataset Loss=0.235, Batch Time=0.027
Epoch 7601: at batch 1: Training dataset Loss=0.201, Batch Time=0.024
		Epoch 7601: Epoch time = 1483.334, Avg epoch time=0.278, Total Time=0.412

Loss vector (slice for the first 20 images)
[[0.22005528]
 [0.2965605 ]
 [0.22126451]
 [0.2697874 ]
 [0.23738359]
 [0.24301666]
 [0.23738359]
 [0.15918534]
 [0.3188031 ]
 [0.19337149]
 [0.22126451]
 [0.1520956 ]
 [0.23738359]
 [0.22126451]
 [0.22126451]
 [0.1581015 ]
 [0.1409272 ]
 [0.14569989]
 [0.3188031 ]
 [0.1706623 ]]
Epoch 7611: at batch 1: Training dataset Loss=0.210, Batch Time=0.032
Epoch 7621: at batch 1: Training dataset Loss=0.226, Batch Time=0.024
Epoch 7631: at batch 1: Training dataset Loss=0.219, Batch Time=0.034
Epoch 7641: at batch 1: Training dataset Loss=0.218, Batch Time=0.032
Epoch 7651: at batch 1: Training dataset Loss=0.228, Batch Time=0.026
Epoch 7661: at batch 1: Training dataset Loss=0.214, Batch Time=0.029
Epoch 7671: at batch 1: Training dataset Loss=0.219, Batch Time=0.030
Epoch 7681: at batch 1: Training dataset Loss=0.242, Batch Time=0.027
Epoch 7691: at batch 1: Training dataset Loss=0.222, Batch Time=0.029
Epoch 7701: at batch 1: Training dataset Loss=0.239, Batch Time=0.024
Loss vector (slice for the first 20 images)
[[0.25713867]
 [0.20129426]
 [0.25838068]
 [0.20030729]
 [0.17344472]
 [0.14109364]
 [0.23403248]
 [0.24136744]
 [0.37267417]
 [0.14775741]
 [0.20030729]
 [0.17344472]
 [0.32435846]
 [0.32435846]
 [0.14109364]
 [0.37267417]
 [0.24232894]
 [0.25625095]
 [0.24232894]
 [0.24136744]]
Epoch 7711: at batch 1: Training dataset Loss=0.205, Batch Time=0.024
Epoch 7721: at batch 1: Training dataset Loss=0.217, Batch Time=0.024
Epoch 7731: at batch 1: Training dataset Loss=0.227, Batch Time=0.027
Epoch 7741: at batch 1: Training dataset Loss=0.231, Batch Time=0.030
Epoch 7751: at batch 1: Training dataset Loss=0.203, Batch Time=0.029
Epoch 7761: at batch 1: Training dataset Loss=0.222, Batch Time=0.027
Epoch 7771: at batch 1: Training dataset Loss=0.217, Batch Time=0.030
Epoch 7781: at batch 1: Training dataset Loss=0.214, Batch Time=0.033
Epoch 7791: at batch 1: Training dataset Loss=0.212, Batch Time=0.032
Epoch 7801: at batch 1: Training dataset Loss=0.211, Batch Time=0.025
Loss vector (slice for the first 20 images)
[[0.19056278]
 [0.2212885 ]
 [0.19056278]
 [0.13503057]
 [0.17710373]
 [0.20935172]
 [0.20898613]
 [0.17597494]
 [0.18244109]
 [0.26681784]
 [0.2212885 ]
 [0.20935172]
 [0.13503057]
 [0.20436093]
 [0.21024063]
 [0.25710997]
 [0.2212885 ]
 [0.14320987]
 [0.14320987]
 [0.26681784]]
Epoch 7811: at batch 1: Training dataset Loss=0.192, Batch Time=0.032
Epoch 7821: at batch 1: Training dataset Loss=0.218, Batch Time=0.033
Epoch 7831: at batch 1: Training dataset Loss=0.232, Batch Time=0.026
Epoch 7841: at batch 1: Training dataset Loss=0.217, Batch Time=0.033
Epoch 7851: at batch 1: Training dataset Loss=0.202, Batch Time=0.029
Epoch 7861: at batch 1: Training dataset Loss=0.227, Batch Time=0.025
Epoch 7871: at batch 1: Training dataset Loss=0.216, Batch Time=0.036
Epoch 7881: at batch 1: Training dataset Loss=0.231, Batch Time=0.030
Epoch 7891: at batch 1: Training dataset Loss=0.194, Batch Time=0.029
Epoch 7901: at batch 1: Training dataset Loss=0.228, Batch Time=0.028
		Epoch 7901: Epoch time = 1612.203, Avg epoch time=0.296, Total Time=0.413

Loss vector (slice for the first 20 images)
[[0.22085267]
 [0.26652163]
 [0.20603046]
 [0.25808287]
 [0.33440989]
 [0.20708317]
 [0.26652163]
 [0.2534079 ]
 [0.268664  ]
 [0.29476053]
 [0.26652163]
 [0.26652163]
 [0.21240744]
 [0.21240744]
 [0.29476053]
 [0.31196594]
 [0.2135756 ]
 [0.33397651]
 [0.16527241]
 [0.20708317]]
Epoch 7911: at batch 1: Training dataset Loss=0.230, Batch Time=0.028
Epoch 7921: at batch 1: Training dataset Loss=0.220, Batch Time=0.028
Epoch 7931: at batch 1: Training dataset Loss=0.213, Batch Time=0.031
Epoch 7941: at batch 1: Training dataset Loss=0.231, Batch Time=0.034
Epoch 7951: at batch 1: Training dataset Loss=0.224, Batch Time=0.036
Epoch 7961: at batch 1: Training dataset Loss=0.184, Batch Time=0.028
Epoch 7971: at batch 1: Training dataset Loss=0.261, Batch Time=0.027
Epoch 7981: at batch 1: Training dataset Loss=0.233, Batch Time=0.026
Epoch 7991: at batch 1: Training dataset Loss=0.258, Batch Time=0.027
(sid2) [ir967@gr014 Learning-to-See-in-the-Dark]$ srun: Force Terminated job 403475
slurmstepd: error: *** STEP 403475.0 ON gr014-ib0 CANCELLED AT 2020-12-13T02:25:53 DUE TO TIME LIMIT ***
exit
(base) [ir967@log-3 Learning-to-See-in-the-Dark]$ 