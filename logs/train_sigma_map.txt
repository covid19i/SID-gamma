(base) [ir967@gr064 Learning-to-See-in-the-Dark]$ conda activate sid2
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py
^CTraceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 7, in <module>
    import tensorflow as tf
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/__init__.py", line 22, in <module>
    from tensorflow.python import pywrap_tensorflow  # pylint: disable=unused-import
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/__init__.py", line 49, in <module>
    from tensorflow.python import pywrap_tensorflow
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow.py", line 58, in <module>
    from tensorflow.python.pywrap_tensorflow_internal import *
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 28, in <module>
    _pywrap_tensorflow_internal = swig_import_helper()
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py", line 24, in swig_import_helper
    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)
KeyboardInterrupt
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 11:48:44
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 11:48:44.958614: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 11:48:45.072010: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 11:48:45.072046: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 11:48:47.546805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 11:48:47.546838: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 11:48:47.546860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 11:48:47.547018: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 117, in <module>
    out_noise_level_map = network(in_image)
  File "train_heteroscedastic_sigma_map.py", line 88, in network
    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=relu, scope='g_conv1_1')
NameError: global name 'slim' is not defined
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 11:49:54
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 11:49:54.470327: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 11:49:54.610844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 11:49:54.610874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 11:49:54.893517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 11:49:54.893552: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 11:49:54.893561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 11:49:54.893658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 117, in <module>
    out_noise_level_map = network(in_image)
  File "train_heteroscedastic_sigma_map.py", line 89, in network
    conv2 = slim.conv1(conv1, 32, [3, 3], rate=1, activation_fn=relu, scope='g_conv2_1')
AttributeError: 'module' object has no attribute 'conv1'
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 11:51:00
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 11:51:00.303120: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 11:51:00.439090: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 11:51:00.439120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 11:51:00.717944: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 11:51:00.717977: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 11:51:00.717990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 11:51:00.718089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 121, in <module>
    G_loss = tf.losses.mean_squared_error(noise_level_map, out_noise_level_map)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py", line 670, in mean_squared_error
    predictions.get_shape().assert_is_compatible_with(labels.get_shape())
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py", line 848, in assert_is_compatible_with
    raise ValueError("Shapes %s and %s are incompatible" % (self, other))
ValueError: Shapes (?, ?, ?, 12) and (?, ?, ?, 2) are incompatible
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 11:54:24
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 11:54:24.300414: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 11:54:24.441395: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 11:54:24.441426: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 11:54:24.726436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 11:54:24.726469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 11:54:24.726550: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 11:54:24.726654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
No checkpoint found at ./heteroschedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,result_dir= ./heteroschedastic_sigma_map_checkpoint/ ,len(train_fns)= 161

Cleared all images in memory.

Starting Training on index [ 83  66  36 144  82 142 145  25  76  71  33  80 122  16  29   8]
dataset index: ['./dataset/Sony/long/00195_00_10s.ARW'
 './dataset/Sony/long/00142_00_30s.ARW'
 './dataset/Sony/long/00014_00_10s.ARW'
 './dataset/Sony/long/00058_00_10s.ARW'
 './dataset/Sony/long/00175_00_30s.ARW'
 './dataset/Sony/long/00166_00_30s.ARW'
 './dataset/Sony/long/00218_00_10s.ARW'
 './dataset/Sony/long/00174_00_30s.ARW'
 './dataset/Sony/long/00060_00_10s.ARW'
 './dataset/Sony/long/00225_00_10s.ARW'
 './dataset/Sony/long/00036_00_10s.ARW'
 './dataset/Sony/long/00128_00_30s.ARW'
 './dataset/Sony/long/00094_00_30s.ARW'
 './dataset/Sony/long/00099_00_30s.ARW'
 './dataset/Sony/long/00231_00_10s.ARW'
 './dataset/Sony/long/00081_00_30s.ARW']
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 263, in <module>
    heteroschedastic_sigma_s_map = np.full(numpy.shape(input_patch), heteroschedastic_sigma_s)
NameError: name 'numpy' is not defined
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 11:55:16
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 11:55:16.152729: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 11:55:16.296966: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 11:55:16.296995: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 11:55:16.583323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 11:55:16.583358: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 11:55:16.583370: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 11:55:16.583467: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
No checkpoint found at ./heteroschedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,result_dir= ./heteroschedastic_sigma_map_checkpoint/ ,len(train_fns)= 161

Cleared all images in memory.

Starting Training on index [ 94  91 118 139 104 126   7  26 119 153  94 145  21 140  12 158]
dataset index: ['./dataset/Sony/long/00182_00_10s.ARW'
 './dataset/Sony/long/00206_00_10s.ARW'
 './dataset/Sony/long/00165_00_30s.ARW'
 './dataset/Sony/long/00146_00_30s.ARW'
 './dataset/Sony/long/00145_00_30s.ARW'
 './dataset/Sony/long/00124_00_30s.ARW'
 './dataset/Sony/long/00078_00_30s.ARW'
 './dataset/Sony/long/00112_00_30s.ARW'
 './dataset/Sony/long/00221_00_10s.ARW'
 './dataset/Sony/long/00158_00_30s.ARW'
 './dataset/Sony/long/00182_00_10s.ARW'
 './dataset/Sony/long/00218_00_10s.ARW'
 './dataset/Sony/long/00143_00_30s.ARW'
 './dataset/Sony/long/00114_00_30s.ARW'
 './dataset/Sony/long/00046_00_10s.ARW'
 './dataset/Sony/long/00047_00_10s.ARW']
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 297, in <module>
    feed_dict={in_image: input_patch, noise_level_map: noise_sigmas_feed, lr: learning_rate})
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1086, in _run
    str(subfeed_t.get_shape())))
ValueError: Cannot feed value of shape (16, 128, 128, 8) for Tensor u'Placeholder_1:0', which has shape '(?, ?, ?, 2)'
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 11:56:37
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 11:56:37.679872: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 11:56:37.815829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 11:56:37.815859: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 11:56:38.097157: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 11:56:38.097193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 11:56:38.097229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 11:56:38.097326: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
No checkpoint found at ./heteroschedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,result_dir= ./heteroschedastic_sigma_map_checkpoint/ ,len(train_fns)= 161

Cleared all images in memory.

Starting Training on index [ 27  19  89 130  47  35 129  63  30 140  71 153 100  54  24 152]
dataset index: ['./dataset/Sony/long/00009_00_10s.ARW'
 './dataset/Sony/long/00202_00_10s.ARW'
 './dataset/Sony/long/00083_00_30s.ARW'
 './dataset/Sony/long/00090_00_30s.ARW'
 './dataset/Sony/long/00160_00_30s.ARW'
 './dataset/Sony/long/00224_00_10s.ARW'
 './dataset/Sony/long/00042_00_10s.ARW'
 './dataset/Sony/long/00127_00_30s.ARW'
 './dataset/Sony/long/00039_00_10s.ARW'
 './dataset/Sony/long/00114_00_30s.ARW'
 './dataset/Sony/long/00225_00_10s.ARW'
 './dataset/Sony/long/00158_00_30s.ARW'
 './dataset/Sony/long/00059_00_10s.ARW'
 './dataset/Sony/long/00051_00_10s.ARW'
 './dataset/Sony/long/00031_00_10s.ARW'
 './dataset/Sony/long/00144_00_30s.ARW']
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 297, in <module>
    feed_dict={in_image: input_patch, noise_level_map: noise_sigmas_feed, lr: learning_rate})
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1308, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors_impl.InvalidArgumentError: Incompatible shapes: [16,128,128,2] vs. [16,1424,2128,2]
	 [[{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:GPU:0"](g_conv10/Maximum, mean_squared_error/ToFloat)]]

Caused by op u'mean_squared_error/SquaredDifference', defined at:
  File "train_heteroscedastic_sigma_map.py", line 121, in <module>
    G_loss = tf.losses.mean_squared_error(noise_level_map, out_noise_level_map)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/ops/losses/losses_impl.py", line 671, in mean_squared_error
    losses = math_ops.squared_difference(predictions, labels)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py", line 8258, in squared_difference
    "SquaredDifference", x=x, y=y, name=name)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py", line 787, in _apply_op_helper
    op_def=op_def)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/util/deprecation.py", line 488, in new_func
    return func(*args, **kwargs)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 3272, in create_op
    op_def=op_def)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py", line 1768, in __init__
    self._traceback = tf_stack.extract_stack()

InvalidArgumentError (see above for traceback): Incompatible shapes: [16,128,128,2] vs. [16,1424,2128,2]
	 [[{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT, _device="/job:localhost/replica:0/task:0/device:GPU:0"](g_conv10/Maximum, mean_squared_error/ToFloat)]]

(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 11:57:55
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 11:57:55.539859: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 11:57:55.675581: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 11:57:55.675613: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 11:57:55.962295: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 11:57:55.962331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 11:57:55.962343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 11:57:55.962439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
No checkpoint found at ./heteroschedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,result_dir= ./heteroschedastic_sigma_map_checkpoint/ ,len(train_fns)= 161

Cleared all images in memory.

Starting Training on index [ 25 120 154  49  98  56  13  25 108  11 152  19 131  50 126  72]
dataset index: ['./dataset/Sony/long/00174_00_30s.ARW'
 './dataset/Sony/long/00012_00_10s.ARW'
 './dataset/Sony/long/00181_00_10s.ARW'
 './dataset/Sony/long/00065_00_10s.ARW'
 './dataset/Sony/long/00096_00_30s.ARW'
 './dataset/Sony/long/00183_00_10s.ARW'
 './dataset/Sony/long/00027_00_10s.ARW'
 './dataset/Sony/long/00174_00_30s.ARW'
 './dataset/Sony/long/00076_00_30s.ARW'
 './dataset/Sony/long/00129_00_30s.ARW'
 './dataset/Sony/long/00144_00_30s.ARW'
 './dataset/Sony/long/00202_00_10s.ARW'
 './dataset/Sony/long/00180_00_10s.ARW'
 './dataset/Sony/long/00024_00_10s.ARW'
 './dataset/Sony/long/00124_00_30s.ARW'
 './dataset/Sony/long/00197_00_10s.ARW']
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 301, in <module>
    g_loss[ind[k],:] = abs(noise_sigmas_feed[k] - output[k])
ValueError: could not broadcast input array from shape (128,128,2) into shape (2)
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:01:01
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:01:01.388421: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:01:01.526188: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:01:01.526218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:01:01.810543: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:01:01.810572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:01:01.810586: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:01:01.810685: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
No checkpoint found at ./heteroschedastic_sigma_map_checkpoint/. Hence, will create the folder.
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 141, in <module>
    g_loss = np.ones((len(train_fns), H, W, 2))
NameError: name 'H' is not defined
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:02:33
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:02:33.165258: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:02:33.305566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:02:33.305599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:02:33.592775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:02:33.592812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:02:33.592828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:02:33.592927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
No checkpoint found at ./heteroschedastic_sigma_map_checkpoint/. Hence, will create the folder.
Killed
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:03:42
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:03:42.096829: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:03:42.231317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:03:42.231352: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:03:44.525156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:03:44.525190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:03:44.525211: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:03:44.525357: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
No checkpoint found at ./heteroschedastic_sigma_map_checkpoint/. Hence, will create the folder.
Killed
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:04:25
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:04:25.768384: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:04:25.903795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:04:25.903831: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:04:26.254011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:04:26.254047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:04:26.254057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:04:26.254203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.
Killed
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:06:26
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:06:26.645363: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:06:26.790045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:06:26.790081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:06:27.146694: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:06:27.146730: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:06:27.146754: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:06:27.146986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,result_dir= ./heteroscedastic_sigma_map_checkpoint/ ,len(train_fns)= 161

Cleared all images in memory.

Starting Training on index [135  78 149  33 102 139 136  32  54 118 160 101  81 116 136   8]
dataset index: ['./dataset/Sony/long/00121_00_30s.ARW'
 './dataset/Sony/long/00108_00_30s.ARW'
 './dataset/Sony/long/00220_00_10s.ARW'
 './dataset/Sony/long/00036_00_10s.ARW'
 './dataset/Sony/long/00151_00_30s.ARW'
 './dataset/Sony/long/00146_00_30s.ARW'
 './dataset/Sony/long/00216_00_10s.ARW'
 './dataset/Sony/long/00212_00_10s.ARW'
 './dataset/Sony/long/00051_00_10s.ARW'
 './dataset/Sony/long/00165_00_30s.ARW'
 './dataset/Sony/long/00219_00_10s.ARW'
 './dataset/Sony/long/00021_00_10s.ARW'
 './dataset/Sony/long/00098_00_30s.ARW'
 './dataset/Sony/long/00149_00_30s.ARW'
 './dataset/Sony/long/00216_00_10s.ARW'
 './dataset/Sony/long/00081_00_30s.ARW']
Killed
(sid2) [ir967@gr064 Learning-to-See-in-the-Dark]$ exit
exit
slurmstepd: error: Detected 4 oom-kill event(s) in step 501635.0 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: gr064-ib0: task 0: Out Of Memory
srun: Terminating job step 501635.0
(base) [ir967@log-3 Learning-to-See-in-the-Dark]$ srun -t3:00:00 --mem=22000 --gres=gpu:1 --pty /bin/bash
(base) [ir967@gr063 Learning-to-See-in-the-Dark]$ conda activate sid2
(sid2) [ir967@gr063 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:08:23
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:08:23.284386: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:08:23.421903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:2f:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:08:23.421940: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:08:25.774933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:08:25.774968: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:08:25.774975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:08:25.775115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:2f:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,result_dir= ./heteroscedastic_sigma_map_checkpoint/ ,len(train_fns)= 161

Cleared all images in memory.

Starting Training on index [119  33  73 111 118 149  87  76  77  13   9 106  90  67 112 154]
dataset index: ['./dataset/Sony/long/00221_00_10s.ARW'
 './dataset/Sony/long/00036_00_10s.ARW'
 './dataset/Sony/long/00137_00_30s.ARW'
 './dataset/Sony/long/00109_00_30s.ARW'
 './dataset/Sony/long/00165_00_30s.ARW'
 './dataset/Sony/long/00220_00_10s.ARW'
 './dataset/Sony/long/00102_00_30s.ARW'
 './dataset/Sony/long/00060_00_10s.ARW'
 './dataset/Sony/long/00194_00_10s.ARW'
 './dataset/Sony/long/00027_00_10s.ARW'
 './dataset/Sony/long/00186_00_10s.ARW'
 './dataset/Sony/long/00196_00_10s.ARW'
 './dataset/Sony/long/00057_00_10s.ARW'
 './dataset/Sony/long/00138_00_30s.ARW'
 './dataset/Sony/long/00062_00_10s.ARW'
 './dataset/Sony/long/00181_00_10s.ARW']
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 305, in <module>
    g_loss[ind[k],yy:yy + ps, xx:xx + ps,:] = abs(noise_sigmas_feed[k] - output[k])
ValueError: could not broadcast input array from shape (128,128,2) into shape (128,29,2)
(sid2) [ir967@gr063 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr063 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:10:22
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:10:22.147149: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:10:22.285504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:2f:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:10:22.285540: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:10:22.570734: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:10:22.570773: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:10:22.570788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:10:22.570883: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:2f:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,result_dir= ./heteroscedastic_sigma_map_checkpoint/ ,len(train_fns)= 161

Cleared all images in memory.

Starting Training on index [ 15  26   5  64  51 128  98  79 160 107 123 126  63  80 113  24]
dataset index: ['./dataset/Sony/long/00044_00_10s.ARW'
 './dataset/Sony/long/00112_00_30s.ARW'
 './dataset/Sony/long/00033_00_10s.ARW'
 './dataset/Sony/long/00013_00_10s.ARW'
 './dataset/Sony/long/00113_00_30s.ARW'
 './dataset/Sony/long/00071_00_10s.ARW'
 './dataset/Sony/long/00096_00_30s.ARW'
 './dataset/Sony/long/00067_00_10s.ARW'
 './dataset/Sony/long/00219_00_10s.ARW'
 './dataset/Sony/long/00122_00_30s.ARW'
 './dataset/Sony/long/00052_00_10s.ARW'
 './dataset/Sony/long/00124_00_30s.ARW'
 './dataset/Sony/long/00127_00_30s.ARW'
 './dataset/Sony/long/00128_00_30s.ARW'
 './dataset/Sony/long/00130_00_30s.ARW'
 './dataset/Sony/long/00031_00_10s.ARW']
rawpy read the 51th file at location: 00113_00_30s.ARW
Killed
(sid2) [ir967@gr063 Learning-to-See-in-the-Dark]$ exit
exit
slurmstepd: error: Detected 1 oom-kill event(s) in step 507949.0 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: gr063-ib0: task 0: Out Of Memory
srun: Terminating job step 507949.0
(base) [ir967@log-3 Learning-to-See-in-the-Dark]$ srun -t3:00:00 --mem=32000 --gres=gpu:1 --pty /bin/bash
(base) [ir967@gr066 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 6, in <module>
    import os, time, scipy.io
ModuleNotFoundError: No module named 'scipy'
(base) [ir967@gr066 Learning-to-See-in-the-Dark]$ conda activate sid2
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:11:19
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:11:19.278499: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:11:19.402057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:11:19.402097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:11:20.489095: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:11:20.489136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:11:20.489196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:11:20.489396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,result_dir= ./heteroscedastic_sigma_map_checkpoint/ ,len(train_fns)= 161

Cleared all images in memory.

Starting Training on index [143 122  76  79 159  47 105  91  41 159  97  59  73 108 138 125]
dataset index: ['./dataset/Sony/long/00205_00_10s.ARW'
 './dataset/Sony/long/00094_00_30s.ARW'
 './dataset/Sony/long/00060_00_10s.ARW'
 './dataset/Sony/long/00067_00_10s.ARW'
 './dataset/Sony/long/00215_00_10s.ARW'
 './dataset/Sony/long/00160_00_30s.ARW'
 './dataset/Sony/long/00189_00_10s.ARW'
 './dataset/Sony/long/00206_00_10s.ARW'
 './dataset/Sony/long/00141_00_30s.ARW'
 './dataset/Sony/long/00215_00_10s.ARW'
 './dataset/Sony/long/00135_00_30s.ARW'
 './dataset/Sony/long/00037_00_10s.ARW'
 './dataset/Sony/long/00137_00_30s.ARW'
 './dataset/Sony/long/00076_00_30s.ARW'
 './dataset/Sony/long/00131_00_30s.ARW'
 './dataset/Sony/long/00209_00_10s.ARW']
rawpy read the 91th file at location: 00206_00_10s.ARW
rawpy read the 41th file at location: 00141_00_30s.ARW
Killed
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:13:02
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:13:02.905733: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:13:03.057025: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:13:03.057065: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:13:03.832490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:13:03.832534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:13:03.832583: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:13:03.832757: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,result_dir= ./heteroscedastic_sigma_map_checkpoint/ ,len(train_fns)= 161

Cleared all images in memory.

Starting Training on index [ 27  61  59  71 131  64 109  90  77 151   3 115  54 106 117  12]
dataset index: ['./dataset/Sony/long/00009_00_10s.ARW'
 './dataset/Sony/long/00169_00_30s.ARW'
 './dataset/Sony/long/00037_00_10s.ARW'
 './dataset/Sony/long/00225_00_10s.ARW'
 './dataset/Sony/long/00180_00_10s.ARW'
 './dataset/Sony/long/00013_00_10s.ARW'
 './dataset/Sony/long/00152_00_30s.ARW'
 './dataset/Sony/long/00057_00_10s.ARW'
 './dataset/Sony/long/00194_00_10s.ARW'
 './dataset/Sony/long/00134_00_30s.ARW'
 './dataset/Sony/long/00132_00_30s.ARW'
 './dataset/Sony/long/00066_00_10s.ARW'
 './dataset/Sony/long/00051_00_10s.ARW'
 './dataset/Sony/long/00196_00_10s.ARW'
 './dataset/Sony/long/00085_00_30s.ARW'
 './dataset/Sony/long/00046_00_10s.ARW']
loading ./dataset/Sony/long/00009_00_10s.ARW; images_in_memory= 0
rawpy read the 27th file at location: 00009_00_10s.ARW
loading ./dataset/Sony/long/00169_00_30s.ARW; images_in_memory= 1
rawpy read the 61th file at location: 00169_00_30s.ARW
loading ./dataset/Sony/long/00037_00_10s.ARW; images_in_memory= 2
rawpy read the 59th file at location: 00037_00_10s.ARW
loading ./dataset/Sony/long/00225_00_10s.ARW; images_in_memory= 3
rawpy read the 71th file at location: 00225_00_10s.ARW
loading ./dataset/Sony/long/00180_00_10s.ARW; images_in_memory= 4
rawpy read the 131th file at location: 00180_00_10s.ARW
loading ./dataset/Sony/long/00152_00_30s.ARW; images_in_memory= 6
rawpy read the 109th file at location: 00152_00_30s.ARW
loading ./dataset/Sony/long/00194_00_10s.ARW; images_in_memory= 8
rawpy read the 77th file at location: 00194_00_10s.ARW
loading ./dataset/Sony/long/00134_00_30s.ARW; images_in_memory= 9
rawpy read the 151th file at location: 00134_00_30s.ARW
loading ./dataset/Sony/long/00132_00_30s.ARW; images_in_memory= 10
rawpy read the 3th file at location: 00132_00_30s.ARW
loading ./dataset/Sony/long/00066_00_10s.ARW; images_in_memory= 11
rawpy read the 115th file at location: 00066_00_10s.ARW
loading ./dataset/Sony/long/00085_00_30s.ARW; images_in_memory= 14
rawpy read the 117th file at location: 00085_00_30s.ARW
Killed
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:16:19
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:16:19.387974: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:16:19.530108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:16:19.530150: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:16:20.123584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:16:20.123624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:16:20.123688: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:16:20.123847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,len(train_fns)= 161 ,
result_dir= ./heteroscedastic_sigma_map_checkpoint/

Cleared all images in memory.

Starting Training on index [133  82  44 115   3  10 106  52 135  46  72  49 101  16  66 130]
dataset index: ['./dataset/Sony/long/00161_00_30s.ARW'
 './dataset/Sony/long/00175_00_30s.ARW'
 './dataset/Sony/long/00095_00_30s.ARW'
 './dataset/Sony/long/00066_00_10s.ARW'
 './dataset/Sony/long/00132_00_30s.ARW'
 './dataset/Sony/long/00038_00_10s.ARW'
 './dataset/Sony/long/00196_00_10s.ARW'
 './dataset/Sony/long/00053_00_10s.ARW'
 './dataset/Sony/long/00121_00_30s.ARW'
 './dataset/Sony/long/00157_00_30s.ARW'
 './dataset/Sony/long/00197_00_10s.ARW'
 './dataset/Sony/long/00065_00_10s.ARW'
 './dataset/Sony/long/00021_00_10s.ARW'
 './dataset/Sony/long/00099_00_30s.ARW'
 './dataset/Sony/long/00142_00_30s.ARW'
 './dataset/Sony/long/00090_00_30s.ARW']
loading ./dataset/Sony/long/00161_00_30s.ARW; images_in_memory= 0
rawpy read the 133th file at location: 00161_00_30s.ARW
loading ./dataset/Sony/long/00175_00_30s.ARW; images_in_memory= 1
loading ./dataset/Sony/long/00095_00_30s.ARW; images_in_memory= 2
loading ./dataset/Sony/long/00066_00_10s.ARW; images_in_memory= 3
rawpy read the 115th file at location: 00066_00_10s.ARW
loading ./dataset/Sony/long/00132_00_30s.ARW; images_in_memory= 4
rawpy read the 3th file at location: 00132_00_30s.ARW
loading ./dataset/Sony/long/00038_00_10s.ARW; images_in_memory= 5
loading ./dataset/Sony/long/00196_00_10s.ARW; images_in_memory= 6
loading ./dataset/Sony/long/00053_00_10s.ARW; images_in_memory= 7
loading ./dataset/Sony/long/00121_00_30s.ARW; images_in_memory= 8
rawpy read the 135th file at location: 00121_00_30s.ARW
loading ./dataset/Sony/long/00157_00_30s.ARW; images_in_memory= 9
loading ./dataset/Sony/long/00197_00_10s.ARW; images_in_memory= 10
loading ./dataset/Sony/long/00065_00_10s.ARW; images_in_memory= 11
rawpy read the 49th file at location: 00065_00_10s.ARW
loading ./dataset/Sony/long/00021_00_10s.ARW; images_in_memory= 12
rawpy read the 101th file at location: 00021_00_10s.ARW
loading ./dataset/Sony/long/00099_00_30s.ARW; images_in_memory= 13
loading ./dataset/Sony/long/00142_00_30s.ARW; images_in_memory= 14
loading ./dataset/Sony/long/00090_00_30s.ARW; images_in_memory= 15
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Killed
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ ls
 5CNN_FC2_low_exposure_images_all.py
 5CNN_FC2_low_exposure_images.py
 absolute_difference.txt
 all_of_gt_Sony
 all_of_gt_Sony_GPU_efficient
 all_of_gt_Sony_GPU_efficient_flattened
 all_of_gt_Sony_GPU_efficient_flattened_3output
 all_of_gt_Sony_GPU_efficient_flattened_3output_more_simpler
 all_of_gt_Sony_GPU_efficient_flattened_3output_more_simpler_print
 all_of_gt_Sony_GPU_efficient_flattened_3output_more_simpler_wider
 all_of_gt_Sony_GPU_efficient_flattened_3output_print_test
 all_of_gt_Sony_GPU_efficient_flattened_3output_simpler
 checkpoint
 check.sh
 cluster_status.txt
 CNN5_FC2_no_log.py
 CNN5_FC3_exposure_101GB.py
 CNN7_FC2_ps32_log_regression.py
 CNN9_FC2_log_regression.py
 CNN9_FC2_ps512_log_regression.py
 CNN9_FC3_exposure_101GB.py
 CNN9_log_regression.py
 conviz.py
 dataset
 dead_simple_logs1.txt
 debug_one_hot.txt
 download_dataset.py
 download_models.py
 efficiency
 exper.py
 exper.sbatch
 FC3_no_log_low_exposure_images_all_lazyload.py
 FC3_no_log_low_exposure_images_all.py
 FC3.py
 filters
 gamma_checkpoint
 gamma_experiment.py
 gamma_piecewise.txt
 gt_Sony_CNN5_FC2_no_log
 gt_Sony_CNN5_FC2_no_log_jitter
 gt_Sony_CNN5_FC2_no_log_low_exposure
 gt_Sony_CNN5_FC2_no_log_low_exposure_all_images
 gt_Sony_CNN5_FC3_exposure_101GB
 gt_Sony_CNN9_FC2_log
 gt_Sony_CNN9_FC2_ps32_log
 gt_Sony_CNN9_FC2_ps512_log
 gt_Sony_CNN9_FC3_1x1Conv_deleted_exposure_101GB_BS64
 gt_Sony_CNN9_FC3_exposure_101GB
 gt_Sony_CNN9_FC3_exposure_101GB_BS64
 gt_Sony_CNN9_log
 gt_Sony_dead_simple
 gt_Sony_dead_simple_3gammas
 gt_Sony_dead_simple_new
 gt_Sony_FC3
 gt_Sony_FC3_16images
 gt_Sony_FC3_exposure_101GB
 gt_Sony_FC3_exposure_lazyload
 gt_Sony_FC3_exposure_lazyload_run2_to_resolve_nan_issue
 gt_Sony_FC3_exposure_lazyload_run3_to_resolve_nan_issue
 gt_Sony_FC3_exposure_lazyload_run4_to_resolve_nan_issue
 gt_Sony_FC3_exposure_lazyload_run5
 gt_Sony_FC3_no_log_low_exposure_all_images
 gt_Sony_FC3_no_log_low_exposure_all_images_run2
 gt_Sony_medium_log_regression_all_images_MSE_lowerLRat1000
 gt_Sony_medium_log_regression_MSE_lowerLRat1000
 gt_Sony_medium_log_regression_MSE_lowerLRat1000_intermediate_output
 gt_Sony_medium_MSE
 gt_Sony_medium_MSE_lowerLRat200_BNeverywhere
 gt_Sony_medium_MSE_lowerLRat500
 gt_Sony_medium_regression_MSE_lowerLRat200_BNeverywhere
 gt_Sony_medium_scaled_regression_MSE_lowerLRat1000
 gt_Sony_medium_scaled_regression_MSE_lowerLRat200
 gt_Sony_resnet_101GB_BS64
 gt_Sony_simple_batched_filters
 gt_Sony_simple_batched_MAE_loss
 gt_Sony_simple_batched_MSE_loss
 gt_Sony_simple_ReLU
 gt_Sony_simple_ReLU_BN
 gt_Sony_simple_ReLU_BN_batched
 gt_Sony_simple_ReLU_BN_batched_new
 gt_Sony_simple_ReLU_BN_new
 gt_Sony_VGG16
 gt_Sony_VGGlike_16Conv_exposure_101GB_BS64
 gt_Sony_VGGlike_16Conv_huber_exposure_101GB_BS64
 half_of_gt_Sony
 heteroscedastic_sigma_map_checkpoint
 heteroschedastic_sigma_checkpoint
 heteroschedastic_sigma_map_checkpoint
 HPC-hw4-1a.sh
 htop.txt
 hyper_parameters.py
 hyper_parameters.pyc
 images
 images-with-gamma-statistics.txt
 LICENSE.md
 logs
 lspci.out
 mean_square_loss.txt
 medium_batched_square_loss.py
 medium_log_regression.py
 medium_regression_square_loss_lr_at_1000.py
 medium_regression_square_loss.py
 NoisePrediction_a_32_50.py
 NoisePrediction_L_64_50.py
 _old_code
 out
 README.md
 resnet_exposure_101GB.py
 resnet.py
 resnet.pyc
 result_Fuji
 result_Sony
'result_Sony__[1-5]_images'
 result_Sony_20_images
 result_Sony_checkDec6
 result_Sony_with_gamma_net
 result_Sony_with_gamma_net_3output
 run1_5_images_result_Sony
 scontrol_8hours.txt
 show.py
 show.pyc
 simple_batched_numpy_diff_loss.py
 simple_batched_numpy.py
 simple_batched.py
 simple_ReLU_BN_Sony.py
 simple_ReLU_Sony.py
 slurm-401646.out
 _slurm_out
 sstat_8hours.txt
 test_for_gamma_Sony_3output.py
 test_for_gamma_Sony_3output.sbatch
 test_for_gamma_Sony.py
 test_for_gamma_Sony.sbatch
 test_Fuji.py
 test_Fuji.sbatch
 test_Sony.py
 test_Sony.sbatch
 test_Sony_with_gamma.py
 test_Sony_with_gamma.sbatch
 train_for_gamma_Sony_3output_more_simpler_wider.py
 train_for_gamma_Sony_3output.py
 train_for_gamma_Sony_3output.sbatch
 train_for_gamma_Sony_dead_simple_3gammas.py
 train_for_gamma_Sony_dead_simple_3gammas.sbatch
 train_for_gamma_Sony_dead_simple.py
 train_for_gamma_Sony_dead_simple.sbatch
 train_for_gamma_Sony.py
 train_for_gamma_Sony.sbatch
 train_Fuji.py
 train_Fuji.sbatch
 train_heteroscedastic_sigma_map.py
 train_heteroschedastic_sigma.py
 train_Sony.py
 train_Sony.sbatch
 utils.py
 utils.pyc
 VGG-16.py
 VGG19like_16Conv_exposure_101GB.py
 VGG19like_16Conv_huber_exposure_101GB.py
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ ls heteroscedastic_sigma_map_checkpoint
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ cd heteroscedastic_sigma_map_checkpoint
(sid2) [ir967@gr066 heteroscedastic_sigma_map_checkpoint]$ ls
(sid2) [ir967@gr066 heteroscedastic_sigma_map_checkpoint]$ cd ..
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ ls *code*
all_of_gt_Sony_avgpool
all_of_gt_Sony_avgpool_2convlayers
all_of_gt_Sony_GPU_efficient_flattened_1output
all_of_gt_Sony_GPU_efficient_flattened_1output_value
test_Sony_with_gamma_unflattened.py
train_for_gamma_Sony_1output.py
train_for_gamma_Sony_1output.sbatch
train_for_gamma_Sony_1output_value.py
train_for_gamma_Sony_1output_value.sbatch
train_for_gamma_Sony_3output_more_simpler.py
train_for_gamma_Sony_3output_more_simpler.sbatch
train_for_gamma_Sony_3output_simpler.py
train_for_gamma_Sony_3output_simpler.sbatch
train_for_gamma_Sony_avgpool_2convlayers.py
train_for_gamma_Sony_avgpool.py
train_for_gamma_Sony_clean_code_not_working.py
train_for_gamma_Sony_unflattened.py
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ mkdir _old_code
mkdir: cannot create directory ‘_old_code’: File exists
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ mv *.py _old_code/
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ mv *.sbatch _old_code/
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ ls
 absolute_difference.txt
 all_of_gt_Sony
 all_of_gt_Sony_GPU_efficient
 all_of_gt_Sony_GPU_efficient_flattened
 all_of_gt_Sony_GPU_efficient_flattened_3output
 all_of_gt_Sony_GPU_efficient_flattened_3output_more_simpler
 all_of_gt_Sony_GPU_efficient_flattened_3output_more_simpler_print
 all_of_gt_Sony_GPU_efficient_flattened_3output_more_simpler_wider
 all_of_gt_Sony_GPU_efficient_flattened_3output_print_test
 all_of_gt_Sony_GPU_efficient_flattened_3output_simpler
 checkpoint
 check.sh
 cluster_status.txt
 dataset
 dead_simple_logs1.txt
 debug_one_hot.txt
 efficiency
 filters
 gamma_checkpoint
 gamma_piecewise.txt
 gt_Sony_CNN5_FC2_no_log
 gt_Sony_CNN5_FC2_no_log_jitter
 gt_Sony_CNN5_FC2_no_log_low_exposure
 gt_Sony_CNN5_FC2_no_log_low_exposure_all_images
 gt_Sony_CNN5_FC3_exposure_101GB
 gt_Sony_CNN9_FC2_log
 gt_Sony_CNN9_FC2_ps32_log
 gt_Sony_CNN9_FC2_ps512_log
 gt_Sony_CNN9_FC3_1x1Conv_deleted_exposure_101GB_BS64
 gt_Sony_CNN9_FC3_exposure_101GB
 gt_Sony_CNN9_FC3_exposure_101GB_BS64
 gt_Sony_CNN9_log
 gt_Sony_dead_simple
 gt_Sony_dead_simple_3gammas
 gt_Sony_dead_simple_new
 gt_Sony_FC3
 gt_Sony_FC3_16images
 gt_Sony_FC3_exposure_101GB
 gt_Sony_FC3_exposure_lazyload
 gt_Sony_FC3_exposure_lazyload_run2_to_resolve_nan_issue
 gt_Sony_FC3_exposure_lazyload_run3_to_resolve_nan_issue
 gt_Sony_FC3_exposure_lazyload_run4_to_resolve_nan_issue
 gt_Sony_FC3_exposure_lazyload_run5
 gt_Sony_FC3_no_log_low_exposure_all_images
 gt_Sony_FC3_no_log_low_exposure_all_images_run2
 gt_Sony_medium_log_regression_all_images_MSE_lowerLRat1000
 gt_Sony_medium_log_regression_MSE_lowerLRat1000
 gt_Sony_medium_log_regression_MSE_lowerLRat1000_intermediate_output
 gt_Sony_medium_MSE
 gt_Sony_medium_MSE_lowerLRat200_BNeverywhere
 gt_Sony_medium_MSE_lowerLRat500
 gt_Sony_medium_regression_MSE_lowerLRat200_BNeverywhere
 gt_Sony_medium_scaled_regression_MSE_lowerLRat1000
 gt_Sony_medium_scaled_regression_MSE_lowerLRat200
 gt_Sony_resnet_101GB_BS64
 gt_Sony_simple_batched_filters
 gt_Sony_simple_batched_MAE_loss
 gt_Sony_simple_batched_MSE_loss
 gt_Sony_simple_ReLU
 gt_Sony_simple_ReLU_BN
 gt_Sony_simple_ReLU_BN_batched
 gt_Sony_simple_ReLU_BN_batched_new
 gt_Sony_simple_ReLU_BN_new
 gt_Sony_VGG16
 gt_Sony_VGGlike_16Conv_exposure_101GB_BS64
 gt_Sony_VGGlike_16Conv_huber_exposure_101GB_BS64
 half_of_gt_Sony
 heteroscedastic_sigma_map_checkpoint
 heteroschedastic_sigma_checkpoint
 heteroschedastic_sigma_map_checkpoint
 HPC-hw4-1a.sh
 htop.txt
 hyper_parameters.pyc
 images
 images-with-gamma-statistics.txt
 LICENSE.md
 logs
 lspci.out
 mean_square_loss.txt
 _old_code
 out
 README.md
 resnet.pyc
 result_Fuji
 result_Sony
'result_Sony__[1-5]_images'
 result_Sony_20_images
 result_Sony_checkDec6
 result_Sony_with_gamma_net
 result_Sony_with_gamma_net_3output
 run1_5_images_result_Sony
 scontrol_8hours.txt
 show.pyc
 slurm-401646.out
 _slurm_out
 sstat_8hours.txt
 utils.pyc
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ mkdir _old_gt_Sony
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ mv *gt_Sony* _old_gt_Sony/
mv: cannot move '_old_gt_Sony' to a subdirectory of itself, '_old_gt_Sony/_old_gt_Sony'
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ mv gt_Sony* _old_gt_Sony/
mv: cannot stat 'gt_Sony*': No such file or directory
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ ls _old_
ls: cannot access '_old_': No such file or directory
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ ls _old_gt_Sony/
all_of_gt_Sony
all_of_gt_Sony_GPU_efficient
all_of_gt_Sony_GPU_efficient_flattened
all_of_gt_Sony_GPU_efficient_flattened_3output
all_of_gt_Sony_GPU_efficient_flattened_3output_more_simpler
all_of_gt_Sony_GPU_efficient_flattened_3output_more_simpler_print
all_of_gt_Sony_GPU_efficient_flattened_3output_more_simpler_wider
all_of_gt_Sony_GPU_efficient_flattened_3output_print_test
all_of_gt_Sony_GPU_efficient_flattened_3output_simpler
gt_Sony_CNN5_FC2_no_log
gt_Sony_CNN5_FC2_no_log_jitter
gt_Sony_CNN5_FC2_no_log_low_exposure
gt_Sony_CNN5_FC2_no_log_low_exposure_all_images
gt_Sony_CNN5_FC3_exposure_101GB
gt_Sony_CNN9_FC2_log
gt_Sony_CNN9_FC2_ps32_log
gt_Sony_CNN9_FC2_ps512_log
gt_Sony_CNN9_FC3_1x1Conv_deleted_exposure_101GB_BS64
gt_Sony_CNN9_FC3_exposure_101GB
gt_Sony_CNN9_FC3_exposure_101GB_BS64
gt_Sony_CNN9_log
gt_Sony_dead_simple
gt_Sony_dead_simple_3gammas
gt_Sony_dead_simple_new
gt_Sony_FC3
gt_Sony_FC3_16images
gt_Sony_FC3_exposure_101GB
gt_Sony_FC3_exposure_lazyload
gt_Sony_FC3_exposure_lazyload_run2_to_resolve_nan_issue
gt_Sony_FC3_exposure_lazyload_run3_to_resolve_nan_issue
gt_Sony_FC3_exposure_lazyload_run4_to_resolve_nan_issue
gt_Sony_FC3_exposure_lazyload_run5
gt_Sony_FC3_no_log_low_exposure_all_images
gt_Sony_FC3_no_log_low_exposure_all_images_run2
gt_Sony_medium_log_regression_all_images_MSE_lowerLRat1000
gt_Sony_medium_log_regression_MSE_lowerLRat1000
gt_Sony_medium_log_regression_MSE_lowerLRat1000_intermediate_output
gt_Sony_medium_MSE
gt_Sony_medium_MSE_lowerLRat200_BNeverywhere
gt_Sony_medium_MSE_lowerLRat500
gt_Sony_medium_regression_MSE_lowerLRat200_BNeverywhere
gt_Sony_medium_scaled_regression_MSE_lowerLRat1000
gt_Sony_medium_scaled_regression_MSE_lowerLRat200
gt_Sony_resnet_101GB_BS64
gt_Sony_simple_batched_filters
gt_Sony_simple_batched_MAE_loss
gt_Sony_simple_batched_MSE_loss
gt_Sony_simple_ReLU
gt_Sony_simple_ReLU_BN
gt_Sony_simple_ReLU_BN_batched
gt_Sony_simple_ReLU_BN_batched_new
gt_Sony_simple_ReLU_BN_new
gt_Sony_VGG16
gt_Sony_VGGlike_16Conv_exposure_101GB_BS64
gt_Sony_VGGlike_16Conv_huber_exposure_101GB_BS64
half_of_gt_Sony
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ ls
 absolute_difference.txt                 lspci.out
 checkpoint                              mean_square_loss.txt
 check.sh                                _old_code
 cluster_status.txt                      _old_gt_Sony
 dataset                                 out
 dead_simple_logs1.txt                   README.md
 debug_one_hot.txt                       resnet.pyc
 efficiency                              result_Fuji
 filters                                 result_Sony
 gamma_checkpoint                       'result_Sony__[1-5]_images'
 gamma_piecewise.txt                     result_Sony_20_images
 heteroscedastic_sigma_map_checkpoint    result_Sony_checkDec6
 heteroschedastic_sigma_checkpoint       result_Sony_with_gamma_net
 heteroschedastic_sigma_map_checkpoint   result_Sony_with_gamma_net_3output
 HPC-hw4-1a.sh                           run1_5_images_result_Sony
 htop.txt                                scontrol_8hours.txt
 hyper_parameters.pyc                    show.pyc
 images                                  slurm-401646.out
 images-with-gamma-statistics.txt        _slurm_out
 LICENSE.md                              sstat_8hours.txt
 logs                                    utils.pyc
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ mv result_Sony* _old_gt_Sony/
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ ls
absolute_difference.txt                images-with-gamma-statistics.txt
checkpoint                             LICENSE.md
check.sh                               logs
cluster_status.txt                     lspci.out
dataset                                mean_square_loss.txt
dead_simple_logs1.txt                  _old_code
debug_one_hot.txt                      _old_gt_Sony
efficiency                             out
filters                                README.md
gamma_checkpoint                       resnet.pyc
gamma_piecewise.txt                    result_Fuji
heteroscedastic_sigma_map_checkpoint   run1_5_images_result_Sony
heteroschedastic_sigma_checkpoint      scontrol_8hours.txt
heteroschedastic_sigma_map_checkpoint  show.pyc
HPC-hw4-1a.sh                          slurm-401646.out
htop.txt                               _slurm_out
hyper_parameters.pyc                   sstat_8hours.txt
images                                 utils.pyc
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ mv *.txt logs/
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ ls
checkpoint                             HPC-hw4-1a.sh         README.md
check.sh                               hyper_parameters.pyc  resnet.pyc
dataset                                images                result_Fuji
efficiency                             LICENSE.md            run1_5_images_result_Sony
filters                                logs                  show.pyc
gamma_checkpoint                       lspci.out             slurm-401646.out
heteroscedastic_sigma_map_checkpoint   _old_code             _slurm_out
heteroschedastic_sigma_checkpoint      _old_gt_Sony          utils.pyc
heteroschedastic_sigma_map_checkpoint  out
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:23:02
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:23:02.124044: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:23:02.273493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:23:02.273536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:23:04.113520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:23:04.113559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:23:04.113589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:23:04.113751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,len(train_fns)= 161 ,
result_dir= ./heteroscedastic_sigma_map_checkpoint/

Cleared all images in memory.

Starting Training on index [ 66  33 142  70  83  94  74  53 143 105 139 150  88 120 129 150]
dataset index: ['./dataset/Sony/long/00142_00_30s.ARW'
 './dataset/Sony/long/00036_00_10s.ARW'
 './dataset/Sony/long/00166_00_30s.ARW'
 './dataset/Sony/long/00164_00_30s.ARW'
 './dataset/Sony/long/00195_00_10s.ARW'
 './dataset/Sony/long/00182_00_10s.ARW'
 './dataset/Sony/long/00150_00_30s.ARW'
 './dataset/Sony/long/00086_00_30s.ARW'
 './dataset/Sony/long/00205_00_10s.ARW'
 './dataset/Sony/long/00189_00_10s.ARW'
 './dataset/Sony/long/00146_00_30s.ARW'
 './dataset/Sony/long/00156_00_30s.ARW'
 './dataset/Sony/long/00110_00_30s.ARW'
 './dataset/Sony/long/00012_00_10s.ARW'
 './dataset/Sony/long/00042_00_10s.ARW'
 './dataset/Sony/long/00156_00_30s.ARW']
loading ./dataset/Sony/long/00142_00_30s.ARW; images_in_memory= 0
loading ./dataset/Sony/long/00036_00_10s.ARW; images_in_memory= 1
rawpy read the 33th file at location: 00036_00_10s.ARW
loading ./dataset/Sony/long/00166_00_30s.ARW; images_in_memory= 2
loading ./dataset/Sony/long/00164_00_30s.ARW; images_in_memory= 3
loading ./dataset/Sony/long/00195_00_10s.ARW; images_in_memory= 4
rawpy read the 83th file at location: 00195_00_10s.ARW
loading ./dataset/Sony/long/00182_00_10s.ARW; images_in_memory= 5
loading ./dataset/Sony/long/00150_00_30s.ARW; images_in_memory= 6
loading ./dataset/Sony/long/00086_00_30s.ARW; images_in_memory= 7
rawpy read the 53th file at location: 00086_00_30s.ARW
loading ./dataset/Sony/long/00205_00_10s.ARW; images_in_memory= 8
rawpy read the 143th file at location: 00205_00_10s.ARW
loading ./dataset/Sony/long/00189_00_10s.ARW; images_in_memory= 9
rawpy read the 105th file at location: 00189_00_10s.ARW
loading ./dataset/Sony/long/00146_00_30s.ARW; images_in_memory= 10
rawpy read the 139th file at location: 00146_00_30s.ARW
loading ./dataset/Sony/long/00156_00_30s.ARW; images_in_memory= 11
loading ./dataset/Sony/long/00110_00_30s.ARW; images_in_memory= 12
loading ./dataset/Sony/long/00012_00_10s.ARW; images_in_memory= 13
loading ./dataset/Sony/long/00042_00_10s.ARW; images_in_memory= 14
rawpy read the 129th file at location: 00042_00_10s.ARW
(16, 128, 128, 4)
(161, 1424, 2128, 2)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Killed
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:24:28
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:24:28.476712: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:24:28.615937: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:24:28.615982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:24:29.510908: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:24:29.510948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:24:29.510998: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:24:29.511169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,len(train_fns)= 161 ,
result_dir= ./heteroscedastic_sigma_map_checkpoint/

Cleared all images in memory.

Starting Training on index [ 57  20 105  86 132  41 131  85  72  58 149  93 100  32 105  86]
dataset index: ['./dataset/Sony/long/00184_00_10s.ARW'
 './dataset/Sony/long/00072_00_30s.ARW'
 './dataset/Sony/long/00189_00_10s.ARW'
 './dataset/Sony/long/00207_00_10s.ARW'
 './dataset/Sony/long/00097_00_30s.ARW'
 './dataset/Sony/long/00141_00_30s.ARW'
 './dataset/Sony/long/00180_00_10s.ARW'
 './dataset/Sony/long/00154_00_30s.ARW'
 './dataset/Sony/long/00197_00_10s.ARW'
 './dataset/Sony/long/00056_00_10s.ARW'
 './dataset/Sony/long/00220_00_10s.ARW'
 './dataset/Sony/long/00050_00_10s.ARW'
 './dataset/Sony/long/00059_00_10s.ARW'
 './dataset/Sony/long/00212_00_10s.ARW'
 './dataset/Sony/long/00189_00_10s.ARW'
 './dataset/Sony/long/00207_00_10s.ARW']
loading ./dataset/Sony/long/00184_00_10s.ARW; images_in_memory= 0
rawpy read the 57th file at location: 00184_00_10s.ARW
loading ./dataset/Sony/long/00072_00_30s.ARW; images_in_memory= 1
loading ./dataset/Sony/long/00189_00_10s.ARW; images_in_memory= 2
rawpy read the 105th file at location: 00189_00_10s.ARW
loading ./dataset/Sony/long/00207_00_10s.ARW; images_in_memory= 3
loading ./dataset/Sony/long/00097_00_30s.ARW; images_in_memory= 4
loading ./dataset/Sony/long/00141_00_30s.ARW; images_in_memory= 5
rawpy read the 41th file at location: 00141_00_30s.ARW
loading ./dataset/Sony/long/00180_00_10s.ARW; images_in_memory= 6
rawpy read the 131th file at location: 00180_00_10s.ARW
loading ./dataset/Sony/long/00154_00_30s.ARW; images_in_memory= 7
rawpy read the 85th file at location: 00154_00_30s.ARW
loading ./dataset/Sony/long/00197_00_10s.ARW; images_in_memory= 8
loading ./dataset/Sony/long/00056_00_10s.ARW; images_in_memory= 9
loading ./dataset/Sony/long/00220_00_10s.ARW; images_in_memory= 10
rawpy read the 149th file at location: 00220_00_10s.ARW
loading ./dataset/Sony/long/00050_00_10s.ARW; images_in_memory= 11
rawpy read the 93th file at location: 00050_00_10s.ARW
loading ./dataset/Sony/long/00059_00_10s.ARW; images_in_memory= 12
loading ./dataset/Sony/long/00212_00_10s.ARW; images_in_memory= 13
(16, 128, 128, 4)
(161, 1424, 2128, 2)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Killed
(sid2) [ir967@gr066 Learning-to-See-in-the-Dark]$ exit
exit
slurmstepd: error: Detected 5 oom-kill event(s) in step 508052.0 cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: gr066-ib0: task 0: Out Of Memory
srun: Terminating job step 508052.0
(base) [ir967@log-3 Learning-to-See-in-the-Dark]$ srun -t3:00:00 --mem=18000 --gres=gpu:1 --pty /bin/bash
(base) [ir967@gr013 Learning-to-See-in-the-Dark]$ conda activate sid2
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:36:22
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:36:22.706058: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:36:22.823664: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:36:22.823703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:36:25.454835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:36:25.454875: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:36:25.454913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:36:25.455068: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,len(train_fns)= 161 ,
result_dir= ./heteroscedastic_sigma_map_checkpoint/

Cleared all images in memory.

Starting Training on index [ 89  31 100 158 140 124 138  31  17  57 159   2 118 148  12  24]
dataset index: ['./dataset/Sony/long/00083_00_30s.ARW'
 './dataset/Sony/long/00230_00_10s.ARW'
 './dataset/Sony/long/00059_00_10s.ARW'
 './dataset/Sony/long/00047_00_10s.ARW'
 './dataset/Sony/long/00114_00_30s.ARW'
 './dataset/Sony/long/00010_00_10s.ARW'
 './dataset/Sony/long/00131_00_30s.ARW'
 './dataset/Sony/long/00230_00_10s.ARW'
 './dataset/Sony/long/00070_00_10s.ARW'
 './dataset/Sony/long/00184_00_10s.ARW'
 './dataset/Sony/long/00215_00_10s.ARW'
 './dataset/Sony/long/00029_00_10s.ARW'
 './dataset/Sony/long/00165_00_30s.ARW'
 './dataset/Sony/long/00001_00_10s.ARW'
 './dataset/Sony/long/00046_00_10s.ARW'
 './dataset/Sony/long/00031_00_10s.ARW']
loading ./dataset/Sony/long/00083_00_30s.ARW; images_in_memory= 0
rawpy read the 89th file at location: 00083_00_30s.ARW
loading ./dataset/Sony/long/00230_00_10s.ARW; images_in_memory= 1
rawpy read the 31th file at location: 00230_00_10s.ARW
loading ./dataset/Sony/long/00059_00_10s.ARW; images_in_memory= 2
loading ./dataset/Sony/long/00047_00_10s.ARW; images_in_memory= 3
loading ./dataset/Sony/long/00114_00_30s.ARW; images_in_memory= 4
loading ./dataset/Sony/long/00010_00_10s.ARW; images_in_memory= 5
loading ./dataset/Sony/long/00131_00_30s.ARW; images_in_memory= 6
loading ./dataset/Sony/long/00070_00_10s.ARW; images_in_memory= 7
rawpy read the 17th file at location: 00070_00_10s.ARW
loading ./dataset/Sony/long/00184_00_10s.ARW; images_in_memory= 8
rawpy read the 57th file at location: 00184_00_10s.ARW
loading ./dataset/Sony/long/00215_00_10s.ARW; images_in_memory= 9
rawpy read the 159th file at location: 00215_00_10s.ARW
loading ./dataset/Sony/long/00029_00_10s.ARW; images_in_memory= 10
loading ./dataset/Sony/long/00165_00_30s.ARW; images_in_memory= 11
loading ./dataset/Sony/long/00001_00_10s.ARW; images_in_memory= 12
loading ./dataset/Sony/long/00046_00_10s.ARW; images_in_memory= 13
loading ./dataset/Sony/long/00031_00_10s.ARW; images_in_memory= 14
(16, 128, 128, 4)
(161, 1, 1, 2)
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 307, in <module>
    g_loss[ind[k], 1,1, :] = abs(np.mean(np.mean((noise_sigmas_feed[k] - output[k]), axis = 1), axis=1))
IndexError: index 1 is out of bounds for axis 1 with size 1
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:37:14
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:37:14.583728: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:37:14.720360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:37:14.720394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:37:15.005536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:37:15.005575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:37:15.005595: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:37:15.005700: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,len(train_fns)= 161 ,
result_dir= ./heteroscedastic_sigma_map_checkpoint/

Cleared all images in memory.

Starting Training on index [126 145 100 104 130 151  65  14  70  90  51 149  85 126   1  53]
dataset index: ['./dataset/Sony/long/00124_00_30s.ARW'
 './dataset/Sony/long/00218_00_10s.ARW'
 './dataset/Sony/long/00059_00_10s.ARW'
 './dataset/Sony/long/00145_00_30s.ARW'
 './dataset/Sony/long/00090_00_30s.ARW'
 './dataset/Sony/long/00134_00_30s.ARW'
 './dataset/Sony/long/00159_00_30s.ARW'
 './dataset/Sony/long/00119_00_30s.ARW'
 './dataset/Sony/long/00164_00_30s.ARW'
 './dataset/Sony/long/00057_00_10s.ARW'
 './dataset/Sony/long/00113_00_30s.ARW'
 './dataset/Sony/long/00220_00_10s.ARW'
 './dataset/Sony/long/00154_00_30s.ARW'
 './dataset/Sony/long/00124_00_30s.ARW'
 './dataset/Sony/long/00148_00_30s.ARW'
 './dataset/Sony/long/00086_00_30s.ARW']
loading ./dataset/Sony/long/00124_00_30s.ARW; images_in_memory= 0
loading ./dataset/Sony/long/00218_00_10s.ARW; images_in_memory= 1
rawpy read the 145th file at location: 00218_00_10s.ARW
loading ./dataset/Sony/long/00059_00_10s.ARW; images_in_memory= 2
loading ./dataset/Sony/long/00145_00_30s.ARW; images_in_memory= 3
loading ./dataset/Sony/long/00090_00_30s.ARW; images_in_memory= 4
loading ./dataset/Sony/long/00134_00_30s.ARW; images_in_memory= 5
rawpy read the 151th file at location: 00134_00_30s.ARW
loading ./dataset/Sony/long/00159_00_30s.ARW; images_in_memory= 6
rawpy read the 65th file at location: 00159_00_30s.ARW
loading ./dataset/Sony/long/00119_00_30s.ARW; images_in_memory= 7
loading ./dataset/Sony/long/00164_00_30s.ARW; images_in_memory= 8
loading ./dataset/Sony/long/00057_00_10s.ARW; images_in_memory= 9
loading ./dataset/Sony/long/00113_00_30s.ARW; images_in_memory= 10
rawpy read the 51th file at location: 00113_00_30s.ARW
loading ./dataset/Sony/long/00220_00_10s.ARW; images_in_memory= 11
rawpy read the 149th file at location: 00220_00_10s.ARW
loading ./dataset/Sony/long/00154_00_30s.ARW; images_in_memory= 12
rawpy read the 85th file at location: 00154_00_30s.ARW
loading ./dataset/Sony/long/00148_00_30s.ARW; images_in_memory= 13
rawpy read the 1th file at location: 00148_00_30s.ARW
loading ./dataset/Sony/long/00086_00_30s.ARW; images_in_memory= 14
rawpy read the 53th file at location: 00086_00_30s.ARW
(16, 128, 128, 4)
(161, 1, 1, 2)
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 307, in <module>
    g_loss[ind[k], 0,0, :] = abs(np.mean(np.mean((noise_sigmas_feed[k] - output[k]), axis = 1), axis=1))
ValueError: could not broadcast input array from shape (128) into shape (2)
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:40:28
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:40:28.021667: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:40:28.158706: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:40:28.158740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:40:28.444573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:40:28.444612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:40:28.444641: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:40:28.444748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,len(train_fns)= 161 ,
result_dir= ./heteroscedastic_sigma_map_checkpoint/

Cleared all images in memory.

Starting Training on index [ 99  89  12 104 149 138   0  61  55  65 138 101 108  54 154 103]
dataset index: ['./dataset/Sony/long/00028_00_10s.ARW'
 './dataset/Sony/long/00083_00_30s.ARW'
 './dataset/Sony/long/00046_00_10s.ARW'
 './dataset/Sony/long/00145_00_30s.ARW'
 './dataset/Sony/long/00220_00_10s.ARW'
 './dataset/Sony/long/00131_00_30s.ARW'
 './dataset/Sony/long/00018_00_10s.ARW'
 './dataset/Sony/long/00169_00_30s.ARW'
 './dataset/Sony/long/00023_00_10s.ARW'
 './dataset/Sony/long/00159_00_30s.ARW'
 './dataset/Sony/long/00131_00_30s.ARW'
 './dataset/Sony/long/00021_00_10s.ARW'
 './dataset/Sony/long/00076_00_30s.ARW'
 './dataset/Sony/long/00051_00_10s.ARW'
 './dataset/Sony/long/00181_00_10s.ARW'
 './dataset/Sony/long/00168_00_30s.ARW']
loading ./dataset/Sony/long/00028_00_10s.ARW; images_in_memory= 0
rawpy read the 99th file at location: 00028_00_10s.ARW
loading ./dataset/Sony/long/00083_00_30s.ARW; images_in_memory= 1
rawpy read the 89th file at location: 00083_00_30s.ARW
loading ./dataset/Sony/long/00046_00_10s.ARW; images_in_memory= 2
loading ./dataset/Sony/long/00145_00_30s.ARW; images_in_memory= 3
loading ./dataset/Sony/long/00220_00_10s.ARW; images_in_memory= 4
rawpy read the 149th file at location: 00220_00_10s.ARW
loading ./dataset/Sony/long/00131_00_30s.ARW; images_in_memory= 5
loading ./dataset/Sony/long/00018_00_10s.ARW; images_in_memory= 6
loading ./dataset/Sony/long/00169_00_30s.ARW; images_in_memory= 7
rawpy read the 61th file at location: 00169_00_30s.ARW
loading ./dataset/Sony/long/00023_00_10s.ARW; images_in_memory= 8
rawpy read the 55th file at location: 00023_00_10s.ARW
loading ./dataset/Sony/long/00159_00_30s.ARW; images_in_memory= 9
rawpy read the 65th file at location: 00159_00_30s.ARW
loading ./dataset/Sony/long/00021_00_10s.ARW; images_in_memory= 10
rawpy read the 101th file at location: 00021_00_10s.ARW
loading ./dataset/Sony/long/00076_00_30s.ARW; images_in_memory= 11
loading ./dataset/Sony/long/00051_00_10s.ARW; images_in_memory= 12
loading ./dataset/Sony/long/00181_00_10s.ARW; images_in_memory= 13
loading ./dataset/Sony/long/00168_00_30s.ARW; images_in_memory= 14
rawpy read the 103th file at location: 00168_00_30s.ARW
(16, 128, 128, 4)
(161, 1, 1, 2)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Epoch 0: at batch 1: Training dataset Loss=0.910144, Batch Time=1.178
Epoch 0: at batch 1: Training dataset Loss=0.910144, Batch Time=1.178; Early rounds
loading ./dataset/Sony/long/00075_00_30s.ARW; images_in_memory= 15
rawpy read the 127th file at location: 00075_00_30s.ARW
loading ./dataset/Sony/long/00151_00_30s.ARW; images_in_memory= 16
loading ./dataset/Sony/long/00150_00_30s.ARW; images_in_memory= 17
loading ./dataset/Sony/long/00070_00_10s.ARW; images_in_memory= 18
rawpy read the 17th file at location: 00070_00_10s.ARW
loading ./dataset/Sony/long/00086_00_30s.ARW; images_in_memory= 19
rawpy read the 53th file at location: 00086_00_30s.ARW
loading ./dataset/Sony/long/00012_00_10s.ARW; images_in_memory= 20
loading ./dataset/Sony/long/00123_00_30s.ARW; images_in_memory= 21
loading ./dataset/Sony/long/00037_00_10s.ARW; images_in_memory= 22
rawpy read the 59th file at location: 00037_00_10s.ARW
loading ./dataset/Sony/long/00118_00_30s.ARW; images_in_memory= 23
loading ./dataset/Sony/long/00052_00_10s.ARW; images_in_memory= 24
rawpy read the 123th file at location: 00052_00_10s.ARW
loading ./dataset/Sony/long/00152_00_30s.ARW; images_in_memory= 25
rawpy read the 109th file at location: 00152_00_30s.ARW
loading ./dataset/Sony/long/00205_00_10s.ARW; images_in_memory= 26
rawpy read the 143th file at location: 00205_00_10s.ARW
loading ./dataset/Sony/long/00136_00_30s.ARW; images_in_memory= 27
rawpy read the 75th file at location: 00136_00_30s.ARW
(16, 128, 128, 4)
(161, 1, 1, 2)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Epoch 0: at batch 2: Training dataset Loss=0.833860, Batch Time=0.126; Early rounds
loading ./dataset/Sony/long/00119_00_30s.ARW; images_in_memory= 28
loading ./dataset/Sony/long/00050_00_10s.ARW; images_in_memory= 29
rawpy read the 93th file at location: 00050_00_10s.ARW
loading ./dataset/Sony/long/00154_00_30s.ARW; images_in_memory= 30
rawpy read the 85th file at location: 00154_00_30s.ARW
loading ./dataset/Sony/long/00038_00_10s.ARW; images_in_memory= 31
loading ./dataset/Sony/long/00121_00_30s.ARW; images_in_memory= 32
rawpy read the 135th file at location: 00121_00_30s.ARW
loading ./dataset/Sony/long/00128_00_30s.ARW; images_in_memory= 33
loading ./dataset/Sony/long/00081_00_30s.ARW; images_in_memory= 34
loading ./dataset/Sony/long/00132_00_30s.ARW; images_in_memory= 35
rawpy read the 3th file at location: 00132_00_30s.ARW
loading ./dataset/Sony/long/00127_00_30s.ARW; images_in_memory= 36
rawpy read the 63th file at location: 00127_00_30s.ARW
loading ./dataset/Sony/long/00186_00_10s.ARW; images_in_memory= 37
rawpy read the 9th file at location: 00186_00_10s.ARW
loading ./dataset/Sony/long/00166_00_30s.ARW; images_in_memory= 38
loading ./dataset/Sony/long/00067_00_10s.ARW; images_in_memory= 39
rawpy read the 79th file at location: 00067_00_10s.ARW
loading ./dataset/Sony/long/00143_00_30s.ARW; images_in_memory= 40
rawpy read the 21th file at location: 00143_00_30s.ARW
loading ./dataset/Sony/long/00010_00_10s.ARW; images_in_memory= 41
loading ./dataset/Sony/long/00122_00_30s.ARW; images_in_memory= 42
rawpy read the 107th file at location: 00122_00_30s.ARW
(16, 128, 128, 4)
(161, 1, 1, 2)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Epoch 0: at batch 3: Training dataset Loss=0.748802, Batch Time=0.124; Early rounds
loading ./dataset/Sony/long/00092_00_30s.ARW; images_in_memory= 43
rawpy read the 45th file at location: 00092_00_30s.ARW
loading ./dataset/Sony/long/00063_00_10s.ARW; images_in_memory= 44
loading ./dataset/Sony/long/00157_00_30s.ARW; images_in_memory= 45
loading ./dataset/Sony/long/00112_00_30s.ARW; images_in_memory= 46
loading ./dataset/Sony/long/00124_00_30s.ARW; images_in_memory= 47
loading ./dataset/Sony/long/00146_00_30s.ARW; images_in_memory= 48
rawpy read the 139th file at location: 00146_00_30s.ARW
loading ./dataset/Sony/long/00042_00_10s.ARW; images_in_memory= 49
rawpy read the 129th file at location: 00042_00_10s.ARW
loading ./dataset/Sony/long/00190_00_10s.ARW; images_in_memory= 50
(16, 128, 128, 4)
(161, 1, 1, 2)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Epoch 0: at batch 4: Training dataset Loss=0.697827, Batch Time=0.125; Early rounds
loading ./dataset/Sony/long/00059_00_10s.ARW; images_in_memory= 51
loading ./dataset/Sony/long/00065_00_10s.ARW; images_in_memory= 52
rawpy read the 49th file at location: 00065_00_10s.ARW
loading ./dataset/Sony/long/00096_00_30s.ARW; images_in_memory= 53
loading ./dataset/Sony/long/00064_00_10s.ARW; images_in_memory= 54
rawpy read the 39th file at location: 00064_00_10s.ARW
loading ./dataset/Sony/long/00218_00_10s.ARW; images_in_memory= 55
rawpy read the 145th file at location: 00218_00_10s.ARW
loading ./dataset/Sony/long/00179_00_10s.ARW; images_in_memory= 56
rawpy read the 141th file at location: 00179_00_10s.ARW
loading ./dataset/Sony/long/00098_00_30s.ARW; images_in_memory= 57
rawpy read the 81th file at location: 00098_00_30s.ARW
loading ./dataset/Sony/long/00202_00_10s.ARW; images_in_memory= 58
rawpy read the 19th file at location: 00202_00_10s.ARW
loading ./dataset/Sony/long/00230_00_10s.ARW; images_in_memory= 59
rawpy read the 31th file at location: 00230_00_10s.ARW
loading ./dataset/Sony/long/00221_00_10s.ARW; images_in_memory= 60
rawpy read the 119th file at location: 00221_00_10s.ARW
(16, 128, 128, 4)
(161, 1, 1, 2)
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Epoch 0: at batch 5: Training dataset Loss=0.642242, Batch Time=0.127; Early rounds
loading ./dataset/Sony/long/00174_00_30s.ARW; images_in_memory= 61
rawpy read the 25th file at location: 00174_00_30s.ARW
loading ./dataset/Sony/long/00060_00_10s.ARW; images_in_memory= 62
loading ./dataset/Sony/long/00013_00_10s.ARW; images_in_memory= 63
loading ./dataset/Sony/long/00019_00_10s.ARW; images_in_memory= 64
rawpy read the 69th file at location: 00019_00_10s.ARW
loading ./dataset/Sony/long/00117_00_30s.ARW; images_in_memory= 65
loading ./dataset/Sony/long/00058_00_10s.ARW; images_in_memory= 66
^CTraceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 231, in <module>
    gt_raw = rawpy.imread(train_fn)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/rawpy/__init__.py", line 21, in imread
    d.unpack()
KeyboardInterrupt
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:41:52
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:41:52.287280: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:41:52.424602: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:41:52.424636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:41:52.709133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:41:52.709177: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:41:52.709197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:41:52.709300: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,len(train_fns)= 161 ,
result_dir= ./heteroscedastic_sigma_map_checkpoint/

Cleared all images in memory.

loading ./dataset/Sony/long/00230_00_10s.ARW; images_in_memory= 0
loading ./dataset/Sony/long/00088_00_30s.ARW; images_in_memory= 5
loading ./dataset/Sony/long/00021_00_10s.ARW; images_in_memory= 12
loading ./dataset/Sony/long/00012_00_10s.ARW; images_in_memory= 14


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.04532065906921279 0.021944446038198142
0.02120319063983267 0.013772476882679073
0.02120319063983267 0.013772476882679073
0.07351459407954053 0.0519174563828758
0.0651852623103295 0.03501408998048174
0.07501497209005947 0.03168378341670575
0.0039537867311345565 0.0035407541347076818
0.04940206161370497 0.021154702907040593
0.003688731255354405 0.002537977207502374
0.009342746671245905 0.005241671286027153
0.03751257296278965 0.057002934087591973
0.04718496371458514 0.021254618443678902
0.013079086771227821 0.005271914993875306
0.032395007186803326 0.02442862205694405
0.32165358909406905 0.15711138684202033
0.011408138319686012 0.01752408959248554
[[0.06880474 0.05569907]
 [0.06023565 0.06099175]]
heteroschedastic_sigma_s [0.03701321]
sigma_c 0.025723043855
[[-0.00209742  0.00095055]
 [ 0.00178441  0.00206677]]
[[-0.01344231  0.02143164]
 [-0.02067552  0.01254652]] 

Epoch 1: at batch 1: Training dataset Loss=0.909493, Batch Time=1.188
Epoch 1: at batch 1: Training dataset Loss=0.909493, Batch Time=1.188; Early rounds
loading ./dataset/Sony/long/00148_00_30s.ARW; images_in_memory= 16
loading ./dataset/Sony/long/00026_00_10s.ARW; images_in_memory= 25
loading ./dataset/Sony/long/00018_00_10s.ARW; images_in_memory= 28
Epoch 1: at batch 2: Training dataset Loss=0.819931, Batch Time=0.124; Early rounds
loading ./dataset/Sony/long/00219_00_10s.ARW; images_in_memory= 32
loading ./dataset/Sony/long/00059_00_10s.ARW; images_in_memory= 33
loading ./dataset/Sony/long/00024_00_10s.ARW; images_in_memory= 42
Epoch 1: at batch 3: Training dataset Loss=0.734531, Batch Time=0.126; Early rounds
Epoch 1: at batch 4: Training dataset Loss=0.673678, Batch Time=0.126; Early rounds
loading ./dataset/Sony/long/00038_00_10s.ARW; images_in_memory= 60
loading ./dataset/Sony/long/00206_00_10s.ARW; images_in_memory= 61
loading ./dataset/Sony/long/00098_00_30s.ARW; images_in_memory= 63
Epoch 1: at batch 5: Training dataset Loss=0.617726, Batch Time=0.127; Early rounds
loading ./dataset/Sony/long/00114_00_30s.ARW; images_in_memory= 65
loading ./dataset/Sony/long/00143_00_30s.ARW; images_in_memory= 69
Epoch 1: at batch 6: Training dataset Loss=0.558992, Batch Time=0.126; Early rounds
loading ./dataset/Sony/long/00128_00_30s.ARW; images_in_memory= 76
loading ./dataset/Sony/long/00057_00_10s.ARW; images_in_memory= 77
Epoch 1: at batch 7: Training dataset Loss=0.528033, Batch Time=0.121; Early rounds
loading ./dataset/Sony/long/00164_00_30s.ARW; images_in_memory= 85
Epoch 1: at batch 8: Training dataset Loss=0.485681, Batch Time=0.119; Early rounds
loading ./dataset/Sony/long/00156_00_30s.ARW; images_in_memory= 87
loading ./dataset/Sony/long/00072_00_30s.ARW; images_in_memory= 90
loading ./dataset/Sony/long/00084_00_30s.ARW; images_in_memory= 95
Epoch 1: at batch 9: Training dataset Loss=0.427161, Batch Time=0.123; Early rounds
loading ./dataset/Sony/long/00180_00_10s.ARW; images_in_memory= 102
Epoch 1: at batch 10: Training dataset Loss=0.380041, Batch Time=0.125; Early rounds
		Epoch 1:  Time = 17.741, Avg epoch time=17.740, Current epoch Time=8.870

Loss vector (slice for the first 10 images)
Traceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 362, in <module>
    print(g_loss[0:min(10,len(train_fns)), 1,1,:])
IndexError: index 1 is out of bounds for axis 1 with size 1
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:42:40
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:42:40.928171: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:42:41.065149: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:42:41.065184: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:42:41.350428: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:42:41.350464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:42:41.350481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:42:41.350585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./heteroscedastic_sigma_map_checkpoint/. Hence, will create the folder.

last epoch of previous run: 1


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,len(train_fns)= 161 ,
result_dir= ./heteroscedastic_sigma_map_checkpoint/

Cleared all images in memory.

loading ./dataset/Sony/long/00179_00_10s.ARW; images_in_memory= 0
loading ./dataset/Sony/long/00038_00_10s.ARW; images_in_memory= 1
loading ./dataset/Sony/long/00072_00_30s.ARW; images_in_memory= 3
loading ./dataset/Sony/long/00128_00_30s.ARW; images_in_memory= 11
loading ./dataset/Sony/long/00012_00_10s.ARW; images_in_memory= 13


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.013220296183883207 0.00969003986170309
0.07535546395343573 0.0458554702833433
0.08034591673080627 0.03731977126853381
0.12083844914075392 0.06678806060145231
0.08444417209130606 0.043836684294847064
0.04098161647615939 0.025642594770551603
0.07110938644010645 0.0649818986381165
0.057524352251583366 0.027455014895865826
0.07110938644010645 0.0649818986381165
0.05470008602881471 0.0658477996505818
0.0029111781212201038 0.0014798691523536598
0.14125088827393117 0.08464425132695635
0.04920470012742939 0.029003723790519
0.018598936170793223 0.008167144242431389
0.05456469806138986 0.05374887495425388
0.0656955486554267 0.05435198415552654
[[0.00491462 0.00516666]
 [0.00453658 0.00529267]]
heteroschedastic_sigma_s [0.09335326]
sigma_c 0.00224242079298
[[-0.00111539  0.00018649]
 [ 0.00011712 -0.0007644 ]]
[[ 0.00235027 -0.00233831]
 [ 0.00044289  0.00407171]] 

Epoch 2: at batch 1: Training dataset Loss=0.911464, Batch Time=1.159
Epoch 2: at batch 1: Training dataset Loss=0.911464, Batch Time=1.159; Early rounds
loading ./dataset/Sony/long/00141_00_30s.ARW; images_in_memory= 17
loading ./dataset/Sony/long/00018_00_10s.ARW; images_in_memory= 20
loading ./dataset/Sony/long/00230_00_10s.ARW; images_in_memory= 23
loading ./dataset/Sony/long/00156_00_30s.ARW; images_in_memory= 24
Epoch 2: at batch 2: Training dataset Loss=0.825423, Batch Time=0.124; Early rounds
loading ./dataset/Sony/long/00164_00_30s.ARW; images_in_memory= 29
Epoch 2: at batch 3: Training dataset Loss=0.747943, Batch Time=0.125; Early rounds
loading ./dataset/Sony/long/00129_00_30s.ARW; images_in_memory= 49
Epoch 2: at batch 4: Training dataset Loss=0.698427, Batch Time=0.121; Early rounds
loading ./dataset/Sony/long/00169_00_30s.ARW; images_in_memory= 53
loading ./dataset/Sony/long/00180_00_10s.ARW; images_in_memory= 54
loading ./dataset/Sony/long/00088_00_30s.ARW; images_in_memory= 57
Epoch 2: at batch 5: Training dataset Loss=0.623947, Batch Time=0.123; Early rounds
loading ./dataset/Sony/long/00098_00_30s.ARW; images_in_memory= 66
loading ./dataset/Sony/long/00026_00_10s.ARW; images_in_memory= 67
Epoch 2: at batch 6: Training dataset Loss=0.551975, Batch Time=0.125; Early rounds
loading ./dataset/Sony/long/00039_00_10s.ARW; images_in_memory= 79
Epoch 2: at batch 7: Training dataset Loss=0.520313, Batch Time=0.123; Early rounds
loading ./dataset/Sony/long/00225_00_10s.ARW; images_in_memory= 81
loading ./dataset/Sony/long/00113_00_30s.ARW; images_in_memory= 83
Epoch 2: at batch 8: Training dataset Loss=0.503323, Batch Time=0.121; Early rounds
Epoch 2: at batch 9: Training dataset Loss=0.466375, Batch Time=0.123; Early rounds
loading ./dataset/Sony/long/00114_00_30s.ARW; images_in_memory= 91
loading ./dataset/Sony/long/00090_00_30s.ARW; images_in_memory= 92
Epoch 2: at batch 10: Training dataset Loss=0.412572, Batch Time=0.126; Early rounds
		Epoch 2:  Time = 15.416, Avg epoch time=15.416, Current epoch Time=7.708

Loss vector (slice for the first 10 images)
[[0.04810138 0.02352353]
 [1.         1.        ]
 [0.15049594 0.0223361 ]
 [0.06045337 0.02659747]
 [0.15039602 0.02243866]
 [1.         1.        ]
 [0.06040696 0.02523266]
 [1.         1.        ]
 [0.04801715 0.02319161]
 [0.04605248 0.01489453]]
Epoch 3: at batch 1: Training dataset Loss=0.375450, Batch Time=0.123
Epoch 3: at batch 1: Training dataset Loss=0.375450, Batch Time=0.123; Early rounds
loading ./dataset/Sony/long/00134_00_30s.ARW; images_in_memory= 109
Epoch 3: at batch 2: Training dataset Loss=0.348859, Batch Time=0.123; Early rounds
loading ./dataset/Sony/long/00084_00_30s.ARW; images_in_memory= 110
Epoch 3: at batch 3: Training dataset Loss=0.332501, Batch Time=0.120; Early rounds
loading ./dataset/Sony/long/00059_00_10s.ARW; images_in_memory= 113
loading ./dataset/Sony/long/00200_00_10s.ARW; images_in_memory= 114
Epoch 3: at batch 4: Training dataset Loss=0.314347, Batch Time=0.121; Early rounds
Epoch 3: at batch 5: Training dataset Loss=0.299442, Batch Time=0.124; Early rounds
loading ./dataset/Sony/long/00219_00_10s.ARW; images_in_memory= 118
loading ./dataset/Sony/long/00206_00_10s.ARW; images_in_memory= 120
Epoch 3: at batch 6: Training dataset Loss=0.281326, Batch Time=0.135; Early rounds
loading ./dataset/Sony/long/00024_00_10s.ARW; images_in_memory= 125
loading ./dataset/Sony/long/00021_00_10s.ARW; images_in_memory= 128
Epoch 3: at batch 7: Training dataset Loss=0.236520, Batch Time=0.131; Early rounds
Epoch 3: at batch 8: Training dataset Loss=0.225930, Batch Time=0.126; Early rounds
loading ./dataset/Sony/long/00143_00_30s.ARW; images_in_memory= 132
Epoch 3: at batch 9: Training dataset Loss=0.191659, Batch Time=0.124; Early rounds
Epoch 3: at batch 10: Training dataset Loss=0.186673, Batch Time=0.123; Early rounds
		Epoch 3:  Time = 22.066, Avg epoch time=6.468, Current epoch Time=7.355

Loss vector (slice for the first 10 images)
[[0.11248471 0.01186678]
 [1.         1.        ]
 [0.00367397 0.0331841 ]
 [0.13408249 0.04242019]
 [0.15039602 0.02243866]
 [0.01377665 0.00745453]
 [0.06040696 0.02523266]
 [0.08427283 0.04860228]
 [0.00354983 0.03326151]
 [0.09012306 0.04017561]]
loading ./dataset/Sony/long/00057_00_10s.ARW; images_in_memory= 139
Epoch 4: at batch 1: Training dataset Loss=0.166941, Batch Time=0.120
loading ./dataset/Sony/long/00109_00_30s.ARW; images_in_memory= 145
loading ./dataset/Sony/long/00148_00_30s.ARW; images_in_memory= 152
Epoch 5: at batch 1: Training dataset Loss=0.089351, Batch Time=0.121
Epoch 6: at batch 1: Training dataset Loss=0.063482, Batch Time=0.122
Epoch 7: at batch 1: Training dataset Loss=0.071351, Batch Time=0.125
Epoch 8: at batch 1: Training dataset Loss=0.066114, Batch Time=0.120
Epoch 9: at batch 1: Training dataset Loss=0.053109, Batch Time=0.120
Epoch 10: at batch 1: Training dataset Loss=0.056294, Batch Time=0.125
Epoch 12: at batch 1: Training dataset Loss=0.055213, Batch Time=0.121
Epoch 17: at batch 1: Training dataset Loss=0.058238, Batch Time=0.123
Epoch 22: at batch 1: Training dataset Loss=0.053242, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.0070873  0.03330537]
 [0.14382439 0.02168194]
 [0.00591237 0.02636511]
 [0.04852144 0.00314131]
 [0.01245625 0.03610424]
 [0.14731376 0.05297254]
 [0.01245625 0.0361044 ]
 [0.01245611 0.03610424]
 [0.01245593 0.03610351]
 [0.00708719 0.03330562]]
Epoch 27: at batch 1: Training dataset Loss=0.045406, Batch Time=0.125
Epoch 32: at batch 1: Training dataset Loss=0.058003, Batch Time=0.121
Epoch 37: at batch 1: Training dataset Loss=0.051748, Batch Time=0.124
Epoch 42: at batch 1: Training dataset Loss=0.053872, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.09419755 0.05149096]
 [0.09142225 0.05872152]
 [0.15508893 0.04013358]
 [0.09199434 0.01947983]
 [0.03779908 0.03126545]
 [0.06122758 0.03371992]
 [0.09419723 0.05149019]
 [0.02115751 0.04261793]
 [0.11771679 0.02126405]
 [0.09235986 0.0038671 ]]
Epoch 47: at batch 1: Training dataset Loss=0.049040, Batch Time=0.125
Epoch 52: at batch 1: Training dataset Loss=0.055280, Batch Time=0.122
Epoch 57: at batch 1: Training dataset Loss=0.060027, Batch Time=0.124
Epoch 62: at batch 1: Training dataset Loss=0.053621, Batch Time=0.122
Loss vector (slice for the first 10 images)
[[0.03210711 0.041779  ]
 [0.13126507 0.01887679]
 [0.05957689 0.04856043]
 [0.03285744 0.02955285]
 [0.06259663 0.00318839]
 [0.01955452 0.05502117]
 [0.01776153 0.0234976 ]
 [0.12177569 0.00181541]
 [0.14308357 0.02533156]
 [0.04539058 0.00796443]]
Epoch 67: at batch 1: Training dataset Loss=0.052752, Batch Time=0.125
Epoch 72: at batch 1: Training dataset Loss=0.058056, Batch Time=0.125
Epoch 77: at batch 1: Training dataset Loss=0.056572, Batch Time=0.120
Epoch 82: at batch 1: Training dataset Loss=0.053775, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.11133398 0.01382303]
 [0.0637804  0.04287904]
 [0.04945769 0.04080878]
 [0.15354936 0.01675376]
 [0.12024358 0.03764774]
 [0.11133396 0.0138204 ]
 [0.04608907 0.00038495]
 [0.15354923 0.01675191]
 [0.09750937 0.02261043]
 [0.12695837 0.04506071]]
Epoch 87: at batch 1: Training dataset Loss=0.058464, Batch Time=0.124
Epoch 92: at batch 1: Training dataset Loss=0.052564, Batch Time=0.125
Epoch 97: at batch 1: Training dataset Loss=0.047626, Batch Time=0.124
Epoch 102: at batch 1: Training dataset Loss=0.060188, Batch Time=0.119
Loss vector (slice for the first 10 images)
[[0.05416289 0.0198774 ]
 [0.0147176  0.03875898]
 [0.07218355 0.05393986]
 [0.04005026 0.02450742]
 [0.08540922 0.05315864]
 [0.0147176  0.03875898]
 [0.1133776  0.03430653]
 [0.0147176  0.03875898]
 [0.0147176  0.03875898]
 [0.11155302 0.02558277]]
Epoch 107: at batch 1: Training dataset Loss=0.055331, Batch Time=0.126
Epoch 112: at batch 1: Training dataset Loss=0.062826, Batch Time=0.125
Epoch 117: at batch 1: Training dataset Loss=0.050097, Batch Time=0.126
Epoch 122: at batch 1: Training dataset Loss=0.050881, Batch Time=0.121
Loss vector (slice for the first 10 images)
[[0.1027236  0.04760937]
 [0.15506275 0.03609859]
 [0.01889423 0.02669651]
 [0.01019482 0.03062872]
 [0.01632754 0.03020176]
 [0.11302865 0.04582931]
 [0.08842825 0.02013513]
 [0.08193326 0.01459063]
 [0.08842861 0.0201348 ]
 [0.10432297 0.01717516]]
Epoch 127: at batch 1: Training dataset Loss=0.043207, Batch Time=0.120
Epoch 132: at batch 1: Training dataset Loss=0.051747, Batch Time=0.121
Epoch 137: at batch 1: Training dataset Loss=0.063808, Batch Time=0.127
Epoch 142: at batch 1: Training dataset Loss=0.042321, Batch Time=0.126
Loss vector (slice for the first 10 images)
[[0.12955658 0.03729046]
 [0.02710601 0.04392381]
 [0.02710601 0.04392394]
 [0.10563922 0.01845227]
 [0.02710601 0.04392392]
 [0.00704913 0.00945468]
 [0.00306665 0.00068244]
 [0.12041291 0.04923952]
 [0.15561581 0.00287932]
 [0.13795336 0.03259526]]
Epoch 147: at batch 1: Training dataset Loss=0.063656, Batch Time=0.120
Epoch 152: at batch 1: Training dataset Loss=0.060595, Batch Time=0.125
Epoch 157: at batch 1: Training dataset Loss=0.058832, Batch Time=0.122
Epoch 162: at batch 1: Training dataset Loss=0.058795, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.05881451 0.05086198]
 [0.03016962 0.03793298]
 [0.05881451 0.05086153]
 [0.00165929 0.02293314]
 [0.0851666  0.02137098]
 [0.13913332 0.02406868]
 [0.02713031 0.00433148]
 [0.10103487 0.00298304]
 [0.11438303 0.00891628]
 [0.07317776 0.04412773]]
Epoch 167: at batch 1: Training dataset Loss=0.059497, Batch Time=0.126
Epoch 172: at batch 1: Training dataset Loss=0.049537, Batch Time=0.123
Epoch 177: at batch 1: Training dataset Loss=0.064683, Batch Time=0.120
Epoch 182: at batch 1: Training dataset Loss=0.055166, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.0767194  0.02162386]
 [0.11331445 0.04875834]
 [0.09911615 0.01328742]
 [0.11331445 0.04875834]
 [0.02400658 0.02019636]
 [0.00065584 0.0323852 ]
 [0.06472706 0.00977761]
 [0.06463256 0.00476614]
 [0.07877826 0.04204793]
 [0.03453442 0.02111803]]
Epoch 187: at batch 1: Training dataset Loss=0.055732, Batch Time=0.120
Epoch 192: at batch 1: Training dataset Loss=0.050705, Batch Time=0.125
Epoch 197: at batch 1: Training dataset Loss=0.052535, Batch Time=0.126


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.07901942308956222 0.07880013957019841
0.016147634924433163 0.007442328359015534
0.3302457395922147 0.3291779774684972
0.14518834321955865 0.06988858993121794
0.050871534629010284 0.02353782555182483
0.07656612499916804 0.05796848217486906
0.09396537861605925 0.0704362305283628
0.0358901755390697 0.026044837601053403
0.014369004857876178 0.00904017445559595
0.054898490675298284 0.07640905180551665
0.10867372836313294 0.08564609343466664
0.008813651924813648 0.009376835212889222
0.008331523791381912 0.004959792147285656
0.07403210267970461 0.04965862857189045
0.04195497264043446 0.029498613482839938
0.24608976631378 0.30369995376059966
[[0.12847331 0.13401802]
 [0.13156071 0.13823956]]
heteroschedastic_sigma_s [0.06215778]
sigma_c 0.0438528412546
[[-0.00498567  0.00117569]
 [ 0.00685551  0.00206601]]
[[-0.01344057  0.03763995]
 [ 0.04183732 -0.00122109]] 

Epoch 202: at batch 1: Training dataset Loss=0.058076, Batch Time=0.130
		Epoch 202:  Time = 273.479, Avg epoch time=1.234, Current epoch Time=1.354

Loss vector (slice for the first 10 images)
[[0.12885931 0.05829765]
 [0.07405702 0.05493036]
 [0.07405702 0.05492912]
 [0.05857712 0.0411048 ]
 [0.09633083 0.03609468]
 [0.09633082 0.03609468]
 [0.07924959 0.05674782]
 [0.05857712 0.0411048 ]
 [0.09253687 0.04883846]
 [0.09253687 0.04883846]]
Epoch 207: at batch 1: Training dataset Loss=0.057630, Batch Time=0.124
Epoch 212: at batch 1: Training dataset Loss=0.050683, Batch Time=0.123
Epoch 217: at batch 1: Training dataset Loss=0.060606, Batch Time=0.121
Epoch 222: at batch 1: Training dataset Loss=0.054806, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.00851993 0.04127078]
 [0.0427992  0.01875058]
 [0.0161576  0.05110475]
 [0.01465611 0.01269916]
 [0.0161576  0.05110475]
 [0.01465611 0.01269916]
 [0.0161576  0.05110475]
 [0.11561595 0.04560193]
 [0.05099159 0.0259868 ]
 [0.0427992  0.01875058]]
Epoch 227: at batch 1: Training dataset Loss=0.050561, Batch Time=0.125
Epoch 232: at batch 1: Training dataset Loss=0.057933, Batch Time=0.124
Epoch 237: at batch 1: Training dataset Loss=0.048032, Batch Time=0.123
Epoch 242: at batch 1: Training dataset Loss=0.054593, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.1297041  0.0546762 ]
 [0.12970406 0.05467592]
 [0.1146942  0.03771369]
 [0.07158455 0.05917327]
 [0.07158455 0.05917327]
 [0.1146942  0.03771369]
 [0.09448264 0.01759177]
 [0.03823526 0.03558144]
 [0.07158454 0.05917327]
 [0.07158455 0.05917316]]
Epoch 247: at batch 1: Training dataset Loss=0.049912, Batch Time=0.126
Epoch 252: at batch 1: Training dataset Loss=0.049609, Batch Time=0.125
Epoch 257: at batch 1: Training dataset Loss=0.054625, Batch Time=0.121
Epoch 262: at batch 1: Training dataset Loss=0.050343, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.10967556 0.00122993]
 [0.14475352 0.02380215]
 [0.07720297 0.00069587]
 [0.15941727 0.00086057]
 [0.10967556 0.00122993]
 [0.14475352 0.02380216]
 [0.07681839 0.03691494]
 [0.07720297 0.00069587]
 [0.0343434  0.05667332]
 [0.03080576 0.00865337]]
Epoch 267: at batch 1: Training dataset Loss=0.060438, Batch Time=0.123
Epoch 272: at batch 1: Training dataset Loss=0.057444, Batch Time=0.124
Epoch 277: at batch 1: Training dataset Loss=0.051740, Batch Time=0.120
Epoch 282: at batch 1: Training dataset Loss=0.062719, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.02256125 0.01104924]
 [0.05616952 0.02305735]
 [0.05616952 0.02305735]
 [0.02687724 0.0398444 ]
 [0.09963677 0.00671048]
 [0.07741691 0.03940303]
 [0.06769568 0.03770401]
 [0.05741888 0.00543239]
 [0.06473359 0.04803345]
 [0.06473359 0.04803345]]
^CTraceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 303, in <module>
    feed_dict={in_image: input_patch, noise_level_map: noise_sigmas_feed, lr: learning_rate})
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1292, in _do_call
    return fn(*args)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1367, in _call_tf_sessionrun
    run_metadata)
KeyboardInterrupt
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 12:49:15
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 12:49:15.806645: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 12:49:15.931045: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 12:49:15.931082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 12:49:16.217287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 12:49:16.217327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 12:49:16.217361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 12:49:16.217463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)

Loaded ./heteroscedastic_sigma_map_checkpoint/model.ckpt

last epoch of previous run: 286


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,len(train_fns)= 161 ,
result_dir= ./heteroscedastic_sigma_map_checkpoint/

Cleared all images in memory.

heteroschedastic_sigma_s [0.00702593]
sigma_c 0.0127694631597
[[ 0.00014457 -0.00012927]
 [-0.0006061  -0.00043354]]
[[ 0.00887707  0.02103506]
 [-0.02810526  0.02822922]] 

Epoch 287: at batch 1: Training dataset Loss=0.907754, Batch Time=1.249
Epoch 287: at batch 1: Training dataset Loss=0.907754, Batch Time=1.249; Early rounds
loading ./dataset/Sony/long/00024_00_10s.ARW; images_in_memory= 15
Epoch 287: at batch 2: Training dataset Loss=0.816995, Batch Time=0.120; Early rounds
loading ./dataset/Sony/long/00114_00_30s.ARW; images_in_memory= 42
Epoch 287: at batch 3: Training dataset Loss=0.731205, Batch Time=0.120; Early rounds
loading ./dataset/Sony/long/00219_00_10s.ARW; images_in_memory= 44
loading ./dataset/Sony/long/00156_00_30s.ARW; images_in_memory= 49
Epoch 287: at batch 4: Training dataset Loss=0.681728, Batch Time=0.125; Early rounds
loading ./dataset/Sony/long/00072_00_30s.ARW; images_in_memory= 62
Epoch 287: at batch 5: Training dataset Loss=0.617613, Batch Time=0.121; Early rounds
loading ./dataset/Sony/long/00018_00_10s.ARW; images_in_memory= 66
loading ./dataset/Sony/long/00090_00_30s.ARW; images_in_memory= 72
Epoch 287: at batch 6: Training dataset Loss=0.553018, Batch Time=0.120; Early rounds
loading ./dataset/Sony/long/00012_00_10s.ARW; images_in_memory= 76
Epoch 287: at batch 7: Training dataset Loss=0.528988, Batch Time=0.126; Early rounds
loading ./dataset/Sony/long/00059_00_10s.ARW; images_in_memory= 79
Epoch 287: at batch 8: Training dataset Loss=0.473264, Batch Time=0.121; Early rounds
loading ./dataset/Sony/long/00039_00_10s.ARW; images_in_memory= 94
Epoch 287: at batch 9: Training dataset Loss=0.435284, Batch Time=0.125; Early rounds
Epoch 287: at batch 10: Training dataset Loss=0.407244, Batch Time=0.126; Early rounds
		Epoch 287:  Time = 18.588, Avg epoch time=18.588, Current epoch Time=9.294

Loss vector (slice for the first 10 images)
[[0.07801312 0.01522196]
 [0.11414824 0.04148273]
 [0.04234041 0.04813833]
 [0.00361629 0.02474305]
 [1.         1.        ]
 [0.11414824 0.04148273]
 [0.11414824 0.04148273]
 [1.         1.        ]
 [1.         1.        ]
 [0.00702593 0.01276946]]
loading ./dataset/Sony/long/00057_00_10s.ARW; images_in_memory= 100
loading ./dataset/Sony/long/00026_00_10s.ARW; images_in_memory= 102
loading ./dataset/Sony/long/00200_00_10s.ARW; images_in_memory= 103
Epoch 288: at batch 1: Training dataset Loss=0.343925, Batch Time=0.122
Epoch 288: at batch 1: Training dataset Loss=0.343925, Batch Time=0.122; Early rounds
Epoch 288: at batch 2: Training dataset Loss=0.300123, Batch Time=0.121; Early rounds
loading ./dataset/Sony/long/00038_00_10s.ARW; images_in_memory= 120
loading ./dataset/Sony/long/00164_00_30s.ARW; images_in_memory= 121
Epoch 288: at batch 3: Training dataset Loss=0.272481, Batch Time=0.124; Early rounds
Epoch 288: at batch 4: Training dataset Loss=0.250415, Batch Time=0.126; Early rounds
Epoch 288: at batch 5: Training dataset Loss=0.246058, Batch Time=0.125; Early rounds
Epoch 288: at batch 6: Training dataset Loss=0.239232, Batch Time=0.132; Early rounds
Epoch 288: at batch 7: Training dataset Loss=0.222631, Batch Time=0.126; Early rounds
Epoch 288: at batch 8: Training dataset Loss=0.191517, Batch Time=0.123; Early rounds
loading ./dataset/Sony/long/00128_00_30s.ARW; images_in_memory= 138
Epoch 288: at batch 9: Training dataset Loss=0.184727, Batch Time=0.121; Early rounds
Epoch 288: at batch 10: Training dataset Loss=0.174557, Batch Time=0.118; Early rounds
		Epoch 288:  Time = 26.594, Avg epoch time=7.968, Current epoch Time=8.865

Loss vector (slice for the first 10 images)
[[0.15097945 0.0555373 ]
 [0.07632763 0.01257366]
 [0.12454486 0.00483874]
 [0.07632763 0.01257366]
 [1.         1.        ]
 [0.06928108 0.05435772]
 [0.11414824 0.04148273]
 [0.07632763 0.01257366]
 [1.         1.        ]
 [0.07632763 0.01257366]]
Epoch 289: at batch 1: Training dataset Loss=0.173680, Batch Time=0.125
loading ./dataset/Sony/long/00084_00_30s.ARW; images_in_memory= 142
Epoch 290: at batch 1: Training dataset Loss=0.102727, Batch Time=0.125
Epoch 291: at batch 1: Training dataset Loss=0.075937, Batch Time=0.121
Epoch 292: at batch 1: Training dataset Loss=0.047439, Batch Time=0.120
Epoch 293: at batch 1: Training dataset Loss=0.049082, Batch Time=0.122
Epoch 294: at batch 1: Training dataset Loss=0.051377, Batch Time=0.122
Epoch 295: at batch 1: Training dataset Loss=0.056530, Batch Time=0.118


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.02097036985599665 0.007210250157384448
0.11316125640612995 0.04753463799706471
0.09559990440789079 0.05150582480219614
0.05304428774236669 0.022177037015026355
0.23622653694428664 0.09150132357644118
0.38134189930431717 0.2325866242013517
0.0838336273346929 0.19133554666383074
0.005134154204961661 0.00454833512613
0.007340493759835809 0.01058132086664584
0.01766494176346356 0.007644898634194287
0.1242667244110578 0.05265676197609528
0.16176799953959886 0.07396953708446861
0.12542665964552668 0.0716634348998371
0.06853073761453743 0.057514816755933944
0.0015240146993462655 0.0010071889934966604
0.07424876190121221 0.0682774654042823
[[0.02772352 0.02721946]
 [0.02797555 0.02646336]]
Epoch 297: at batch 1: Training dataset Loss=0.060315, Batch Time=0.120
Epoch 302: at batch 1: Training dataset Loss=0.057285, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.05069125673019759 0.025089879262226646
0.14054467095968448 0.05689807632594063
0.08791987928070455 0.09813256485623446
0.10534475973270929 0.0695131037029309
0.007028376486361232 0.01340202286504885
0.006846982352534781 0.004283479848947676
0.06374161602440154 0.02195801694394596
0.09020398473924729 0.07319492368665857
0.09962647301030358 0.04822868741632298
0.005893794043877243 0.006461028403158112
0.015012429462927912 0.011871963559101507
0.07812148889095916 0.05480503035446826
0.059561621265473264 0.028098199030919008
0.02066371730995442 0.016352086331628164
0.056184331338975824 0.04386751127183377
0.037993079161918075 0.03820611099696358
[[0.06048768 0.05620314]
 [0.0567072  0.07157709]]
Epoch 307: at batch 1: Training dataset Loss=0.059349, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.03699173 0.01578525]
 [0.04878791 0.04948677]
 [0.06602061 0.00585194]
 [0.04878791 0.04948677]
 [0.14649381 0.00916802]
 [0.13097792 0.02419152]
 [0.118013   0.00655011]
 [0.12082137 0.0557241 ]
 [0.13966915 0.04174825]
 [0.08519007 0.0123396 ]]
Epoch 312: at batch 1: Training dataset Loss=0.054838, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.03839610981982933 0.01756839858472242
0.011223927204004447 0.006575153808193272
0.08084578122951314 0.03583460143560658
0.15504182331049066 0.12131816938966042
0.018755729324538173 0.013684679345421008
0.01416144840687572 0.007940530214407902
0.015608748036852305 0.012334388510754015
0.14878208180422092 0.1342153693334178
0.08267265561984516 0.03979842048868715
0.17114458176932068 0.13733944046353547
0.033053627127852536 0.05278188123788548
0.24283647012015308 0.15342399806668428
0.26979697276779646 0.29429911548210014
0.07849125324750794 0.05068426734337655
0.031944810898767884 0.01653515153320929
0.02016996747436295 0.009142107758410224
[[0.06288198 0.06105476]
 [0.06105476 0.06111776]]
Epoch 317: at batch 1: Training dataset Loss=0.060722, Batch Time=0.124
Epoch 322: at batch 1: Training dataset Loss=0.060275, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.08972152590111904 0.1504172405433198
0.06715084350886968 0.03369040493220721
0.4202254153502345 0.4277176211872821
0.028834279204338564 0.04164343706303903
0.018711934459950896 0.007893437285680725
0.040594647419530894 0.026620937615375633
0.011706809085819891 0.00854763724469822
0.04608312451757968 0.02404190996380095
0.018462899224184426 0.00875011077429779
0.023836758033574545 0.05140704380617525
0.1854290465426658 0.13270542165960736
0.06856993688110435 0.027039387948359497
0.054009698658158634 0.061565338859535024
0.008588147568662663 0.0034092778608593713
0.09694631011417698 0.03895661377293361
0.08003596553147929 0.05249037541554293
[[0.03906496 0.04108122]
 [0.03074791 0.02923571]]
Epoch 327: at batch 1: Training dataset Loss=0.064330, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.11362565 0.04195396]
 [0.07087061 0.05407884]
 [0.12629054 0.034177  ]
 [0.11362565 0.04195396]
 [0.0197418  0.04471935]
 [0.03799536 0.0009362 ]
 [0.07087061 0.05407884]
 [0.02783826 0.03714641]
 [0.14075413 0.05747179]
 [0.12409441 0.03008398]]
Epoch 332: at batch 1: Training dataset Loss=0.057987, Batch Time=0.120


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.28720382168017977 0.1283530976764487
0.0009682480914818825 0.0006581094517588208
0.008679571480709747 0.007196894099736156
0.10818437034293993 0.038854841536352575
0.1256321451987148 0.05768450909512294
0.01159748343417899 0.013359225496412127
0.03251692848214205 0.027223843261885046
0.021825861916799916 0.02844733836714317
0.0184921217493077 0.014760470390537296
0.07457779642471252 0.035619091237219354
0.049891650398578236 0.02423890404474575
0.24513078974726454 0.1260606563124576
0.013267826187893128 0.014285924023055906
0.08900739418380255 0.04663653565530141
0.28489488841100297 0.13803978467752698
0.04503819984878987 0.017983133594713062
[[0.4166089  0.39190978]
 [0.4196333  0.37552768]]
Epoch 337: at batch 1: Training dataset Loss=0.052065, Batch Time=0.119
Epoch 342: at batch 1: Training dataset Loss=0.059801, Batch Time=0.120


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.045956115366367456 0.07480083370096849
0.07356843677187008 0.05838250577242068
0.08024926265012766 0.038294552733429965
0.0772003381805888 0.038524714364034554
0.10554906371314132 0.07030431158610946
0.23712726221276625 0.11607486584416682
0.007745569386237605 0.0026711113396183594
0.030725407632345103 0.021543003957434963
0.011199587750671647 0.014619200952071957
0.13074786884358502 0.10423862434069683
0.06971182902867668 0.052801073196010656
0.1508338874856463 0.05510875479155494
0.021498878111197328 0.022777488304887933
0.0637294655877838 0.026483924111660458
0.04454934559211665 0.02089956590919709
0.22013113439197696 0.09141106702084385
[[0.00957722 0.01083738]
 [0.01008128 0.00856909]]
Epoch 347: at batch 1: Training dataset Loss=0.061833, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.10098296 0.04149168]
 [0.06390031 0.00911035]
 [0.0682696  0.00370988]
 [0.10270935 0.01816816]
 [0.04082674 0.04706863]
 [0.03322353 0.0538897 ]
 [0.04082624 0.04706797]
 [0.15087619 0.02765838]
 [0.15087619 0.02765838]
 [0.10098296 0.04149168]]
Epoch 352: at batch 1: Training dataset Loss=0.058742, Batch Time=0.126


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.024465079612454588 0.014858811816976496
0.08632788022656257 0.03983297625658946
0.19272302074112702 0.2639938916981194
0.02163863769570895 0.013965965585122779
0.3030999514135999 0.2578336330062151
0.009897171119888526 0.007934279327996415
0.00849147043967946 0.0062263861597961255
0.007206532531228049 0.005252610244061104
0.008636041964540375 0.008523997475520947
0.01349858472815857 0.009887923506641904
0.02276193397116799 0.011432585143876616
0.02432069652569613 0.03391674779751507
0.050974327385598706 0.09014641967666602
0.06148711100533433 0.03138029089742773
0.01563575643106141 0.012637802784198045
0.009993302175237861 0.01145051662285558
[[0.02898368 0.0259593 ]
 [0.02873165 0.03024384]]
Epoch 357: at batch 1: Training dataset Loss=0.052695, Batch Time=0.122
Epoch 362: at batch 1: Training dataset Loss=0.053981, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.12188698400064979 0.10670289700542716
0.10711113451640131 0.0734709789666658
0.03624026433678296 0.02787441125251223
0.02026229511583466 0.034267996628270216
0.09630184340579717 0.03545317619447372
0.10711113451640131 0.0734709789666658
0.03871090948545586 0.03385031737358968
0.010913415522245895 0.008765709947792943
0.349749639423635 0.13905574952056593
0.012112746140015673 0.012402035342645433
0.04776676477194641 0.027112080086523938
0.010022564120493627 0.013064304177545333
0.04778164762197168 0.06506033617501716
0.012112746140015673 0.012402035342645433
0.15052902515320454 0.11164015411587036
0.08388554429233608 0.07642439367682437
[[0.15021108 0.14819482]
 [0.08392666 0.08947136]]
Epoch 367: at batch 1: Training dataset Loss=0.057281, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.14549525 0.05819395]
 [0.09249681 0.00255066]
 [0.14549525 0.05819361]
 [0.14549525 0.05819395]
 [0.07298189 0.03535984]
 [0.15113332 0.00853195]
 [0.13930369 0.02231007]
 [0.01719871 0.04560307]
 [0.09249681 0.00255066]
 [0.12121072 0.03672174]]
Epoch 372: at batch 1: Training dataset Loss=0.052043, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.2756245130426578 0.10698671612034766
0.058537941169205965 0.030295211514026192
0.02671471214395016 0.08399479776159918
0.3125456102335278 0.31641227120271087
0.037635659488564954 0.01435582482601522
0.09032192486863266 0.1818388921618316
0.07096778285745131 0.14470720698951609
0.019357428050309267 0.007884606952929283
0.04835036178233576 0.02966201462705155
0.145864713053804 0.14969884635432315
0.09376495598891665 0.03440127236389368
0.044182533752639586 0.08703444281853134
0.045715708950653244 0.028433641266847626
0.08619309988037571 0.046406422446351875
0.06025843669218034 0.0435897647664443
0.18898932169821592 0.17476784095581316
[[0.3825846  0.39216182]
 [0.38283661 0.39165774]]
Epoch 377: at batch 1: Training dataset Loss=0.049333, Batch Time=0.118
Epoch 382: at batch 1: Training dataset Loss=0.050500, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.06896283506788592 0.043380067996338735
0.11237812411150827 0.08209033315174469
0.043375998453441866 0.03946151251595631
0.24144119521275798 0.13829861611683705
0.03892042145804453 0.020766356081916036
0.01886651250397975 0.013879857164871251
0.038977129243891184 0.021794111666361592
0.16101764141413533 0.1332002110332894
0.3142415152124194 0.20404670762052862
0.02907152448034722 0.015407057067726972
0.04969231219739356 0.0324205102382804
0.01629739429892929 0.01535937978238465
0.17143464012437448 0.08553986244187901
0.03913727586129312 0.01505212462239537
0.0013118539836503018 0.000780086245206765
0.030481528503152333 0.016415343446756527
[[0.05645517 0.10812173]
 [0.04763405 0.09425997]]
Epoch 387: at batch 1: Training dataset Loss=0.062146, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.08154144 0.03915336]
 [0.11294906 0.05414893]
 [0.02494656 0.04092437]
 [0.02945212 0.04119674]
 [0.08154144 0.03915336]
 [0.06478619 0.03536648]
 [0.04932737 0.02399351]
 [0.0126034  0.05926972]
 [0.1065923  0.02285848]
 [0.10713678 0.02364117]]
Epoch 392: at batch 1: Training dataset Loss=0.063713, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.06070837146416608 0.049054527211273784
0.02990942630167126 0.017608373617093755
0.12965736588634158 0.08995101258395398
0.021781917057955624 0.036177274229257504
0.051352918606049514 0.023085999010223294
0.01126978337244644 0.012598416745791308
0.43798290787943017 0.2747610565836467
0.08080517062522308 0.08330731895931091
0.015228684896896993 0.007789856709116596
0.13940204520009658 0.1663731141254553
0.30594914845465837 0.23665451336632537
0.15231892985268303 0.10046537301105286
0.009914138356849733 0.030161823310076212
0.058708313512449095 0.03474947008013792
0.12093078066749285 0.11526612649287053
0.17768452705269056 0.08975289882048425
[[0.06099175 0.07983114]
 [0.02016256 0.03748976]]
Epoch 397: at batch 1: Training dataset Loss=0.058826, Batch Time=0.125
Epoch 402: at batch 1: Training dataset Loss=0.052925, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.034071873268445074 0.11396490300554706
0.08732449422856803 0.08246033665351808
0.12752388891318844 0.10274949915644631
0.23865339880251213 0.1505802800482496
0.02483388641974127 0.020492645374453795
0.2800842790702518 0.10807895286748687
0.017040987933375717 0.013089626035534666
0.0014420137301216496 0.0009856072571037826
0.08201980520869512 0.09784065720173636
0.0014420137301216496 0.0009856072571037826
0.04640053635676722 0.03617166783177426
0.028729733748200204 0.014846919652538253
0.33056296155820064 0.2630565765823635
0.10897525837042643 0.06347816592002976
0.04011221932574216 0.0738892105940293
0.07241453448443735 0.03341838359412227
[[0.00655283 0.0063008 ]
 [0.0063008  0.0063008 ]]
Epoch 407: at batch 1: Training dataset Loss=0.059808, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.15253445 0.02515431]
 [0.047343   0.04291073]
 [0.06470413 0.03321846]
 [0.15253445 0.02515431]
 [0.04502107 0.00863591]
 [0.06470413 0.03321847]
 [0.15253445 0.02515431]
 [0.08501271 0.059224  ]
 [0.15253445 0.02515431]
 [0.047343   0.04291078]]
Epoch 412: at batch 1: Training dataset Loss=0.055648, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.003774657719755048 0.0024910615425166108
0.04452712125685743 0.03675278526336935
0.0915757663261445 0.05522392394061926
0.04061430376415043 0.04400686136188138
0.032461691686831884 0.02894985818725319
0.0070350987438025925 0.002443678172798152
0.006144630018647845 0.002448052299412448
0.0033783003131784284 0.0019530905453539551
0.05457295189651745 0.13579290390052798
0.3760538265049611 0.32692369493344475
0.15584327461109115 0.09269600412232416
0.02095341798223771 0.02323927555090352
0.1426505665886566 0.09216970658583039
0.09760452918416274 0.12401633267447496
0.08978408777561242 0.07797129720377777
0.33199241993429496 0.16764877384641821
[[0.00201626 0.0063008 ]
 [0.00579674 0.00680486]]
Epoch 417: at batch 1: Training dataset Loss=0.050014, Batch Time=0.123
Epoch 422: at batch 1: Training dataset Loss=0.057012, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.06935023870090617 0.0732939734625687
0.01749406945938148 0.006298277492553373
0.1852882995569729 0.09700529597542495
0.05880833355206505 0.05160191835934376
0.13422493068948427 0.06275450847345979
0.07258209946978145 0.056150168595880796
0.16964125886708814 0.11478633289495878
0.0277229055825039 0.019085735277021787
0.03826243124738227 0.032714479567266204
0.17984948537226586 0.26861928662509243
0.06633545833171883 0.05044373391575976
0.055461292027153775 0.024067561793047393
0.049289340204352605 0.06334121934305789
0.07037941526086211 0.04814789273651913
0.2600597790680155 0.10393147798528564
0.20898448659829683 0.1878918422244152
[[0.12973347 0.19324555]
 [0.12588999 0.19324555]]
Epoch 427: at batch 1: Training dataset Loss=0.055016, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.04694654 0.00520673]
 [0.0466781  0.02122371]
 [0.04185792 0.04826695]
 [0.0532532  0.02114399]
 [0.04694654 0.00520673]
 [0.09383646 0.02050425]
 [0.13511438 0.0422068 ]
 [0.02880775 0.0527054 ]
 [0.06473998 0.04581977]
 [0.12480254 0.05798999]]
Epoch 432: at batch 1: Training dataset Loss=0.054807, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.338234300153772 0.14412785966846764
0.055278131758626614 0.022179726904991474
0.3479037903587905 0.22071062954430642
0.024313212782134386 0.027850327612354116
0.14454051434188386 0.06281373770105474
0.338234300153772 0.14412785966846764
0.024313212782134386 0.027850327612354116
0.009573403296231575 0.006570524291830121
0.17820689278366686 0.07912883924951941
0.14289732221948803 0.09609526014864384
0.009555780348947884 0.010877580496143272
0.04452577527022683 0.024939050779596565
0.025157406178565545 0.03935773024077007
0.014078504258236535 0.006451426628073502
0.0790460382620779 0.035077149442910176
0.022115504891338844 0.013030806011949168
[[0.37048706 0.47306406]
 [0.40551952 0.42316175]]
Epoch 437: at batch 1: Training dataset Loss=0.063985, Batch Time=0.118
Epoch 442: at batch 1: Training dataset Loss=0.050772, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.10032323366623608 0.09752255870045495
0.3417843188811389 0.1444959508634841
0.015850777389041504 0.010730957742774699
0.17133211748156896 0.160918712324926
0.016199105803439195 0.01003794197913565
0.009736405342308352 0.007953338370581563
0.030775239285599332 0.028986634819545203
0.02879290713445304 0.042996031564481965
0.12220470449323528 0.06014189459398872
0.14919461905310527 0.15111896270373526
0.01304038073612901 0.007124196772829507
0.05384896845328413 0.15750916849484917
0.02454653927464534 0.011615696075105313
0.07689499413216083 0.03438710893967999
0.011578035707032086 0.011731693991126068
0.03831741711304737 0.024845890549346786
[[0.18473946 0.17869069]
 [0.17667444 0.17491022]]
Epoch 447: at batch 1: Training dataset Loss=0.054027, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.0036225  0.04848153]
 [0.06243628 0.03296306]
 [0.15075785 0.03173045]
 [0.15389294 0.00487187]
 [0.00370477 0.03016814]
 [0.15075785 0.03173045]
 [0.0036225  0.04848153]
 [0.0929674  0.00053542]
 [0.08621527 0.04678953]
 [0.15389294 0.00487187]]
Epoch 452: at batch 1: Training dataset Loss=0.051500, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.03156790890681904 0.04627658997438591
0.013885261537354587 0.016363673353720753
0.07663242875693044 0.14039229135982462
0.013233399447623029 0.014566566878583728
0.0316965130600142 0.06613765879863139
0.016733039470189226 0.008106492544670243
0.005724627323841602 0.0025514413343855446
0.14026703424037024 0.10338408030269557
0.004690961889201262 0.004439356772148849
0.04792359829395387 0.04254024358379875
0.08922341511063081 0.12391180532923048
0.03156790890681904 0.04627658997438591
0.25307957768387723 0.1069918478494962
0.0066846273511771415 0.0038994866105063453
0.08736497890950456 0.09510980382228358
0.03705307006496383 0.02897890504890351
[[0.00453658 0.00478861]
 [0.00453658 0.00453658]]
Epoch 457: at batch 1: Training dataset Loss=0.049662, Batch Time=0.122
Epoch 462: at batch 1: Training dataset Loss=0.046528, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.004172117887989435 0.0031064953005499785
0.004172117887989435 0.0031064953005499785
0.006728403939236571 0.004257018016027123
0.004052736693926917 0.0014907459954933615
0.009631221510118415 0.006555251594509839
0.004052736693926917 0.0014907459954933615
0.009910131147321977 0.00340832544203587
0.180391571394523 0.16023095241093072
0.09522301590190096 0.07586192773848674
0.04626482149989286 0.03060202555292127
0.01947818698308268 0.011156980235547056
0.05354663368157375 0.06396246707183736
0.180391571394523 0.16023095241093072
0.04055377048059228 0.026159304466070104
0.0758438326883546 0.042177211030547765
0.039917388292920464 0.022526155303923913
[[0.00333942 0.00296138]
 [0.00327642 0.00296138]]
Epoch 467: at batch 1: Training dataset Loss=0.052199, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.13815708 0.03883342]
 [0.00483081 0.0291514 ]
 [0.02025259 0.01930061]
 [0.10520279 0.05057708]
 [0.15299872 0.04914173]
 [0.03053231 0.02691792]
 [0.09856395 0.00833193]
 [0.07675599 0.02672199]
 [0.10416599 0.01327601]
 [0.14942176 0.02943996]]
Epoch 472: at batch 1: Training dataset Loss=0.056318, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.09815601453010636 0.07829474159269052
0.04834725250082528 0.040395873959089314
0.11683984462288066 0.08962543238139453
0.018223607059539404 0.00968906516548921
0.08321401916353466 0.030856410941850168
0.1300163276492814 0.16523456356826266
0.027084020826094957 0.014053351850639854
0.28763029097051174 0.16915541125981018
0.31973291804953874 0.16029047685662198
0.03968943809374892 0.026475665720371352
0.020753745239110444 0.008590039918707115
0.01673163381045839 0.014460228010099562
0.027084020826094957 0.014053351850639854
0.035483939479727056 0.05962766065163426
0.0989863980042287 0.038823303097421616
0.11207698345212336 0.19350409177926078
[[0.03478042 0.02999181]
 [0.03578854 0.02721946]]
Epoch 477: at batch 1: Training dataset Loss=0.062586, Batch Time=0.118
Epoch 482: at batch 1: Training dataset Loss=0.046769, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.04268887127275889 0.0202215096524332
0.06541715436944884 0.028260849715987288
0.020726702243875295 0.024245624280596484
0.04268887127275889 0.0202215096524332
0.010734407646627808 0.0060750066063580375
0.03789754324855821 0.022316245604260185
0.22963478620413014 0.1900142433905023
0.0036942200351246512 0.0031952318230401705
0.011191665604219914 0.008513557918944693
0.019915554968238602 0.02095126378940076
0.15577363274064737 0.06134664174900371
0.04359490548252154 0.03092689729656666
0.013344601802493194 0.00866334755718443
0.0068763558347475495 0.004486786582639118
0.02970944878339754 0.023021293351445548
0.017149037778714682 0.02565300955374438
[[0.05620314 0.05532103]
 [0.05538403 0.05632915]]
heteroschedastic_sigma_s [0.01839792]
sigma_c 0.00287779481934
[[ 0.00567657 -0.00263334]
 [-0.00038647 -0.00578945]]
[[-0.00283086  0.00380437]
 [-0.0015706  -0.00568954]] 

Epoch 487: at batch 1: Training dataset Loss=0.050394, Batch Time=0.124
		Epoch 487:  Time = 278.372, Avg epoch time=1.229, Current epoch Time=1.378

Loss vector (slice for the first 10 images)
[[0.00860202 0.02547411]
 [0.02870359 0.004427  ]
 [0.0557429  0.0524682 ]
 [0.12652353 0.04643108]
 [0.0072572  0.05277665]
 [0.01660235 0.03314358]
 [0.01602608 0.05370288]
 [0.08526633 0.01143393]
 [0.11726387 0.05906534]
 [0.00898515 0.01493224]]
Epoch 492: at batch 1: Training dataset Loss=0.057363, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.21249692506041207 0.2578402860970735
0.025491620864690034 0.03567746523164132
0.027317491530278737 0.017228567841571705
0.12942603144063014 0.09778248045765991
0.01696478051847805 0.00867561717470433
0.014449033935625799 0.01320335559910762
0.02218963948537711 0.022331308795962276
0.18713345068190002 0.15057113826544777
0.21249692506041207 0.2578402860970735
0.09753051091449016 0.08203665582105488
0.04351072496558217 0.026469063366712793
0.054793652951161675 0.047768700015322525
0.0423700163265508 0.02753615543420577
0.036388290271137436 0.03226898243423605
0.011237764042149045 0.009098057023627554
0.011724964649161507 0.010742521739509212
[[0.05481696 0.05481696]
 [0.05053242 0.06067671]]
Epoch 497: at batch 1: Training dataset Loss=0.048943, Batch Time=0.124
Epoch 502: at batch 1: Training dataset Loss=0.051180, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.054092046699333984 0.06761157740251748
0.03632389877614628 0.027144117256054166
0.4249429905856772 0.27284783129801926
0.19008448082277596 0.10145530719150796
0.01779745700057589 0.008745313039678987
0.03574543482040937 0.021954801823994546
0.04312568257611815 0.03002898855213286
0.03735638550688236 0.03196497715206928
0.25040433721964916 0.19399071659117562
0.035089861630410724 0.02515655783943555
0.026862194886922097 0.016639008983863123
0.14371713099643557 0.09067611016114983
0.05061464934125581 0.026221769425105845
0.04163731948187177 0.030342570444239703
0.026862194886922097 0.016639008983863123
0.060865283851981644 0.036320116782327376
[[0.02646336 0.03024384]
 [0.04637389 0.04536576]]
Epoch 507: at batch 1: Training dataset Loss=0.051176, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.10509838 0.0380951 ]
 [0.11285464 0.00050309]
 [0.06754624 0.03711492]
 [0.07316008 0.0469029 ]
 [0.02460822 0.02468956]
 [0.11738225 0.01183306]
 [0.07938964 0.00595386]
 [0.06095294 0.05022922]
 [0.05677115 0.02777619]
 [0.12248938 0.04943137]]
Epoch 512: at batch 1: Training dataset Loss=0.049432, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.006047279902784197 0.002121843465421517
0.24065777832537094 0.20976991584469054
0.0837440724152998 0.05045468827642144
0.001448745638213289 0.0007752153547963523
0.006047279902784197 0.002121843465421517
0.21478245338300894 0.08420403997402379
0.010331085664355477 0.007627723650176289
0.01802149997396807 0.022856462944754064
0.020255080514949952 0.009631716496165461
0.34625375289732574 0.16474491668068725
0.04517821036957059 0.029228080052581967
0.04472708627034194 0.02139125853306864
0.09858601320442517 0.04425680394435702
0.013470821631142549 0.010063357667971244
0.029967394524690327 0.017600849673058448
0.12634791514440735 0.06786304542514585
[[0.00800202 0.00838006]
 [0.00838006 0.00800202]]
Epoch 517: at batch 1: Training dataset Loss=0.047705, Batch Time=0.123
Epoch 522: at batch 1: Training dataset Loss=0.053850, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.095664717993726 0.1027672875232229
0.20889380874126573 0.10978059219849669
0.06470305722463898 0.033375795665566345
0.1329558947606202 0.08787893546168103
0.029101144058966355 0.01509360362254891
0.2155531053915638 0.08644304694007958
0.010567221467688626 0.006833322312374992
0.08167211518103557 0.09793280400101564
0.05847035389657229 0.13511254397863842
0.03715943549090306 0.019634589878897536
0.051848053864784305 0.028468837788497434
0.014757046897670989 0.006829045734971742
0.021778055965871346 0.02970184795984308
0.010529447044421492 0.009106916139044163
0.24291048166656282 0.3678688190597598
0.031575846427031706 0.013264607979382077
[[0.09678029 0.03830887]
 [0.0637641  0.04133325]]
Epoch 527: at batch 1: Training dataset Loss=0.062230, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.11791623 0.04493188]
 [0.1503725  0.02252351]
 [0.1155626  0.02732838]
 [0.14967475 0.02250543]
 [0.14580536 0.05923588]
 [0.06802568 0.03310189]
 [0.1503725  0.02252351]
 [0.10876236 0.05274456]
 [0.1368978  0.0532641 ]
 [0.11791623 0.04493188]]
Epoch 532: at batch 1: Training dataset Loss=0.058876, Batch Time=0.118


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.016457125587017174 0.006824457614957972
0.09463027573300131 0.1253475503549626
0.04506377761647684 0.02560290674761674
0.046879741744746184 0.027683621246763664
0.015843889737897854 0.013901442103835373
0.0269601468727938 0.01924613868267634
0.07498229900499354 0.07033256894648042
0.07352848665595957 0.03508636431598437
0.03646985859375107 0.02388318262984986
0.1717280018354952 0.1887616494548748
0.0010650521329580442 0.0006828931132765879
0.09273943575218357 0.06975363102957312
0.09273943575218357 0.06975363102957312
0.07498229900499354 0.07033256894648042
0.32617294025135113 0.28172035048032984
0.1512754598002175 0.09748239823357378
[[0.02343898 0.01991053]
 [0.02545523 0.0252032 ]]
Epoch 537: at batch 1: Training dataset Loss=0.067623, Batch Time=0.119
Epoch 542: at batch 1: Training dataset Loss=0.053503, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.10071329183335109 0.07450807627542133
0.03579872858139055 0.024385316768346463
0.08375885143042794 0.06012355560693989
0.01914441837269898 0.013937655643027812
0.09904332308104813 0.04962149339151551
0.016582533957901546 0.013158693634931641
0.07763271925741133 0.06499105451789801
0.05930778076279353 0.04512032756921222
0.029454321911744685 0.01219298778587865
0.025875460489180213 0.014119560800872361
0.03836907741893647 0.03430214800784397
0.08437263434133513 0.06667218789818745
0.03694125239272239 0.022375095063713723
0.06337815482811493 0.027683355543743442
0.1589771920503047 0.08044841656206901
0.5003223466427755 0.33318579803066734
[[0.16256064 0.16155252]
 [0.14315417 0.15625985]]
Epoch 547: at batch 1: Training dataset Loss=0.045806, Batch Time=0.118
Loss vector (slice for the first 10 images)
[[0.12479682 0.00391132]
 [0.10281901 0.05208652]
 [0.02093004 0.02005556]
 [0.1133323  0.04797206]
 [0.12479682 0.00391132]
 [0.13513133 0.02447031]
 [0.1133323  0.04797206]
 [0.12488944 0.03847117]
 [0.13513133 0.02447031]
 [0.12479682 0.00391132]]
Epoch 552: at batch 1: Training dataset Loss=0.056405, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.07246476321873363 0.0888658817377289
0.0032324597229083096 0.012317459152226309
0.07519518177263151 0.04317422557583886
0.10869015336817256 0.09212914756953305
0.10876715587451713 0.15322265295831683
0.03754755837757706 0.01907362492995392
0.015494586437817759 0.005491134983939253
0.005055276898697336 0.0030847650631381828
0.03109107055347593 0.022571840963083702
0.02819652988089727 0.03732244270025141
0.019520578163262137 0.019040440976407755
0.06278446474926724 0.0933443141112433
0.04216146575070212 0.03277656812374055
0.04612528306590313 0.08101844210481621
0.05281585778102027 0.023400450753733548
0.05576483724087744 0.035536585898501644
[[0.11845504 0.08745511]
 [0.14416231 0.0952681 ]]
Epoch 557: at batch 1: Training dataset Loss=0.040840, Batch Time=0.125
Epoch 562: at batch 1: Training dataset Loss=0.059853, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.001183473833294979 0.00041948211609883556
0.17357974261268083 0.1413383971559701
0.0508716509525442 0.026125123739840595
0.006995068834182661 0.006019563690261889
0.041118989804800776 0.038327081042918106
0.048810354015763835 0.03144667837184238
0.059880916485354874 0.03066401926967463
0.02522859782727238 0.04282298722207431
0.019013026103281483 0.04355074862724194
0.23282706508473439 0.09547840706221497
0.001183473833294979 0.00041948211609883556
0.021544185307327268 0.06236643926117867
0.02102324443278558 0.01836909848816801
0.5146640078004339 0.3882449631381015
0.21823947811991928 0.22849893513402614
0.159566584521599 0.17228146029647212
[[0.00170122 0.0015752 ]
 [0.00151219 0.00138618]]
Epoch 567: at batch 1: Training dataset Loss=0.046601, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.05432788 0.0326778 ]
 [0.10664146 0.03133392]
 [0.05432788 0.0326778 ]
 [0.05064862 0.00971727]
 [0.12620019 0.02711646]
 [0.02730002 0.04586108]
 [0.03887392 0.00022027]
 [0.12620019 0.027116  ]
 [0.07487608 0.03423723]
 [0.11941026 0.01148403]]
Epoch 572: at batch 1: Training dataset Loss=0.048158, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.019391035685893065 0.01212831225315263
0.011859075860742863 0.009103753129638856
0.11852654314563438 0.08338044942987188
0.0537918155292374 0.024328746374297563
0.21033435537219702 0.08243181037761659
0.04077509166219784 0.03524339350917786
0.10476448641120939 0.07694648668156129
0.11707008971086452 0.042996724678609675
0.03674109989373342 0.101072203471509
0.011859075860742863 0.009103753129638856
0.04373669369624622 0.016097863121849756
0.014255764317347541 0.010526809866224531
0.02066126759038589 0.02077796420612165
0.07721025333481002 0.09706297158810474
0.08788636779998615 0.11066830644197914
0.013925791392345843 0.011370273574020607
[[0.02822758 0.02948775]
 [0.03074791 0.02873165]]
Epoch 577: at batch 1: Training dataset Loss=0.064958, Batch Time=0.121
Epoch 582: at batch 1: Training dataset Loss=0.051783, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.04524850115219614 0.03113720312529868
0.02631712406422082 0.009254130187304891
0.015792407304429545 0.025487892915017733
0.1131031170579746 0.10205949527092699
0.07909054458374953 0.06380829472877597
0.029313288464187792 0.019962580335717992
0.0442446351202328 0.026249572263958035
0.005067537008074829 0.00835720553284711
0.0017050790606796262 0.0012325923607032375
0.0012405854121464444 0.0008575863709654901
0.019488848249578083 0.013951648080888578
0.009913300000505387 0.006774490663941056
0.01615886050154902 0.012534233394998203
0.06841269566780284 0.024019633945096257
0.04111783804063407 0.02813490752889593
0.07909054458374953 0.06380829472877597
[[0.05361981 0.05790436]
 [0.05487997 0.05714826]]
Epoch 587: at batch 1: Training dataset Loss=0.052775, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.09273947 0.05304058]
 [0.14238042 0.03646863]
 [0.11022329 0.0203845 ]
 [0.12376732 0.00441624]
 [0.02193472 0.00890492]
 [0.02649089 0.03932029]
 [0.11291697 0.03158765]
 [0.03895272 0.03522945]
 [0.09031697 0.00161521]
 [0.14577236 0.00507939]]
Epoch 592: at batch 1: Training dataset Loss=0.054635, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.09714470127833863 0.0517114856053588
0.08027767856938084 0.06271208418413426
0.010426682165798118 0.012052855903071123
0.16329902798215556 0.08596602304999197
0.004539609447455462 0.002346098551870628
0.037585923005504895 0.055899916604324765
0.2188190342942562 0.08472453444719458
0.05094270416247415 0.06648803575404282
0.002353243498737001 0.00259603987359495
0.12500432643893689 0.09965980974482645
0.2400867461802818 0.1734860018259515
0.10262884910859249 0.05603788742946184
0.042772511438982974 0.017106483985400475
0.12464505602437015 0.09413492141328009
0.2564230589763312 0.2944362887163537
0.08026344176708378 0.07622803601303613
[[0.15928423 0.15966228]
 [0.15752001 0.15808707]]
Epoch 597: at batch 1: Training dataset Loss=0.053124, Batch Time=0.126
Epoch 602: at batch 1: Training dataset Loss=0.050326, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.0025424415241857012 0.0016906660373438
0.07940326564826705 0.03603679761955686
0.14740292510652697 0.09514917743109312
0.001853659727855117 0.0014036779627408153
0.029145333151973762 0.03133811935311579
0.057387802847117086 0.03600493384240038
0.12892945116087517 0.11430601812727299
0.20919307169769752 0.10917121041949057
0.009147943501304212 0.004143559434266673
0.07289977572587336 0.10793813005961614
0.06453509236206578 0.02978949161024764
0.07104291731344503 0.054049247561027074
0.010140397441189775 0.0064534251730752135
0.008854889397654553 0.011690953785443263
0.016348295074646302 0.009219148876116518
0.05138186810667378 0.034367598033094825
[[0.00466259 0.00378048]
 [0.00384349 0.00371747]]
Epoch 607: at batch 1: Training dataset Loss=0.059715, Batch Time=0.121
Loss vector (slice for the first 10 images)
[[0.1583482  0.01498411]
 [0.07838802 0.03161451]
 [0.02056852 0.05795322]
 [0.14164421 0.04412101]
 [0.11131917 0.03353036]
 [0.14258355 0.05164794]
 [0.14164421 0.04412101]
 [0.15989945 0.00595127]
 [0.12270709 0.03800837]
 [0.13502033 0.04799517]]
Epoch 612: at batch 1: Training dataset Loss=0.065601, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.30451558561522063 0.15852335139485785
0.00831004939026414 0.006926929447732299
0.007001600782484019 0.006957127035422238
0.08074419696777824 0.04114684712970596
0.09365303830263372 0.0474204280059121
0.011226330768739246 0.007603240226410946
0.13532815527170783 0.0626001417107005
0.02271246188170606 0.017303245090664682
0.3500440934098492 0.16216376134105687
0.011226330768739246 0.007603240226410946
0.061728674985772614 0.02521745063172772
0.02069863149327933 0.013106926951467793
0.0050752822593489455 0.002604957501962452
0.06628748702326881 0.08510294033807603
0.11567597331724855 0.14457305684356475
0.09365303830263372 0.0474204280059121
[[0.50683635 0.49599898]
 [0.51338923 0.48314536]]
Epoch 617: at batch 1: Training dataset Loss=0.058537, Batch Time=0.120
Epoch 622: at batch 1: Training dataset Loss=0.055177, Batch Time=0.118


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.1847292978270616 0.0663713747493926
0.03798754133328153 0.02500834510442474
0.0364718035578564 0.04088181232682806
0.01895254858690887 0.021145493688964363
0.09142908342735545 0.046907465958553676
0.10495495542048872 0.10096640620974259
0.024537174998835454 0.021201544470524688
0.014731616214938015 0.006043833026049663
0.09984250923811544 0.09059116077526003
0.08952836775389272 0.07485091196397836
0.01601466012043451 0.014938615786700983
0.007070183124356255 0.005015908039145847
0.012850125127620515 0.006193728381042814
0.09142908342735545 0.046907465958553676
0.0335583756932154 0.0327079832550444
0.013709305235071412 0.00911675493933991
[[0.26110515 0.2709344 ]
 [0.25606453 0.24976373]]
Epoch 627: at batch 1: Training dataset Loss=0.053305, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.03448121 0.00244125]
 [0.03891412 0.04706474]
 [0.12286979 0.03443807]
 [0.03579966 0.01429194]
 [0.15806494 0.03907731]
 [0.02980673 0.03030784]
 [0.14387557 0.03216254]
 [0.03702764 0.05971538]
 [0.15806494 0.03907731]
 [0.0937348  0.05093533]]
Epoch 632: at batch 1: Training dataset Loss=0.060362, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.02583090801286758 0.014986665938676941
0.004244091178005505 0.0030472171132247612
0.011452811920064754 0.01803772528612746
0.03513208262834944 0.0633073856103689
0.03541370059995863 0.036445518282619695
0.05850407686403969 0.02092796726652464
0.02946679741373437 0.025780949398571437
0.003174571306973295 0.0019756155978593822
0.007835512691515945 0.00905657695353461
0.06699366965888309 0.08760595194237447
0.09722252487177219 0.06674332049231704
0.06162002812237688 0.02759750602557674
0.1702805955165445 0.11832103159229487
0.002805698171221671 0.001410421189566952
0.029044162304817434 0.06487719073223512
0.01780626173573907 0.01514594032255266
[[0.05947955 0.06351206]
 [0.0637641  0.05973159]]
Epoch 637: at batch 1: Training dataset Loss=0.051569, Batch Time=0.123
Epoch 642: at batch 1: Training dataset Loss=0.059120, Batch Time=0.120


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.16812321651881756 0.12727690177613477
0.12996818714907477 0.0650688113707361
0.03269608633811405 0.04052505179561081
0.020572693360498384 0.02885151821593284
0.022839259517088095 0.03361429871182252
0.14892806169385153 0.19937294984382023
0.25996929349116726 0.10186175578892188
0.022225811205883517 0.025547369449755684
0.232932942090045 0.13564679962291046
0.03255639306501834 0.01864121406756772
0.11132560219164844 0.08651892931004063
0.19612804299924846 0.08400995813621812
0.08715531502677276 0.11189806719887736
0.09352020002653205 0.08358428993494743
0.16812321651881756 0.12727690177613477
0.025708826132984086 0.008718425134167965
[[0.22808896 0.24421902]
 [0.19154432 0.20212968]]
Epoch 647: at batch 1: Training dataset Loss=0.042255, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.11122157 0.01447056]
 [0.01166948 0.02554858]
 [0.07359637 0.03053614]
 [0.00432588 0.04044132]
 [0.14611644 0.04933386]
 [0.15482217 0.02111751]
 [0.14611644 0.04933386]
 [0.11122157 0.01447056]
 [0.02774462 0.02918259]
 [0.12628086 0.02427786]]
Epoch 652: at batch 1: Training dataset Loss=0.065820, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.07978892429518858 0.06653432706903398
0.09945357017704737 0.040578449184951335
0.03960657472034157 0.03063634203421155
0.009748732755261003 0.007227159451174449
0.08181192187043962 0.12011645002457517
0.13940308936316104 0.06781072605510774
0.03792534382652235 0.037358587013198885
0.17583269785991007 0.1191187301925152
0.009748732755261003 0.007227159451174449
0.04141772790459086 0.040150360802392884
0.021702337916501335 0.026909522979712566
0.017807691381042545 0.008848355304432594
0.032491827577079135 0.030978042876197216
0.1634515678019426 0.08103314658434255
0.10655693407996303 0.0736495438138529
0.10007665968360158 0.04523290098872642
[[0.06477223 0.06830067]
 [0.06578036 0.06477223]]
Epoch 657: at batch 1: Training dataset Loss=0.057492, Batch Time=0.123
Epoch 662: at batch 1: Training dataset Loss=0.061713, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.06266106960713103 0.0348379409410961
0.38862476521862277 0.19785289610898446
0.020954175583836943 0.016510734187412324
0.08617043041887129 0.05908281495520529
0.08982929778558102 0.07065787441014236
0.012333508735258292 0.00863784483493311
0.012187383551157005 0.005969385731774889
0.011274931815613343 0.005703902976810291
0.008659265210281042 0.01304514080164799
0.030119719953431545 0.021612555368659803
0.04090025102403949 0.026472791337634635
0.05629786901332956 0.0620084402338683
0.1363257037990877 0.0637297641113734
0.1255449525070631 0.0883729548843053
0.013437229418375551 0.010396033603712143
0.10426925307162094 0.09123903322812237
[[0.07762586 0.0693088 ]
 [0.08039821 0.06527629]]
Epoch 667: at batch 1: Training dataset Loss=0.059502, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.04810263 0.00018013]
 [0.00616733 0.02481675]
 [0.0086173  0.01426532]
 [0.04810263 0.00018013]
 [0.0086173  0.01426532]
 [0.10759604 0.05137202]
 [0.0086173  0.01426532]
 [0.05400696 0.04554042]
 [0.00616733 0.02481675]
 [0.10852272 0.00887235]]
Epoch 672: at batch 1: Training dataset Loss=0.055054, Batch Time=0.120


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.018628782669566135 0.008714638148095243
0.008480702479101154 0.0029278939825162983
0.007451834560928461 0.004599687792430821
0.07804403160298179 0.06610037688637052
0.07314654487597227 0.041237880109038436
0.019478233146109858 0.060424722820925536
0.06998202720004798 0.0692718033812941
0.05722680246695688 0.0250375676029947
0.008257787250321336 0.007650148457378494
0.03076946496031141 0.031063533277292072
0.03177666521889222 0.01953023445642774
0.03177666521889222 0.01953023445642774
0.03924827826241639 0.026796320203112046
0.06998202720004798 0.0692718033812941
0.01908646365387856 0.026971839588612077
0.060225362686825434 0.06310792242364889
[[0.01852435 0.01915443]
 [0.01959549 0.01915443]]
Epoch 677: at batch 1: Training dataset Loss=0.054535, Batch Time=0.124
Epoch 682: at batch 1: Training dataset Loss=0.045500, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.07508335638374142 0.06272831091903044
0.03382549055145034 0.02474672379840929
0.2243722566535098 0.09562997806805673
0.046960857252132726 0.03417759905718851
0.031870508068214676 0.028345076820297115
0.022705674208576276 0.03410144413472889
0.06908159518576484 0.09512648128840714
0.07280367352220196 0.04494379951247092
0.01609302208053176 0.014580674991979034
0.0181536729444991 0.011456633001023097
0.1457033347712695 0.07145537217707773
0.04966521537604862 0.025626970918989585
0.04807699377728625 0.03477820054424962
0.07831596609049551 0.0536277338774546
0.20905597055423186 0.09898333318236334
0.030815516297401757 0.06804055775854789
[[0.10358515 0.08695104]
 [0.11568269 0.08997542]]
heteroschedastic_sigma_s [0.04940012]
sigma_c 0.0360512026753
[[ 0.00231637 -0.00082056]
 [ 0.0012973  -0.00111742]]
[[-0.08440013  0.01075223]
 [ 0.00582242  0.02813243]] 

Epoch 687: at batch 1: Training dataset Loss=0.051691, Batch Time=0.125
		Epoch 687:  Time = 528.183, Avg epoch time=1.222, Current epoch Time=1.314

Loss vector (slice for the first 10 images)
[[0.07744373 0.00896328]
 [0.04878635 0.0271473 ]
 [0.13483365 0.01628857]
 [0.02197955 0.03050642]
 [0.03885354 0.03610819]
 [0.03243926 0.03867677]
 [0.08086978 0.02255488]
 [0.04008496 0.0376512 ]
 [0.08762565 0.0032382 ]
 [0.08933028 0.02802462]]
Epoch 692: at batch 1: Training dataset Loss=0.059983, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.059938580861398805 0.029739828241978385
0.09222193684801994 0.04029267701462921
0.022063153335183827 0.02295688361773806
0.02829732193148704 0.034379410979020766
0.02335509619930143 0.016985463129500007
0.10786360019485208 0.10967746461527018
0.08201339152657283 0.043744923781242555
0.05235854211675317 0.0424630743451741
0.04259574565582369 0.020192745038713357
0.025609014817132625 0.042425689899186526
0.04387024823305019 0.03570782405717889
0.09222193684801994 0.04029267701462921
0.15703515414917035 0.14052779224785653
0.0064005461655050055 0.0022217127245316047
0.04734783306890478 0.02376891223848299
0.03620713647722873 0.016628289894347517
[[0.09350388 0.09627622]
 [0.09627622 0.09325184]]
Epoch 697: at batch 1: Training dataset Loss=0.064812, Batch Time=0.121
Epoch 702: at batch 1: Training dataset Loss=0.048747, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.21542064778520853 0.08412143481469776
0.008970959436965131 0.003805915116167222
0.006125782221152676 0.0021334491864215713
0.026912358238824652 0.0235597949160572
0.025735231732360475 0.034449251189837285
0.01576072064462508 0.009666325087849268
0.013472644494871844 0.010881265936339549
0.026912358238824652 0.0235597949160572
0.30617710246131935 0.24445388645202246
0.02412642696243239 0.011914112444455458
0.006201938686562158 0.0022276308180660728
0.007022942502010743 0.004874150744222484
0.001887038512549566 0.0017378272581756243
0.006400303917775574 0.0033562246868023716
0.038987870303375693 0.02170850098622999
0.08236741062853525 0.0484837124570562
[[0.29991809 0.30017012]
 [0.29336527 0.29588556]]
Epoch 707: at batch 1: Training dataset Loss=0.064725, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.00065429 0.0010013 ]
 [0.08265919 0.04063041]
 [0.08265919 0.04063041]
 [0.03912311 0.04332205]
 [0.01855    0.03452662]
 [0.1018078  0.04565415]
 [0.15512265 0.00803099]
 [0.10132613 0.00118031]
 [0.12737666 0.00340339]
 [0.08265919 0.04063041]]
Epoch 712: at batch 1: Training dataset Loss=0.061832, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.040788755448126324 0.020970746146225974
0.01645849849170311 0.009034154665919858
0.05101988071174901 0.021620695454381472
0.2166569606830535 0.09296295618259212
0.023643645071821595 0.009581066143316611
0.19673127612524155 0.0798469132886478
0.05745421043778265 0.02422651652577874
0.0850180462040484 0.07376352030254923
0.0020578396529131737 0.0013840953266408991
0.03479493080244378 0.026589823188791918
0.010836487033184738 0.005175742425523444
0.040788755448126324 0.020970746146225974
0.023643645071821595 0.009581066143316611
0.03479493080244378 0.026589823188791918
0.03307677443362955 0.03699651115402095
0.015996171891658584 0.01386898451321135
[[0.05998362 0.06477223]
 [0.05973159 0.06527629]]
Epoch 717: at batch 1: Training dataset Loss=0.058919, Batch Time=0.120
Epoch 722: at batch 1: Training dataset Loss=0.056875, Batch Time=0.120


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.13397982087362692 0.061019712008228436
0.013023842277702258 0.014309754813651739
0.0019293277853149604 0.0013462610977977062
0.04249903581495573 0.022137222595271525
0.0708939164745459 0.02882406980049916
0.10323821617146223 0.05732901718452743
0.09840168573629171 0.046777965035685334
0.006518436236075509 0.006208970900719435
0.006109757195207344 0.005532076881276529
0.015967063761313405 0.012633186187630303
0.034723719909131034 0.04972299561227801
0.10493785457471816 0.05403157064836865
0.02619224261447428 0.04746430131834377
0.07372831708279648 0.038447600678680784
0.05055555338240403 0.029240424313418827
0.10220130979708131 0.05376535476365769
[[0.20137358 0.20036544]
 [0.19230042 0.2038939 ]]
Epoch 727: at batch 1: Training dataset Loss=0.049439, Batch Time=0.119
Loss vector (slice for the first 10 images)
[[0.0624354  0.00843052]
 [0.0624354  0.00843052]
 [0.10606602 0.00666343]
 [0.05554434 0.05104618]
 [0.0778426  0.05204247]
 [0.09096434 0.00326916]
 [0.12761607 0.05254681]
 [0.0714857  0.05260289]
 [0.13657638 0.02005876]
 [0.09023589 0.03908688]]
Epoch 732: at batch 1: Training dataset Loss=0.055540, Batch Time=0.118


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.03102801831779267 0.01714336244504353
0.010348898976305865 0.009183797569005786
0.21729268648609334 0.265470827612276
0.03492393011235162 0.07348003422193358
0.04168718859952136 0.07587517166909372
0.019179633493602743 0.012329483357603923
0.019934795028426322 0.00899824608473856
0.5700762390181424 0.36550773470594866
0.09406742050844308 0.0661153272133131
0.07176280700839754 0.06496074218559095
0.10460076712612931 0.04617145625462618
0.07639549497940834 0.038578735057294374
0.15064307141256705 0.09082396214893614
0.239613109360306 0.12616962007526322
0.21729268648609334 0.265470827612276
0.03158500691229804 0.017218190451429
[[0.01764224 0.02570727]
 [0.01234957 0.02192678]]
Epoch 737: at batch 1: Training dataset Loss=0.049357, Batch Time=0.124
Epoch 742: at batch 1: Training dataset Loss=0.051965, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.039258594344283804 0.06901734863557721
0.013763383498297088 0.0062011719660082735
0.06534009031413035 0.04212328238224358
0.06247685273619297 0.046067128003369054
0.01906083012274351 0.010887438573423067
0.040122756552834815 0.09730057718687103
0.18548817806477658 0.11744169368706074
0.10350549934753772 0.20695780643016118
0.7113994400796244 0.3507544145448552
0.009819821539108542 0.033244898124246136
0.0367933351207661 0.04098958061911366
0.009819821539108542 0.033244898124246136
0.2360791503958808 0.09142026668867428
0.12700005029333283 0.06590491834455056
0.06541568144435672 0.03381123877157744
0.004120113396516523 0.0027608315032515277
[[0.04416861 0.05298973]
 [0.03213408 0.0409552 ]]
Epoch 747: at batch 1: Training dataset Loss=0.054103, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.11723057 0.01634076]
 [0.09108768 0.02244061]
 [0.12659378 0.03373612]
 [0.09718323 0.02651503]
 [0.13612345 0.01962917]
 [0.08293992 0.02054523]
 [0.08983068 0.03644551]
 [0.11723057 0.01634076]
 [0.09194816 0.01897369]
 [0.11740951 0.04205409]]
Epoch 752: at batch 1: Training dataset Loss=0.055070, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.0328604401692445 0.011477198199859212
0.02198420104935117 0.017312311672793148
0.057261459943564574 0.022365812374486017
0.04648846452901978 0.024929787649393412
0.04337145859759772 0.020583358314062904
0.007500978801211922 0.004824862708633551
0.009633917349777477 0.010098437025541963
0.06853416510537613 0.03447380159422201
0.10196547204127171 0.06851748012498246
0.06034378340881119 0.02198234744402626
0.026972716553660625 0.026215688268012502
0.015854559644928656 0.009592901659252
0.07317686919546418 0.031244626113514223
0.006585170722725131 0.004300687626221923
0.0028357158120018156 0.002008533635810925
0.01992409916817195 0.018101082481300396
[[0.04832714 0.04750803]
 [0.04561779 0.04731901]]
Epoch 757: at batch 1: Training dataset Loss=0.050099, Batch Time=0.122
Epoch 762: at batch 1: Training dataset Loss=0.054629, Batch Time=0.120


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.02904263940388674 0.016730499877910428
0.08610231142349178 0.06297439258886002
0.05347734179144936 0.03646926243246402
0.15799335723193053 0.07060629848437909
0.05528939485851936 0.02583961464645825
0.028589142532560174 0.01375227212097886
0.010157152198702946 0.013278845623078508
0.03776769790740886 0.016466172017882884
0.1206712726113146 0.09355517556502513
0.09467746920537223 0.06693857069397474
0.010498812165966598 0.0052830239904935496
0.05125170258512668 0.032358520363195566
0.15850117179645196 0.26378621806733693
0.024662205547601346 0.011972496002005732
0.10714183859469362 0.09896136906633236
0.05508092217990068 0.0751865988873275
[[0.04712998 0.04511373]
 [0.04914624 0.04183731]]
Epoch 767: at batch 1: Training dataset Loss=0.058476, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.12361675 0.05454633]
 [0.05283822 0.03197077]
 [0.083854   0.02301116]
 [0.04352775 0.01768667]
 [0.06707518 0.01529251]
 [0.12078    0.0459692 ]
 [0.15951974 0.03636309]
 [0.11403223 0.04796744]
 [0.06984899 0.01767669]
 [0.12361675 0.05454633]]
Epoch 772: at batch 1: Training dataset Loss=0.053954, Batch Time=0.120


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.02414298656995939 0.017418368221075668
0.0565138898830142 0.04560623482348448
0.005739685182916787 0.0034940871268716966
0.22163368906683445 0.08835509015689984
0.21859080523078944 0.19896145078126035
0.011953276379614408 0.004148189067326935
0.02983570994972773 0.013735494528244758
0.03169797636299698 0.03149485219032837
0.12785237735415933 0.11173583081347913
0.03169797636299698 0.03149485219032837
0.04832776056808541 0.028943929487018447
0.23256400836885405 0.11044499210971934
0.03606047097397003 0.04972449186164902
0.036486686420938774 0.014828882867225977
0.03644713145226497 0.024548269803745866
0.01974918215691357 0.025966054401803905
[[0.02923571 0.03326822]
 [0.02772352 0.03049587]]
Epoch 777: at batch 1: Training dataset Loss=0.053611, Batch Time=0.122
Epoch 782: at batch 1: Training dataset Loss=0.049742, Batch Time=0.120


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.02940721592382367 0.06094444869893816
0.01877875836515841 0.01037526858572002
0.0106011492228415 0.0088842623217963
0.015295823178582246 0.017167131399427764
0.0525340311704241 0.034723899948505145
0.03722608824653051 0.028741480393248125
0.047641098702729145 0.031560389031078155
0.015007368516707587 0.011888672961085943
0.06208037225277252 0.11074979020004835
0.11321754402470674 0.05011032646495576
0.01340539084041481 0.007978878500478619
0.0422392508662881 0.02832499775483881
0.006501245939178446 0.005493891647349341
0.01982701534538478 0.011487286947221583
0.31799951045495334 0.2059596834669876
0.062170150201275476 0.031260328198932194
[[0.00982925 0.00403251]
 [0.00680486 0.00579674]]
Epoch 787: at batch 1: Training dataset Loss=0.057218, Batch Time=0.118
Loss vector (slice for the first 10 images)
[[0.01440595 0.037396  ]
 [0.00776208 0.01237378]
 [0.03775992 0.00069749]
 [0.12458344 0.0077305 ]
 [0.01440595 0.037396  ]
 [0.00776208 0.01237378]
 [0.08162692 0.05775676]
 [0.10060252 0.02234391]
 [0.08162692 0.05775676]
 [0.03775992 0.00069749]]
Epoch 792: at batch 1: Training dataset Loss=0.048518, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.06980810912562774 0.037144548690303536
0.017406148046646308 0.008039671374125304
0.08207383351052044 0.052229613293246546
0.018014582522283717 0.010243915548848199
0.004462604037803963 0.0032333831114703996
0.2003810715389589 0.08619287535110724
0.005389072397162575 0.002966167452543945
0.014658682451063498 0.017064666943632206
0.03813363191663566 0.12388144656645267
0.002600999080799582 0.001450540209227692
0.017594011580712277 0.006715089387559214
0.06373118076515993 0.037378665015231176
0.01553326839317215 0.07441288893154764
0.024876512159067943 0.02004783415446893
0.017387416574303316 0.019545919995975793
0.037357772814946344 0.025856255511230337
[[0.09829248 0.08443072]
 [0.09778842 0.08997542]]
Epoch 797: at batch 1: Training dataset Loss=0.049443, Batch Time=0.123
Epoch 802: at batch 1: Training dataset Loss=0.057109, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.11818358333408696 0.06790929180841421
0.269362255498514 0.14003137839784838
0.03960368275994597 0.015121014493187852
0.08492313429223941 0.031686008501030975
0.0009502155889498098 0.0006487793261344794
0.15185040010101147 0.05815095155031787
0.061920398684037536 0.023163891429342564
0.07046188347876381 0.05738923184418355
0.07673283621980431 0.03583933009347016
0.013150543777811663 0.00849918962592899
0.05802920844793036 0.034634931997254335
0.03391288414056248 0.016121456748423402
0.041859033827388537 0.031068587045680416
0.05225760298562676 0.034833962212090494
0.023776948677584286 0.011896551183822516
0.04027806916204213 0.016304745605307903
[[0.16079642 0.14668263]
 [0.15752001 0.14592654]]
Epoch 807: at batch 1: Training dataset Loss=0.043999, Batch Time=0.122
Loss vector (slice for the first 10 images)
[[0.12870642 0.00087284]
 [0.12643586 0.02205806]
 [0.09584222 0.04762608]
 [0.12870642 0.00087284]
 [0.12870642 0.00087284]
 [0.13867025 0.0163532 ]
 [0.074677   0.00883514]
 [0.12870642 0.00087284]
 [0.07354118 0.04213095]
 [0.03065144 0.0030501 ]]
Epoch 812: at batch 1: Training dataset Loss=0.061109, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.0046656264070641384 0.001651819281484422
0.09657188475745215 0.05056266735769003
0.1151499503216371 0.06246649145647626
0.011455925995268235 0.010480698692808358
0.00666798790206613 0.0023420211117627446
0.08355601561403603 0.0535494680362343
0.12778865025334518 0.11671418335792547
0.003183107791311812 0.0013076362870784925
0.03381549940533901 0.030325024239448863
0.05945466068563121 0.038240737978595035
0.01906687748282998 0.013548414072209204
0.053994975384579647 0.033649508279002406
0.004928076420537053 0.0038088352276694844
0.06525045082905478 0.023732400571164984
0.027326078997781167 0.020617382744888637
0.016294109100914778 0.01488681667962961
[[0.0063008  0.0063008 ]
 [0.0063008  0.00655283]]
Epoch 817: at batch 1: Training dataset Loss=0.051321, Batch Time=0.124
Epoch 822: at batch 1: Training dataset Loss=0.056840, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.051267314218932825 0.02971317985762162
0.039368126691044836 0.017718798786778178
0.024136812304402566 0.020109373043058605
0.015484847189185302 0.005825978739286634
0.008065301141868986 0.010894933464584372
0.018047569996966217 0.009383695012164206
0.254983881449661 0.31396608275103777
0.010933726595203197 0.008555159023533576
0.020018150641658572 0.03095927364386925
0.032138249772408756 0.029112137594930004
0.08579098640757721 0.05681084726503126
0.06690597233655815 0.18776342687323896
0.0033260449000556136 0.002325474476875623
0.15930881859634383 0.10427347419571698
0.02084286938203661 0.010604501255265412
0.11796930840779396 0.045031167590974995
[[0.04857917 0.05223363]
 [0.05015437 0.05160355]]
Epoch 827: at batch 1: Training dataset Loss=0.065781, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.08498076 0.03979279]
 [0.08498076 0.03979279]
 [0.1503839  0.00966534]
 [0.04153581 0.02161257]
 [0.14107457 0.02070784]
 [0.00595259 0.02523611]
 [0.04467695 0.0596117 ]
 [0.14107457 0.02070784]
 [0.0972458  0.03539901]
 [0.06281944 0.00079276]]
Epoch 832: at batch 1: Training dataset Loss=0.054727, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.14924380168672258 0.07433629118662277
0.0956105118244075 0.09980783174121816
0.25493431039837233 0.19960719287125167
0.04910032770984252 0.03728197078246553
0.0379346205560438 0.013254961697374319
0.03816533300146485 0.020942825395981746
0.06735293138065312 0.0756750266280046
0.08736667879958304 0.0375495092823359
0.08426365861683038 0.06308505179574916
0.18092555114890274 0.08678813203321585
0.061525395935845495 0.050855249231841274
0.06420567323584869 0.059814419145903304
0.12203272080376149 0.18336467432323725
0.10121164303739505 0.07682863345489048
0.04523904171114168 0.02298400908806379
0.16422940835294497 0.1059440420247586
[[0.20849349 0.22468653]
 [0.22229223 0.23438977]]
Epoch 837: at batch 1: Training dataset Loss=0.051296, Batch Time=0.121
Epoch 842: at batch 1: Training dataset Loss=0.056818, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.008828065614810754 0.008875714703404898
0.0478816282283141 0.02868444103948028
0.09739698813511666 0.038194641531511093
0.010187014086818102 0.012557075331942367
0.6775763493302236 0.3719620559012138
0.0014523932828278685 0.0011152752189774568
0.026744358693624015 0.028367455305377837
0.09566805607506979 0.05035002230857835
0.029255072202449384 0.025519078736091324
0.09023757215368278 0.040215503053128154
0.09732661942024379 0.05782879058278509
0.21608383552131727 0.08414954878715267
0.015706844251480945 0.009530233791672994
0.041945131466171404 0.025936067219889287
0.006527019844265958 0.007186018115934148
0.03223155038642034 0.05430921225241351
[[0.00025203 0.        ]
 [0.         0.        ]]
Epoch 847: at batch 1: Training dataset Loss=0.052280, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[4.87573344e-02 1.86266306e-02]
 [1.56777312e-01 7.21111753e-03]
 [8.02275578e-03 5.64857961e-02]
 [1.38962348e-01 1.80851961e-02]
 [1.13148507e-01 4.74331326e-02]
 [8.71393205e-03 2.58517836e-02]
 [8.49488131e-02 1.23661209e-02]
 [1.01282557e-01 6.38261260e-05]
 [5.65867266e-03 2.58902699e-02]
 [5.65867266e-03 2.58902699e-02]]
Epoch 852: at batch 1: Training dataset Loss=0.049132, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.025518111021107792 0.02317644023105325
0.3031260944795804 0.1594149938403259
0.012770888243679224 0.005278543969622433
0.022139397295600105 0.011123530415281399
0.004337628305676411 0.0023329428374217983
0.0021460177806691405 0.0007216503753493186
0.09921592498798759 0.04390191907229566
0.08242948022626706 0.030394437698337825
0.0658747699884259 0.030489059241560135
0.017680908151326946 0.008003941558573887
0.013559324724979005 0.008245807963907139
0.07527654142644025 0.03799903742572323
0.02663388701564262 0.05725011128092877
0.00934331294738966 0.007979247109546374
0.0642115340585434 0.027752003726879245
0.005351830616236342 0.0037868832040729486
[[0.01354672 0.0134207 ]
 [0.01297965 0.01039632]]
Epoch 857: at batch 1: Training dataset Loss=0.057976, Batch Time=0.122
Epoch 862: at batch 1: Training dataset Loss=0.056793, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.027312522899206826 0.030025617214559094
0.03945739220262112 0.019900837661435307
0.08244277102510189 0.04594482332443009
0.34301858189803625 0.14001560703798271
0.054985861223478594 0.028820429623683554
0.06376754189382083 0.12888180713138342
0.029404239337791793 0.0373098053276703
0.011777484465532062 0.014992564842003482
0.012241996360386587 0.031304367833752775
0.037772766473834984 0.023570070317338886
0.11304580840578637 0.048172414188749
0.04752603526641508 0.03457620840167135
0.012458922889935664 0.03114253731705977
0.269177161649381 0.23320831847551102
0.269177161649381 0.23320831847551102
0.16264920600514188 0.3308452997295721
[[0.00648982 0.00648982]
 [0.00648982 0.00604877]]
Epoch 867: at batch 1: Training dataset Loss=0.051714, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.1333354  0.04891861]
 [0.14021195 0.00500057]
 [0.03211982 0.0384947 ]
 [0.0458296  0.02957516]
 [0.13601523 0.01835208]
 [0.09032701 0.02462451]
 [0.10722996 0.02415239]
 [0.09144246 0.03322849]
 [0.10153649 0.00964892]
 [0.00984475 0.03781408]]
Epoch 872: at batch 1: Training dataset Loss=0.058123, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.08692613034196839 0.07398580659568733
0.01198191531345838 0.011456238786704053
0.13888188502915 0.058617802710550324
0.014584425775291177 0.008507970289475917
0.08027485580934979 0.04193630017865446
0.07747575686564634 0.10772592942880628
0.01317259600153764 0.011601134614637174
0.04372029656237686 0.04097847707973223
0.14193267853147518 0.1055948304048449
0.013877620123916579 0.009402138004920308
0.1876245201373763 0.08406142916556657
0.07550562613980105 0.05752668411675939
0.09528361267808805 0.042221803688200436
0.04384290816446068 0.03884986771135975
0.050740495159728116 0.05709775189090259
0.07729083331012632 0.05409115031202611
[[0.08846323 0.07712179]
 [0.09375591 0.08543885]]
Epoch 877: at batch 1: Training dataset Loss=0.047626, Batch Time=0.120
Epoch 882: at batch 1: Training dataset Loss=0.051814, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.08161619099047535 0.10247207962770488
0.071995798940236 0.03417243630642764
0.01323189577569206 0.0326410352501356
0.0043445217269608705 0.001670623898463904
0.15353548347707147 0.062160007011336005
0.11171426341510937 0.04934104529361764
0.01753469932450713 0.011604969177550009
0.07066662387614997 0.03334575323168886
0.020230694024647278 0.019028515422075624
0.07363049204261785 0.041380710324709635
0.08451377067785426 0.11735202497604913
0.004815954945596168 0.0018485338760417178
0.0545885616041879 0.07226712983284311
0.015185613022801192 0.006784545703810175
0.05379278177914237 0.11983381302597128
0.03243286525198741 0.024222035470122732
[[0.01764224 0.01688614]
 [0.01991053 0.01789427]]
heteroschedastic_sigma_s [0.15291557]
sigma_c 0.0348783781167
[[ 0.03121166  0.13131006]
 [ 0.0171771  -0.05385486]]
[[-0.01506709  0.036182  ]
 [ 0.01547264 -0.01622202]] 

Epoch 887: at batch 1: Training dataset Loss=0.059214, Batch Time=0.124
		Epoch 887:  Time = 778.038, Avg epoch time=1.227, Current epoch Time=1.292

Loss vector (slice for the first 10 images)
[[0.12578362 0.01880661]
 [0.03778348 0.02073458]
 [0.00022676 0.02077671]
 [0.01266942 0.01109794]
 [0.12578362 0.01880661]
 [0.0405163  0.03997901]
 [0.08522704 0.03244917]
 [0.01266942 0.01109794]
 [0.15291557 0.03487838]
 [0.10558106 0.01080492]]
Epoch 892: at batch 1: Training dataset Loss=0.056815, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.29064565496413763 0.1541410521072031
0.013865510003007842 0.023986049989568686
0.14082249034203187 0.05461148298536147
0.044881186519731386 0.026437771255392765
0.040470648473664994 0.03993777133029663
0.07944175825176103 0.035946542393158545
0.03882784288310859 0.014789612058840407
0.18209880595209427 0.13105352499528372
0.08349909439556313 0.03236386937734591
0.09197256216886007 0.05253563500821178
0.003448654574449428 0.0025124534047773765
0.09133513296147555 0.07249271752607651
0.007811438579755858 0.0065859041535951904
0.04105914298498359 0.041712261997778555
0.007512015971506081 0.0046973039147715275
0.022336816487047884 0.008832428387444056
[[0.41717598 0.51439732]
 [0.48012099 0.48610672]]
Epoch 897: at batch 1: Training dataset Loss=0.060555, Batch Time=0.125
Epoch 902: at batch 1: Training dataset Loss=0.054979, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.1246146528789609 0.08284025344893778
0.013636334930708927 0.006999654170910882
0.15699683557761546 0.0780268954264299
0.04026962396531175 0.025315515502360927
0.06868624056895456 0.040142926390570974
0.020979680282303015 0.01687170360482931
0.19191310605123846 0.2048459444931767
0.11236696485737063 0.10620686268171826
0.07059114428906632 0.0581789301091663
0.011894632281472362 0.012792394332792702
0.01643704330522411 0.013556510060595554
0.3318056611517761 0.1562301894542029
0.008421904563644222 0.004782926405248731
0.11650522997850032 0.05284691491594613
0.21656604055631945 0.18874038178039132
0.061499926824558404 0.057436715760890274
[[0.22078004 0.18751182]
 [0.18625166 0.21019469]]
Epoch 907: at batch 1: Training dataset Loss=0.061886, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.01314284 0.0136497 ]
 [0.08409587 0.01917652]
 [0.11188782 0.05064199]
 [0.09423885 0.04542467]
 [0.01871394 0.00331842]
 [0.01314284 0.0136497 ]
 [0.08409587 0.01917652]
 [0.08189455 0.05939149]
 [0.01314284 0.0136497 ]
 [0.03294202 0.01752763]]
Epoch 912: at batch 1: Training dataset Loss=0.050966, Batch Time=0.118


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.033088215411908095 0.02460673027142448
0.0055086314802672565 0.006484315017841565
0.026378019887445703 0.010029699677007998
0.06610718215583233 0.03233348278232004
0.052093048223383676 0.02157687396990659
0.10695174258724904 0.06185995892853579
0.036436280802019105 0.053557984577011046
0.0277639478852727 0.016700562032902832
0.008809283201319573 0.004332224070127704
0.02495351853482397 0.013815115055766023
0.011803420846551838 0.007985915442766082
0.012747767887513639 0.01483688436147618
0.03823949647167524 0.016170407327697197
0.002478090407366018 0.001475535672241029
0.01481863678939721 0.007928306144894245
0.15187092657484058 0.2316646710334305
[[0.02388003 0.02469914]
 [0.02734547 0.02306093]]
Epoch 917: at batch 1: Training dataset Loss=0.062717, Batch Time=0.123
Epoch 922: at batch 1: Training dataset Loss=0.050503, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.08985960971009277 0.060397577687257294
0.09319783586632013 0.06174689099560573
0.04703475052792072 0.03118295950321891
0.06079890510303443 0.030711455740482146
0.007643016007719439 0.005944127329954652
0.1221145649957407 0.05957910995508816
0.0016951359965799995 0.0010989967933475347
0.017893011176795426 0.009443322467248418
0.08150617690742479 0.0589175808835035
0.15075465929364107 0.12250030484717626
0.08246802959234856 0.03847023824610487
0.008567288474452539 0.004208209971153884
0.01077085336951189 0.007767027569128119
0.010864180902038001 0.004764101432249871
0.15645156483685696 0.05993367260912636
0.005768427959006672 0.004422777636561297
[[0.06880474 0.07863399]
 [0.06956083 0.13760948]]
Epoch 927: at batch 1: Training dataset Loss=0.057874, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[5.03769568e-02 4.66045189e-02]
 [7.05210843e-02 1.00878059e-04]
 [1.07045031e-01 2.75931860e-02]
 [1.07045031e-01 2.75931860e-02]
 [4.12931114e-03 6.16045204e-03]
 [1.50900982e-01 3.77417308e-02]
 [3.43235030e-02 5.37983167e-02]
 [1.16130824e-01 2.46568609e-02]
 [4.21811324e-02 1.45703379e-02]
 [6.32051503e-02 5.08299600e-02]]
Epoch 932: at batch 1: Training dataset Loss=0.055787, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.007027630417795194 0.004874599415189487
0.13062834054678873 0.11254435888421332
0.025852759275572623 0.012358122286858115
0.22169004786212554 0.10469831608521797
0.1309757104174878 0.06374323772744936
0.02526517335763856 0.014766356469930738
0.09941904347385755 0.07488996617972461
0.005349288627567006 0.0033014482769850627
0.2034583764029838 0.2695282141208354
0.11259961451493439 0.054079891236199384
0.22528221158808037 0.09312160630792829
0.02962787851509674 0.026510004549065276
0.04162188474358608 0.062461458910576105
0.021255390294593113 0.01929073052825463
0.020555492498310457 0.012048911861637333
0.027407807878649493 0.022185426199824013
[[0.00604877 0.00604877]
 [0.00756096 0.00655283]]
Epoch 937: at batch 1: Training dataset Loss=0.059075, Batch Time=0.122
Epoch 942: at batch 1: Training dataset Loss=0.060701, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.024074244647003695 0.010155812501472053
0.09573703352872798 0.06169165386113251
0.003997773884481859 0.003162008724508268
0.08123874863353464 0.06287444468134638
0.05907064408279172 0.03449410737262577
0.026203706653411984 0.018878095983430786
0.0698895457230364 0.036361118516409524
0.10966281917484721 0.08382824520399265
0.1182456837248651 0.06408426444621583
0.019036404130654105 0.033368219507873005
0.3279537047658181 0.15772064787108986
0.011266995233778943 0.00903674719123212
0.08512133027742053 0.06836567844892578
0.08750363574174003 0.06505667742427863
0.057332613189649884 0.05961818772016813
0.06889986938245585 0.07551745689142046
[[0.03276416 0.03301619]
 [0.03125197 0.03049587]]
Epoch 947: at batch 1: Training dataset Loss=0.044460, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.15913404 0.04480884]
 [0.12484283 0.0112443 ]
 [0.14971921 0.03975282]
 [0.03904042 0.01571954]
 [0.1287024  0.05555142]
 [0.07604529 0.01761468]
 [0.09788525 0.03865436]
 [0.0052866  0.04630229]
 [0.03904042 0.01571954]
 [0.1287024  0.05555142]]
Epoch 952: at batch 1: Training dataset Loss=0.061815, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.003585925987567684 0.002542097170460752
0.15452264475129596 0.06516858109150238
0.005942642159551426 0.0040462622921134585
0.04014874967157667 0.023942972988144486
0.10028766862220095 0.04469193270141793
0.04385114083860486 0.06545391375753534
0.07718556587845526 0.04111049360632362
0.17237956012824007 0.11082397092291092
0.034703264602343076 0.04434202541642699
0.07718556587845526 0.04111049360632362
0.025050180281622758 0.02428091186244563
0.17237956012824007 0.11082397092291092
0.0848749283575998 0.08119138711917291
0.17340481698519739 0.07752529697277305
0.08886507239525798 0.07006788514487981
0.04070075419723196 0.061644800747252426
[[0.00882112 0.00277235]
 [0.00730893 0.00856909]]
Epoch 957: at batch 1: Training dataset Loss=0.061703, Batch Time=0.119
Epoch 962: at batch 1: Training dataset Loss=0.051830, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.19683880199595905 0.1524165223027033
0.34573426753136616 0.20913840960961783
0.08540037639688336 0.060736405588439714
0.09318174354016406 0.050930688227749724
0.07559948825598894 0.04069563959382826
0.008889619941472304 0.004312088438332762
0.017856485648583664 0.016948046775127647
0.11184284166100156 0.05305082791806874
0.0354198287372256 0.024901493286060106
0.13324429460487863 0.07405011585285294
0.015502118237626483 0.009890706120546064
0.08889411513223244 0.06747009572253725
0.07321065274919647 0.02935639729408531
0.04452688379823044 0.023328139831603877
0.08133461049062873 0.03828695414154476
0.013405210093317521 0.00672272230211797
[[0.4581942  0.4599584 ]
 [0.44156009 0.42341378]]
Epoch 967: at batch 1: Training dataset Loss=0.053579, Batch Time=0.122
Loss vector (slice for the first 10 images)
[[0.05020766 0.01396158]
 [0.05577701 0.02979761]
 [0.08003932 0.01370572]
 [0.0323251  0.0128512 ]
 [0.08842383 0.00025046]
 [0.00675915 0.05432775]
 [0.07133244 0.05432228]
 [0.11875374 0.0307777 ]
 [0.03294529 0.05335771]
 [0.0323251  0.0128512 ]]
Epoch 972: at batch 1: Training dataset Loss=0.058532, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.09573796419921621 0.05605244055675037
0.09499287649493482 0.1149843066402157
0.022351008077157175 0.016930694284419116
0.12110101838321441 0.06028684740345626
0.018243124003936018 0.008505293228672202
0.018243124003936018 0.008505293228672202
0.09499287649493482 0.1149843066402157
0.015595803407916442 0.011140425941431782
0.05132267310094818 0.022786433549065812
0.01744613274842699 0.04140398066721789
0.057031363852475536 0.04555975030431435
0.006657273816293774 0.007246610542153873
0.09979919120839043 0.06626845847669684
0.08675830383856731 0.1259834147288902
0.0812246214452017 0.06043471899153543
0.02236206159916776 0.03384175508370261
[[0.19141831 0.19475773]
 [0.19381261 0.20105854]]
Epoch 977: at batch 1: Training dataset Loss=0.059191, Batch Time=0.121
Epoch 982: at batch 1: Training dataset Loss=0.062328, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.033363836943594904 0.08846454898665469
0.04197799301907068 0.04264967741935419
0.008717101698508145 0.013629110261474363
0.06491965494098118 0.036417551734965775
0.011713177568299926 0.013779512866438028
0.0469731837004943 0.021904650816552802
0.1324238032766143 0.23059555586373753
0.08180583025674792 0.04889743631804883
0.010508003396052068 0.012539750668331904
0.031026587713795628 0.03186148175039866
0.48053552205863426 0.408584048517124
0.09924169502515667 0.045308731092496494
0.08704403578039432 0.043733599698307465
0.22320568954476272 0.16551157156899488
0.03767424728945823 0.026505186146129463
0.03854326758254434 0.03354284953453281
[[0.0054817  0.0054817 ]
 [0.00573373 0.0063008 ]]
Epoch 987: at batch 1: Training dataset Loss=0.055498, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.09489266 0.01009467]
 [0.06467779 0.02693185]
 [0.06076017 0.04221723]
 [0.04774004 0.04160217]
 [0.06097009 0.00393854]
 [0.06365905 0.00487063]
 [0.0860206  0.00450654]
 [0.026134   0.02371615]
 [0.01997603 0.01849267]
 [0.09339935 0.04458036]]
Epoch 992: at batch 1: Training dataset Loss=0.051371, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.047493881350696654 0.03742872862890495
0.2637844695361622 0.10612698454616369
0.01477650325880564 0.01658357874930773
0.24726888474803843 0.3747580478306419
0.0483515424097547 0.0900326453452052
0.029516114692320627 0.0487411154186771
0.05599855410088139 0.04275882130337246
0.24861506998840355 0.14149075222895227
0.044809618924103845 0.08426150774857324
0.04399401738118769 0.14475484347376752
0.053173988896894286 0.04014506746078629
0.12001601669722106 0.06078537893888188
0.041072787536149846 0.03288572352266303
0.10417572845705081 0.19104544325678313
0.2288120172318031 0.16105395883406629
0.14379166745653293 0.0904686907144198
[[0.07989415 0.08140634]
 [0.07107303 0.07082099]]
Epoch 997: at batch 1: Training dataset Loss=0.057442, Batch Time=0.124
Epoch 1002: at batch 1: Training dataset Loss=0.054121, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.0018792990345639637 0.0013345327265621813
0.022059253790541078 0.023181856898375527
0.010419979092233689 0.007243964388544929
0.0652102055355126 0.04719499232620229
0.10207057123253094 0.04405909016425099
0.2924983743351959 0.13515702637318244
0.016142208625087306 0.011529528328211896
0.009895879929034956 0.011862873317193599
0.03654821576360234 0.015338752476299575
0.0036948709155488757 0.002682516982441093
0.03122864479817622 0.01288148754111225
0.2538780986869966 0.35196362268964426
0.14674518673243142 0.059017315759823066
0.006258805114845778 0.0046034678740569665
0.0307853015716546 0.012915064533929634
0.10985227100216832 0.06532034375356247
[[0.00126016 0.00151219]
 [0.00176422 0.00176422]]
Epoch 1007: at batch 1: Training dataset Loss=0.063563, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.01432118 0.00719342]
 [0.02661033 0.03362934]
 [0.02661033 0.03362934]
 [0.15106355 0.03356491]
 [0.07735049 0.04776712]
 [0.027812   0.03868252]
 [0.08114443 0.00653807]
 [0.05359229 0.01876735]
 [0.08569656 0.0585471 ]
 [0.02661033 0.03362934]]
Epoch 1012: at batch 1: Training dataset Loss=0.059470, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.21605571192841388 0.11001777851134303
0.08072159577515592 0.04578372026487975
0.20172532186396097 0.27813550587822533
0.026594853116330697 0.013949745232426678
0.011494257051486656 0.005828856031247981
0.019934987303116714 0.0184651959397762
0.056366526346089074 0.12969829660846
0.21605571192841388 0.11001777851134303
0.03426628121386788 0.021967770124635366
0.07937483433998693 0.054379440891286907
0.21605571192841388 0.11001777851134303
0.03734172854729323 0.03899732858850099
0.08072159577515592 0.04578372026487975
0.006358024259894179 0.004148702931170984
0.09577702403780108 0.0603501186114625
0.05762197060685992 0.03238389785467615
[[0.38132444 0.33041397]
 [0.37729192 0.38863337]]
Epoch 1017: at batch 1: Training dataset Loss=0.060303, Batch Time=0.125
Epoch 1022: at batch 1: Training dataset Loss=0.055955, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.1754203971252366 0.24757342596982634
0.14091538908700585 0.09714969802131955
0.02465784931311532 0.013969827980646643
0.016540269676816877 0.013451474290028406
0.17459253935807606 0.1578072129205851
0.010606234192822583 0.006594794386169349
0.3180654056823755 0.2261882275186906
0.16270255540570133 0.07274607434759961
0.0984270097000639 0.137408844013625
0.06571167756032281 0.11726101156479914
0.02465784931311532 0.013969827980646643
0.008433034979741905 0.007180996145418043
0.0246596356629194 0.021546988185787267
0.003317872785664644 0.0029807464811439895
0.07446790543255588 0.03538710269614147
0.022916986960319186 0.011375748969111098
[[0.05084746 0.0504064 ]
 [0.04927226 0.05235965]]
Epoch 1027: at batch 1: Training dataset Loss=0.055450, Batch Time=0.121
Loss vector (slice for the first 10 images)
[[0.03434428 0.00578473]
 [0.14894242 0.02482481]
 [0.06733878 0.03461989]
 [0.14894242 0.02482481]
 [0.00520674 0.01550711]
 [0.03434428 0.00578473]
 [0.15198987 0.04326661]
 [0.06460978 0.04321506]
 [0.00520674 0.01550711]
 [0.0334454  0.04326205]]
Epoch 1032: at batch 1: Training dataset Loss=0.052435, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.0707034080700879 0.0420037093870465
0.003931581633393577 0.0025595404230638985
0.09278250568473823 0.053985432366413316
0.016551235708874845 0.05705883756733528
0.0656611815265582 0.08761110096039876
0.01633777417333171 0.020944713072732993
0.048435064328629096 0.04184725827678492
0.32058393833571586 0.30438481204839146
0.08888837735744026 0.07729650140336033
0.05422867688056954 0.032760728136010296
0.01633777417333171 0.020944713072732993
0.21158907370724478 0.1355698913319973
0.11623811316529498 0.0622181527994115
0.02929964871816182 0.044021673382038815
0.039018616681904206 0.028012145931141604
0.07174103548544508 0.03075133936982961
[[0.09923761 0.0999937 ]
 [0.10144288 0.0999937 ]]
Epoch 1037: at batch 1: Training dataset Loss=0.051957, Batch Time=0.125
Epoch 1042: at batch 1: Training dataset Loss=0.055195, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.09107249836885245 0.09436341413439812
0.06465970464086013 0.025601671643574245
0.09884959482997502 0.09108491532829034
0.06465970464086013 0.025601671643574245
0.017554635442395217 0.012396783657553867
0.023976222446215445 0.042998965846276826
0.153151216971537 0.08572119070597281
0.05188505914037478 0.024303356059632835
0.050153646644714556 0.024200208575218606
0.009305017422560624 0.0061145858795468395
0.12153860177817499 0.1072475800423552
0.010531439124278563 0.0035931566068375057
0.01618709952202657 0.012764288483544847
0.030237414788786054 0.020654692585262084
0.01759584985045759 0.04401338481272615
0.01840360904212268 0.010654211497028256
[[0.15928423 0.13181274]
 [0.21120283 0.08241446]]
Epoch 1047: at batch 1: Training dataset Loss=0.046223, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.12905203 0.01185566]
 [0.08216131 0.05902027]
 [0.0359145  0.057601  ]
 [0.09764605 0.02711771]
 [0.09167978 0.00690528]
 [0.14994604 0.01286776]
 [0.10168889 0.03522328]
 [0.08216131 0.05902027]
 [0.05507822 0.04758886]
 [0.09689772 0.02292999]]
Epoch 1052: at batch 1: Training dataset Loss=0.057436, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.007839243014646335 0.004739445019868241
0.019958653766522616 0.012974834444520524
0.03612580660390563 0.028448378656457392
0.0745167525818724 0.08720303191528131
0.005528202262816251 0.00241144151831667
0.07918306834171318 0.03176960884923644
0.025672830394145585 0.032544891962384345
0.03073894832671442 0.012089204685790738
0.007701420704059103 0.006570501491200432
0.015620608199119257 0.013612196841015082
0.12129614557798618 0.051636289294550934
0.025672830394145585 0.032544891962384345
0.04701138403708427 0.04256597861961919
0.007034018122589636 0.0052428181840526705
0.015620608199119257 0.013612196841015082
0.020850533855830022 0.011447910944685629
[[0.01209754 0.01587802]
 [0.00579674 0.0126016 ]]
Epoch 1057: at batch 1: Training dataset Loss=0.049294, Batch Time=0.121
Epoch 1062: at batch 1: Training dataset Loss=0.046871, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.02287603885360845 0.07342168807000735
0.05820663287367367 0.08749584582891534
0.05396776608623366 0.04722691481675392
0.048246943086102334 0.09811366899363744
0.03326989794719548 0.03050548964013666
0.2568385911211113 0.17869483065606748
0.04198299625421953 0.043849197675511736
0.08792796293575833 0.08120682552222809
0.21359648392617459 0.2953563281048009
0.03902658114373114 0.021486624561318782
0.05649792921168606 0.07340795280114557
0.028385952886651467 0.012780451922300323
0.14801800685032163 0.06672385114189251
0.041996771524761556 0.09301207676841985
0.014114007802819017 0.01221923895968447
0.023586786359899747 0.017054745386537413
[[0.02293491 0.04561779]
 [0.02772352 0.03654464]]
Epoch 1067: at batch 1: Training dataset Loss=0.055464, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.13578298 0.01348646]
 [0.11036544 0.03621851]
 [0.12428812 0.05840532]
 [0.1599951  0.00549043]
 [0.12440975 0.04772653]
 [0.11506892 0.00160261]
 [0.04828999 0.0055355 ]
 [0.10195396 0.00225463]
 [0.01329187 0.0022493 ]
 [0.11506892 0.00160261]]
Epoch 1072: at batch 1: Training dataset Loss=0.061434, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.23187235381112004 0.11992248800175585
0.16966069411842 0.06330980895194079
0.04283964974664656 0.024113163566658744
0.0024310959154538736 0.0020733134466212103
0.009868224511778934 0.007929549252695768
0.024899432609299943 0.02038150336907539
0.07437560466439663 0.030932152902006484
0.011818972874401368 0.004992234806733106
0.08174872543006018 0.05601311943818636
0.16431876230785747 0.06061950348495452
0.06277338912864394 0.06226298756194054
0.017673548437250552 0.008832519267138886
0.03090121683373015 0.029338689957165976
0.021788177872864356 0.010807882623646526
0.03138690319875703 0.01994284839640766
0.006500934435361039 0.004324751439788434
[[0.3660135  0.35404196]
 [0.35631025 0.36160293]]
Epoch 1077: at batch 1: Training dataset Loss=0.049634, Batch Time=0.119
Epoch 1082: at batch 1: Training dataset Loss=0.052205, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.041555930935658125 0.02508500741833878
0.03996693155222886 0.018038921930641074
0.0924990965968675 0.06748877971330021
0.1328454480725938 0.0855603333965442
0.02566900392579452 0.03747823849984549
0.07459257448940804 0.03502134983507081
0.02387051080686753 0.02474832389342205
0.1448427368131604 0.2106897073119326
0.04886070099666995 0.028918680333428134
0.09915060956731736 0.06625947801403052
0.14159404030039013 0.05976288655269066
0.0917445812825779 0.16700608773746395
0.05203354942601024 0.0691087859106086
0.03595920977548861 0.04683190645372455
0.035406774494688875 0.04293031028838793
0.10662648372158934 0.17784259321751217
[[0.05847143 0.05998362]
 [0.05418688 0.05469095]]
heteroschedastic_sigma_s [0.10681123]
sigma_c 0.0231645359596
[[-0.04634235 -0.00933081]
 [-0.0086293  -0.02753084]]
[[ 0.00199245 -0.02021632]
 [-0.01251042  0.01167867]] 

Epoch 1087: at batch 1: Training dataset Loss=0.052799, Batch Time=0.122
		Epoch 1087:  Time = 1027.901, Avg epoch time=1.212, Current epoch Time=1.282

Loss vector (slice for the first 10 images)
[[0.10577367 0.02860199]
 [0.06222996 0.05267171]
 [0.12929006 0.00060565]
 [0.08963203 0.00955295]
 [0.08438176 0.0220366 ]
 [0.07681638 0.01632276]
 [0.11525472 0.04991965]
 [0.06222971 0.05267146]
 [0.12661473 0.02538356]
 [0.08706719 0.0107673 ]]
Epoch 1092: at batch 1: Training dataset Loss=0.044566, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.0057539421608732155 0.0038499170177771157
0.16564830268536213 0.18714142389091812
0.2979353835566485 0.16936867687407683
0.026046737565188494 0.019143321344955867
0.008301727292043815 0.008932658980495011
0.025206261952341436 0.010622574652698056
0.0194683612043125 0.007166873560489893
0.03538995147180213 0.019524774579577332
0.04621607340303768 0.021865713440109565
0.08415903915411604 0.05685902339047963
0.09683453477944681 0.039961443640481834
0.05031194352037005 0.025221973545282397
0.3288620281020016 0.3757840288066734
0.04088432213271176 0.021864157138422834
0.026513205969550313 0.009626670921613024
0.011249166554673451 0.007513831275554168
[[0.0086951  0.00875811]
 [0.00819104 0.00901014]]
Epoch 1097: at batch 1: Training dataset Loss=0.054910, Batch Time=0.121
Epoch 1102: at batch 1: Training dataset Loss=0.059273, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.014272781520070055 0.006345844854054139
0.2660379747034938 0.192989971838566
0.13641433564106364 0.07910916049492936
0.03059551131179461 0.029273555212166822
0.1064363097382639 0.05695757721069513
0.17229671504324529 0.07810788751158104
0.008655922331054278 0.007788535718325334
0.015180434785121655 0.01891291914720472
0.19730866035511951 0.15379783411438652
0.027415864627206687 0.020821141797829606
0.015180434785121655 0.01891291914720472
0.07086699848423095 0.08764619918794439
0.015087512974671924 0.0081325708139279
0.04232666367020421 0.061353840576079136
0.00466746080516689 0.004621894280222528
0.004087532599721833 0.007318528751143725
[[0.01991053 0.0189024 ]
 [0.02318694 0.02293491]]
Epoch 1107: at batch 1: Training dataset Loss=0.053018, Batch Time=0.119
Loss vector (slice for the first 10 images)
[[0.08162312 0.00137198]
 [0.10620797 0.01678222]
 [0.02859521 0.0158669 ]
 [0.14382993 0.02121899]
 [0.10620797 0.01678222]
 [0.06728809 0.03351881]
 [0.02623342 0.0324742 ]
 [0.07460364 0.0590419 ]
 [0.15797321 0.00331777]
 [0.10620797 0.01678222]]
Epoch 1112: at batch 1: Training dataset Loss=0.067654, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.0020485791973516854 0.0015124006755371075
0.08803784240872048 0.11246354839815137
0.07913461634930652 0.04850405736511282
0.21609258123068287 0.3042192367993348
0.040868296130897264 0.019717666297829794
0.0618413733376002 0.03281172506292139
0.0020485791973516854 0.0015124006755371075
0.030995841312869032 0.016341953707374762
0.01617861301905421 0.007272018794062657
0.025091967682065786 0.018374151936840284
0.006117164002098718 0.004730269152636527
0.11480145262852659 0.059972462616974816
0.06591039657536157 0.023570773918545146
0.0349334837898283 0.027188838751780332
0.1620500972917398 0.07201890256337211
0.09173563988473177 0.04568289774149054
[[0.0007561  0.00126016]
 [0.00100813 0.00176422]]
Epoch 1117: at batch 1: Training dataset Loss=0.055273, Batch Time=0.120
Epoch 1122: at batch 1: Training dataset Loss=0.042519, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.03715660025580547 0.01936920869131029
0.0011203456545318957 0.0007254873903360048
0.07269965878813167 0.06665113798781362
0.029215003847313348 0.020687968443568162
0.023753659112008663 0.025238141024980683
0.04077504934076259 0.03734522180326685
0.05816823254649783 0.02709903125941474
0.10523300263449897 0.07262647602104362
0.09482390110052741 0.10173626459813724
0.014485314303184893 0.00988397936139546
0.02540046999337875 0.015350775376430665
0.12553334520900705 0.2047024425304199
0.021795400097058604 0.020289166478928932
0.09535587341252594 0.05536611653962584
0.10847442860962175 0.07882292504459155
0.1915210249906636 0.12077203480892118
[[0.04082919 0.04032512]
 [0.03956902 0.03604058]]
Epoch 1127: at batch 1: Training dataset Loss=0.055001, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.06483453 0.03401148]
 [0.00349167 0.00786947]
 [0.09039484 0.05517225]
 [0.09039484 0.05517225]
 [0.12219102 0.02562221]
 [0.10756533 0.02653676]
 [0.05744377 0.05674919]
 [0.15273831 0.00393746]
 [0.09383716 0.01082465]
 [0.05744377 0.05674919]]
Epoch 1132: at batch 1: Training dataset Loss=0.059197, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.10724984131633164 0.04788516339434843
0.033875884638684184 0.03760232434263127
0.20172927905741744 0.07697411746342737
0.035814974753055395 0.018221261448534483
0.026149113040741412 0.022009930097890252
0.04808564277301741 0.03997284090521428
0.016916359274391723 0.011900696622376705
0.02933268523426591 0.02462118969799181
0.10102397083448844 0.15882639011689678
0.03783634752278253 0.019930394520155895
0.16510032270309694 0.16967274495461432
0.02189481536866822 0.014553744012615815
0.8152745266015131 0.2817600524457879
0.0616947635900722 0.18678936874937518
0.40868007879896595 0.23926150065570462
0.011802420963448412 0.022182561428590725
[[0.14794278 0.13609728]
 [0.14769076 0.13660134]]
Epoch 1137: at batch 1: Training dataset Loss=0.057977, Batch Time=0.121
Epoch 1142: at batch 1: Training dataset Loss=0.062106, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.0165237023987439 0.011359437686605649
0.05934158548468815 0.026531409735930392
0.026091581346635806 0.020459399506115382
0.046535209050036164 0.042573930157611295
0.05831647864646072 0.02970931676546024
0.021627442844659228 0.0227388320912578
0.0237624619465322 0.02526897955577873
0.004276048978284486 0.0016985323572949263
0.012374557769442518 0.012240119242385526
0.00633831213740077 0.0025963350716548747
0.014983640530518372 0.009230594959702453
0.08420667973732154 0.10711593305198132
0.04833772090443489 0.0930388559434885
0.0056037549288312505 0.0019402498551104664
0.026219547105108987 0.04254945588508807
0.4884983049914382 0.31949444883084704
[[0.02847962 0.02797555]
 [0.02923571 0.02923571]]
Epoch 1147: at batch 1: Training dataset Loss=0.054815, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.14408005 0.01357493]
 [0.06624339 0.03126468]
 [0.14971002 0.00087682]
 [0.07183402 0.00038442]
 [0.00027083 0.00880364]
 [0.07676647 0.02321929]
 [0.00027083 0.00880364]
 [0.07676647 0.02321929]
 [0.14971002 0.00087682]
 [0.14408005 0.01357493]]
Epoch 1152: at batch 1: Training dataset Loss=0.057851, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.02405709277125645 0.01145087860965946
0.04405058095560577 0.021074844718101435
0.0019992484402855393 0.002143931541951696
0.012456767371926247 0.008275865070979814
0.012510595679300351 0.0065677796410958085
0.016132394390126592 0.007655859918227175
0.08879665348367993 0.1953876657851959
0.06995375935718862 0.05670228222441327
0.2188809471314812 0.10076530683780108
0.025556797769141326 0.01037187305177476
0.016418360887545447 0.010380522114137954
0.03711708949587944 0.015831495753209007
0.1653722080225286 0.19826533941230823
0.044115234925072855 0.0245216305424358
0.05019082207737924 0.025613263076506337
0.009628637200824741 0.00646698872780248
[[0.0448617  0.02873165]
 [0.04360154 0.0433495 ]]
Epoch 1157: at batch 1: Training dataset Loss=0.058415, Batch Time=0.124
Epoch 1162: at batch 1: Training dataset Loss=0.056378, Batch Time=0.118


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.017094477829671995 0.02034810722029326
0.0013176408030206588 0.0004590889708197492
0.024869089950279033 0.018278621431214698
0.03378482606120059 0.08755083593163031
0.01648019114864141 0.013911731688524179
0.10018837249674561 0.04356944781490241
0.019441698000626673 0.00895820848631458
0.12768563149407441 0.07710394179219222
0.007380927484350153 0.005572486350316972
0.020943188409352054 0.01294706757869428
0.2724310410472981 0.12256108191397665
0.02568374449832067 0.019279466481239033
0.06882626642675937 0.0643569148050359
0.08572997626006895 0.03950051211744312
0.0013382066628380151 0.001036403406632426
0.032879031247412094 0.05571543670508795
[[0.00655283 0.00529267]
 [0.00352845 0.00579674]]
Epoch 1167: at batch 1: Training dataset Loss=0.051756, Batch Time=0.121
Loss vector (slice for the first 10 images)
[[0.07532177 0.0387101 ]
 [0.12686926 0.0459072 ]
 [0.07532177 0.0387101 ]
 [0.01769418 0.04358734]
 [0.13650899 0.00182845]
 [0.14384549 0.00793254]
 [0.01474458 0.03707963]
 [0.12210981 0.03227655]
 [0.078255   0.04275428]
 [0.00101921 0.0116619 ]]
Epoch 1172: at batch 1: Training dataset Loss=0.058145, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.1302576109494602 0.09773703396111837
0.0158281454343161 0.01758317601552931
0.031682634899397044 0.022129940715700253
0.049007188617309794 0.046132818609588996
0.11409534308100433 0.080513774052335
0.023866571843973894 0.019814635930884644
0.1315286965978757 0.11111876742131294
0.1250750046700091 0.05038393608615719
0.05568329973659303 0.027473647061908368
0.04822158453950465 0.021376574967765252
0.04349884560022588 0.027205586360653036
0.05568329973659303 0.027473647061908368
0.10569679253808317 0.058265768170335734
0.04938062950793132 0.049072048062689125
0.009653349684500157 0.005616540390142913
0.03005008578734536 0.030374447329238993
[[0.20187764 0.21246298]
 [0.20212968 0.20187764]]
Epoch 1177: at batch 1: Training dataset Loss=0.054394, Batch Time=0.118
Epoch 1182: at batch 1: Training dataset Loss=0.056593, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.09564067466953219 0.04819942696965349
0.015273682518443188 0.010609210351123454
0.026937966778509903 0.022088694868402543
0.02847174086949167 0.01071021097129979
0.014098498025298056 0.004645788277817405
0.012275869323136845 0.0051911998718855035
0.0684980058741047 0.032021734531562276
0.13302363297958664 0.05417371494792679
0.18861533372464834 0.2607384761201089
0.01024107602698665 0.009914538174040985
0.12176844404734943 0.15066622597006266
0.05918971097359638 0.06894587020984387
0.09677567628356343 0.04915280652093594
0.012625997538565414 0.009206379894813398
0.0050731555786072136 0.002091651396996224
0.062043561132213654 0.021259624644719588
[[0.11643879 0.12878835]
 [0.11820301 0.14290215]]
Epoch 1187: at batch 1: Training dataset Loss=0.065406, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.00465729 0.02429843]
 [0.1515757  0.0233697 ]
 [0.10786977 0.03471508]
 [0.04647383 0.02189062]
 [0.1426842  0.0098044 ]
 [0.0740498  0.03441869]
 [0.15648589 0.01562586]
 [0.09576719 0.05294626]
 [0.06513598 0.02903867]
 [0.12210522 0.00220611]]
Epoch 1192: at batch 1: Training dataset Loss=0.065275, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.1803648553150765 0.07458887443008605
0.01010237400222369 0.008966852987906594
0.136743797067993 0.057223983097571485
0.07998605119828994 0.03518835505641256
0.002549216693852774 0.004498513012503565
0.12592696830612837 0.07128339660925896
0.3511065880850879 0.18055633425324583
0.002549216693852774 0.004498513012503565
0.26072488185525344 0.11831071159755199
0.11640702609121689 0.0507711996021942
0.04294980510846358 0.016037296765696898
0.04611720899072225 0.03224798832541603
0.15540556049364795 0.07382876414934861
0.0027803819135776564 0.0020551860736722797
0.014156539369263754 0.011269417884751335
0.06488834710007296 0.04644258650792471
[[0.26513767 0.26286939]
 [0.25379622 0.25984499]]
Epoch 1197: at batch 1: Training dataset Loss=0.051247, Batch Time=0.125
Epoch 1202: at batch 1: Training dataset Loss=0.048973, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.05977522407565061 0.026464131378879045
0.090901048264449 0.09088815218746352
0.014271025953571437 0.011393903930520281
0.0028520081395599917 0.005202328319028142
0.0346450637499407 0.03928297777668435
0.06679703209215404 0.04221345242611105
0.3122967085032542 0.16446146647382956
0.09208415296518524 0.03371945962942767
0.008404680610071047 0.005859484769515523
0.07948906711368409 0.048508744453297545
0.024334429532220092 0.028469446589907303
0.03788826450985994 0.036931417393254104
0.011475832302310529 0.0076858212890569535
0.12615453301987145 0.05122938976763244
0.11160555114732063 0.09596797731506894
0.0346450637499407 0.03928297777668435
[[0.09117258 0.09148762]
 [0.08632097 0.08909331]]
Epoch 1207: at batch 1: Training dataset Loss=0.063715, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.02970908 0.02497858]
 [0.03161719 0.02826402]
 [0.08022674 0.00441289]
 [0.06486727 0.0196368 ]
 [0.06521698 0.04024595]
 [0.10678721 0.04149814]
 [0.09187382 0.02258177]
 [0.12406135 0.02837343]
 [0.05144659 0.0005819 ]
 [0.05144659 0.0005819 ]]
Epoch 1212: at batch 1: Training dataset Loss=0.060961, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.047115220909452304 0.06680003857910267
0.07496042752107712 0.05457405987350354
0.004013660486760884 0.0033214409762705325
0.04557082592510309 0.02753961959469668
0.047115220909452304 0.06680003857910267
0.010371496313697826 0.009316112895692157
0.04897217449504154 0.025890861130994477
0.06394121962450328 0.059398966469726526
0.003186834286769802 0.0020349237960756938
0.017054347914545964 0.01243790997242122
0.09948460600180908 0.040204015226810504
0.08361252242272599 0.06380228219167548
0.006403449702996333 0.0033813557112858756
0.023836866688695224 0.025210109068885413
0.005459714130601867 0.001907903533732796
0.09598948004889962 0.03725120860908027
[[0.01272762 0.01512192]
 [0.01486989 0.01392477]]
Epoch 1217: at batch 1: Training dataset Loss=0.047394, Batch Time=0.118
Epoch 1222: at batch 1: Training dataset Loss=0.057925, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.1707096597700115 0.0764753886741537
0.09543209527457108 0.10261964990984922
0.0244126847743118 0.016057543098894934
0.013489415603979538 0.011217560225611538
0.15612883729562643 0.14569329902670297
0.07079068243021425 0.03399468536270851
0.07627222865229388 0.04531933668639847
0.018898516439037394 0.011364577566466782
0.07179100756039958 0.09170107072199113
0.29102463899384645 0.31516791807664696
0.07895815046646248 0.026899823286936977
0.011845713970690674 0.007059759597430723
0.0052659531835062 0.0034131086355915336
0.002220605193433478 0.0017265091372944824
0.0034005015598115307 0.0026035948186976954
0.01047740313292067 0.0073987464655153965
[[0.22380443 0.22481255]
 [0.22103207 0.22304833]]
Epoch 1227: at batch 1: Training dataset Loss=0.058141, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.03083801 0.02097984]
 [0.13703313 0.0336868 ]
 [0.13320708 0.02928915]
 [0.03403119 0.0555958 ]
 [0.10564654 0.0150102 ]
 [0.13703313 0.0336868 ]
 [0.12950484 0.04877283]
 [0.13320708 0.02928915]
 [0.11039619 0.04910867]
 [0.05600389 0.01371546]]
Epoch 1232: at batch 1: Training dataset Loss=0.049087, Batch Time=0.118


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.20509319440670204 0.35620982153600655
0.005035106187774474 0.003119845866803778
0.044457809182656405 0.03206145389409096
0.008729654071171211 0.005180588516602609
0.05664009428110006 0.037358408334645654
0.011177621091951195 0.007472667163561095
0.04493379669122355 0.030134428792797863
0.0292925408880631 0.025851676225799675
0.03343108291691088 0.03202537912896252
0.005035106187774474 0.003119845866803778
0.03964863421689713 0.03497913089587662
0.04007700805822889 0.059027707066802534
0.012018480251827057 0.009668680251584088
0.05255943012694786 0.04497641533009438
0.32068469112865294 0.14620244321490872
0.04930866480637519 0.020658707360108886
[[0.02243085 0.02324995]
 [0.0212337  0.02148573]]
Epoch 1237: at batch 1: Training dataset Loss=0.060997, Batch Time=0.121
Epoch 1242: at batch 1: Training dataset Loss=0.048339, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.019458981543278853 0.02594201022125307
0.11031767013234806 0.24525147801490216
0.09080489893558763 0.04621342563834719
0.2301708599624135 0.10774635181473004
0.038007881262072996 0.1269884573309628
0.011149850318126653 0.005780243892862988
0.019358361604663976 0.023420623810056
0.04217012626139649 0.02309628452777741
0.007135297616685188 0.023501983162847503
0.11518906396199213 0.05483644591627263
0.015842497596535665 0.008528395201843507
0.04727279955151786 0.0699331850085668
0.019358361604663976 0.023420623810056
0.043188524285741714 0.02353747959579419
0.043188524285741714 0.02353747959579419
0.0015827885764450045 0.0005637137689317142
[[0.01234957 0.01008128]
 [0.01033331 0.01108941]]
Epoch 1247: at batch 1: Training dataset Loss=0.053372, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.11806399 0.03142436]
 [0.08771176 0.05604341]
 [0.1323327  0.04412059]
 [0.0313094  0.03720094]
 [0.15968657 0.01949449]
 [0.09520603 0.02226714]
 [0.0516005  0.04114966]
 [0.11081026 0.05669649]
 [0.11081026 0.05669649]
 [0.09520603 0.02226714]]
Epoch 1252: at batch 1: Training dataset Loss=0.060286, Batch Time=0.120


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.12669602824131232 0.06940347793044956
0.0892095820255685 0.05558617654500966
0.05351308374892483 0.022964467264817597
0.023788398318210557 0.04171695834086321
0.025059319598426555 0.012303859181389806
0.010812898445802022 0.01931093978785079
0.2678513195984351 0.31655207933540597
0.059002067555084636 0.050553968769967074
0.09313019187672467 0.06904757931342935
0.10205104279512511 0.14715763355609338
0.09495493289021084 0.04626890529016755
0.04025735616847648 0.034559932022235965
0.20242993538307985 0.2160307172604265
0.029147788601399327 0.012781822663825333
0.047287067008667805 0.07655731295869933
0.010704885145994325 0.011571285589244763
[[0.08644698 0.05947955]
 [0.11996724 0.04813811]]
Epoch 1257: at batch 1: Training dataset Loss=0.058146, Batch Time=0.121
Epoch 1262: at batch 1: Training dataset Loss=0.055633, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.02911416947552148 0.03971830019225998
0.08957093196863042 0.09140784103928683
0.08314475031375235 0.0825414481890212
0.014140662382533975 0.0070306602956143995
0.015028489116486021 0.014801783785693802
0.003825194105823826 0.0034054074085513703
0.104592206600012 0.10975704338532855
0.07759918084611606 0.07640880258700976
0.044276585229910737 0.03645024166339912
0.09450599224014411 0.09856900678909229
0.06261707478562606 0.1448255802599032
0.060506368187489556 0.02720370403719711
0.04816224135542768 0.021216760916834564
0.12216246326920022 0.10344532575172369
0.04150194111092631 0.024034235802775382
0.001436224025318289 0.0007488776005207506
[[0.24673934 0.26740596]
 [0.25026777 0.26589376]]
Epoch 1267: at batch 1: Training dataset Loss=0.056903, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.03440045 0.04677769]
 [0.15087671 0.03141281]
 [0.01554216 0.03055451]
 [0.0684354  0.04754179]
 [0.15322071 0.01558235]
 [0.01554216 0.03055451]
 [0.07872666 0.03756911]
 [0.06983575 0.05673657]
 [0.11154733 0.00405346]
 [0.03147777 0.02397437]]
Epoch 1272: at batch 1: Training dataset Loss=0.057279, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.11829640663747565 0.07620376965924949
0.033495483059425624 0.019471393978156684
0.012242780899900163 0.009276228786963903
0.02936243654994275 0.03399163408990328
0.028866586957068563 0.01826936929929502
0.1688016139919455 0.12945082869937416
0.12539135802500567 0.09326737723354257
0.2729509493615012 0.1755675739522577
0.19162129412192996 0.14437498439713023
0.012760204890667204 0.01061129987040122
0.19015427750278313 0.16554971363165377
0.04131134419310545 0.05836856496256269
0.016672135000495913 0.01125533093751278
0.0304279492015187 0.03869231444319132
0.11491312033234635 0.052589015757999125
0.1461206253374172 0.06773768704178311
[[0.18618865 0.18184109]
 [0.18385735 0.1747212 ]]
Epoch 1277: at batch 1: Training dataset Loss=0.066714, Batch Time=0.121
Epoch 1282: at batch 1: Training dataset Loss=0.064523, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.09339061040074625 0.06036321163824863
0.19288666023443213 0.1404357541199062
0.10007950451630876 0.05624159032187647
0.22912735718500699 0.12108256256971507
0.032942101752574615 0.04416623125408045
0.15300506870760344 0.12028127282405332
0.025384139226606095 0.014378304161625402
0.03562141475201752 0.08455336908309584
0.09296596879458718 0.054921259461437034
0.2437325199466045 0.11509622729319836
0.09878490625558811 0.051195090451750754
0.02003268739771258 0.010029545959422247
0.2437325199466045 0.11509622729319836
0.044272912573456225 0.05448754226649886
0.027142574544073206 0.02050131573436668
0.006418317199438306 0.003759404279796329
[[0.08600593 0.08487178]
 [0.08707706 0.08436771]]
heteroschedastic_sigma_s [0.08735689]
sigma_c 0.03019621526
[[-0.00988596  0.01050592]
 [ 0.01082881 -0.00077931]]
[[-0.01234759  0.00788517]
 [-0.03110575 -0.0388149 ]] 

Epoch 1287: at batch 1: Training dataset Loss=0.049721, Batch Time=0.125
		Epoch 1287:  Time = 1277.668, Avg epoch time=1.226, Current epoch Time=1.275

Loss vector (slice for the first 10 images)
[[0.11426357 0.00243671]
 [0.04052575 0.02543393]
 [0.04052575 0.02543393]
 [0.03165953 0.00535541]
 [0.04584612 0.00810048]
 [0.03165953 0.00535541]
 [0.09517241 0.04705   ]
 [0.01843669 0.05132933]
 [0.05307261 0.02137899]
 [0.05252112 0.03438866]]
Epoch 1292: at batch 1: Training dataset Loss=0.060037, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.1968557653459584 0.0945521681952019
0.010483902374396337 0.007585608646016434
0.057732231719697324 0.027693993006046705
0.1376648237996676 0.06139912622675489
0.043627434428456624 0.022604018007367284
0.04513724688260368 0.02905452420106605
0.057732231719697324 0.027693993006046705
0.18983370739237415 0.1052533722987022
0.18983370739237415 0.1052533722987022
0.03052653669633143 0.028691395379456225
0.05253453784484918 0.021398026678313548
0.043694115955609014 0.033853773122530816
0.17646861180898554 0.07387761840469864
0.014980639929307538 0.011684014208347908
0.03483465691801424 0.03089228381357585
0.17646861180898554 0.07387761840469864
[[0.28832462 0.30974734]
 [0.22556865 0.2424548 ]]
Epoch 1297: at batch 1: Training dataset Loss=0.052404, Batch Time=0.124
Epoch 1302: at batch 1: Training dataset Loss=0.052746, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.09450211188408275 0.060734233118514176
0.03238286340570795 0.015943159388370085
0.07470594679617193 0.03378799711140222
0.10446469363188626 0.21388145652563215
0.020195537560331545 0.02404346675444534
0.006185106052610578 0.003057075683394255
0.05624965544224381 0.04554402084758273
0.19535016488077872 0.07629293748806647
0.2142860942447271 0.25630528132486696
0.01867046143497575 0.020270901172175868
0.043917504296175025 0.016254410900619406
0.14506394534745493 0.09502779031257391
0.01973031513074197 0.012915126701636158
0.010481137310212496 0.006080188928633105
0.012752832680803294 0.008274943967842672
0.11215885847377027 0.09546181246368794
[[0.09930061 0.10257703]
 [0.09930061 0.10056077]]
Epoch 1307: at batch 1: Training dataset Loss=0.051000, Batch Time=0.119
Loss vector (slice for the first 10 images)
[[0.14900309 0.01790318]
 [0.12939715 0.03833081]
 [0.08466768 0.04925438]
 [0.10517202 0.03493457]
 [0.02399429 0.01976204]
 [0.10517202 0.03493457]
 [0.14900309 0.01790318]
 [0.02574489 0.02338838]
 [0.07350583 0.0178304 ]
 [0.06370655 0.05704153]]
Epoch 1312: at batch 1: Training dataset Loss=0.054108, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.11513532506208435 0.06252773278677214
0.004271918685792464 0.0032627338865413116
0.031461118536029176 0.015922069737718415
0.12648044385069568 0.1204496709120566
0.0122414233776027 0.004234153058637434
0.004225785637839152 0.0016430538299209755
0.060577479103892884 0.09747205858578448
0.029235993685709616 0.03476114294278909
0.08095080355008122 0.027995027017099144
0.026704779681352875 0.011085438430158086
0.13704949394442334 0.3157458903563316
0.03564923070376835 0.03204905347399009
0.056322736299577514 0.03658299297918606
0.03667103021721552 0.02375746822772709
0.056322736299577514 0.03658299297918606
0.28814212330237865 0.14337640418714528
[[0.1323168  0.133829  ]
 [0.1330729  0.13433306]]
Epoch 1317: at batch 1: Training dataset Loss=0.054748, Batch Time=0.121
Epoch 1322: at batch 1: Training dataset Loss=0.052006, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.01596682148248263 0.013350946653524154
0.04177869522841071 0.021245728644199553
0.04598780394248081 0.039272965543370654
0.07355340873374416 0.04136428883695695
0.027597889439948453 0.017053464968281657
0.21580763895184418 0.09957571947561765
0.036476952942599006 0.04006117472650113
0.15082350792573607 0.12585996796953414
0.15082350792573607 0.12585996796953414
0.022963870856592017 0.011024795511383235
0.05804624497594091 0.039664727824277386
0.011012798095777043 0.012525154586719725
0.04281949345747549 0.023470238266386192
0.030818500551017536 0.014027953013736216
0.05017075038304242 0.024545636853492942
0.13745343318148429 0.08820633975222873
[[0.01764224 0.01915443]
 [0.01083738 0.01134144]]
Epoch 1327: at batch 1: Training dataset Loss=0.059233, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.11260268 0.00574808]
 [0.11926196 0.037988  ]
 [0.04309613 0.03568449]
 [0.0321754  0.00809867]
 [0.11569669 0.05712935]
 [0.11926196 0.037988  ]
 [0.08749001 0.02850322]
 [0.11301426 0.01384471]
 [0.14688073 0.04991106]
 [0.08749001 0.02850322]]
Epoch 1332: at batch 1: Training dataset Loss=0.054138, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.005892732629795283 0.0035482312962999106
0.014813263389001463 0.0067998503120294315
0.017922450042167526 0.011066331537934177
0.023088276505248473 0.010831592434070671
0.029284633174390784 0.02190246821144422
0.02477400492339754 0.012314270606321181
0.01237341559086147 0.005145975932477825
0.04994248098813969 0.021141691902890905
0.0035180493251498746 0.0027051937040798576
0.27706343281005275 0.15324611821963272
0.025657474502741806 0.01735159390164412
0.0417865000626465 0.062357969861614675
0.17560567062787413 0.06984396271688335
0.12407649188111947 0.11507422797257816
0.009103067056802416 0.004312469652890457
0.1196462888329366 0.09328096570133128
[[0.00604877 0.0055447 ]
 [0.00579674 0.00478861]]
Epoch 1337: at batch 1: Training dataset Loss=0.050838, Batch Time=0.120
Epoch 1342: at batch 1: Training dataset Loss=0.054817, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.1560379065915889 0.1445123495459545
0.04661240386044341 0.023120350912052653
0.009871254922825479 0.0063121743319943515
0.01731070632977527 0.0113553061513084
0.0010130014965281653 0.000545065873046143
0.0618469688690908 0.03314643171567417
0.2242284119900546 0.11332458374277526
0.015312740437076755 0.020274023398282465
0.018992086242282546 0.023840487410254668
0.029100736379852776 0.014785774126365007
0.1492575001637899 0.10224280491605822
0.0022151116095203705 0.0014928594708320505
0.026527108195367077 0.017130343152928655
0.025702455786539957 0.022701411109696923
0.01743078265928233 0.007003187681335407
0.06008749033888705 0.04057379393657073
[[0.00970323 0.01008128]
 [0.0102073  0.01045933]]
Epoch 1347: at batch 1: Training dataset Loss=0.060707, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.07752204 0.04496521]
 [0.08559823 0.03257226]
 [0.13914035 0.01300025]
 [0.07752204 0.04496521]
 [0.07115428 0.03291164]
 [0.09810138 0.03875976]
 [0.00684918 0.03279861]
 [0.00684918 0.03279861]
 [0.11881075 0.01282601]
 [0.02854001 0.0274138 ]]
Epoch 1352: at batch 1: Training dataset Loss=0.049811, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.07430270169840014 0.034816452410339344
0.11722600322099375 0.06288498337002336
0.006816343638236688 0.007113030452327302
0.1455644597526824 0.1321703953047583
0.049911732643835194 0.038708616080330153
0.03332979574638095 0.03388990809627013
0.14715400812846724 0.10027751370173889
0.18594566294837023 0.15546537223604234
0.02470758964182984 0.04806008020821
0.1821659442554946 0.12976725489498106
0.027183007302021167 0.020053473969060282
0.0744919352928406 0.037112165243581054
0.04915471750371836 0.053070779431808254
0.02498719631081947 0.028478629870412418
0.1255124438404902 0.17538823603622103
0.08737956183722417 0.17566234290961272
[[0.10282906 0.1008128 ]
 [0.08921933 0.10005671]]
Epoch 1357: at batch 1: Training dataset Loss=0.055395, Batch Time=0.120
Epoch 1362: at batch 1: Training dataset Loss=0.050375, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.038632367709126925 0.023385844854937207
0.1903390250873258 0.11393932635770923
0.0451445046965091 0.020852753602208524
0.05081716598936481 0.022637198401504056
0.01928031016923626 0.02579971765580707
0.018720348863698888 0.02316740523379168
0.01928031016923626 0.02579971765580707
0.08914398583639738 0.1508914233675843
0.0816160409575275 0.03870095787241444
0.09467156125820964 0.05890234839568131
0.03699702085465795 0.09010612162789149
0.01783083769106497 0.021933581132971773
0.022830482667131946 0.08890646501981742
0.013751357987421109 0.03615070853051548
0.015954497936164103 0.010472551485869267
0.07730665168242967 0.13853607175648305
[[0.04001008 0.05160355]
 [0.05160355 0.05456493]]
Epoch 1367: at batch 1: Training dataset Loss=0.059894, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.02746905 0.01082931]
 [0.10423412 0.00924388]
 [0.07686806 0.04578619]
 [0.09906815 0.05436994]
 [0.12341786 0.03360614]
 [0.11001664 0.00076922]
 [0.09586226 0.03449835]
 [0.08599397 0.02600645]
 [0.00748529 0.00550889]
 [0.10423412 0.00924388]]
Epoch 1372: at batch 1: Training dataset Loss=0.055614, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.2323096064704515 0.22916347846675456
0.25175697425300747 0.27031988728301704
0.02667084135149622 0.030435916429618116
0.18594979709704518 0.09939114999082987
0.03441602520414033 0.022846894423957858
0.12179894047275752 0.06948156864731579
0.011207202247453196 0.004965047358504387
0.04606393734640779 0.052618066767842975
0.16178421791956055 0.06765173311594258
0.03267202858433116 0.01586550429925812
0.03835472524722405 0.01991602143633037
0.05981244663391294 0.021855302157249245
0.1999345584636103 0.08735496707100474
0.0622840560972211 0.06365108623540854
0.13499150240244262 0.056656152693916585
0.02695204205052182 0.03292640433292191
[[0.21926785 0.23357067]
 [0.1771155  0.24667633]]
Epoch 1377: at batch 1: Training dataset Loss=0.061555, Batch Time=0.125
Epoch 1382: at batch 1: Training dataset Loss=0.055696, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.05737762712630401 0.06494182258403058
0.068828508490828 0.03878692056299558
0.09417619239877695 0.04772558319884706
0.01872632507794947 0.010655328504164039
0.032931083807188344 0.05056826183127589
0.04987361404904789 0.05450215430289772
0.01779566779012387 0.010915562278511703
0.02772512262542115 0.028332292376982263
0.1365591994901365 0.05783379782364355
0.014663731854482442 0.013966686007737586
0.033750495502729905 0.014993054167557349
0.08708190823753625 0.06681912420477727
0.1167027029785288 0.051194333450134924
0.14485239316192633 0.07320691117243176
0.00705563098989348 0.00531141560662936
0.13622163520978603 0.10092439167070791
[[0.08304454 0.08153235]
 [0.08392666 0.08077626]]
Epoch 1387: at batch 1: Training dataset Loss=0.055970, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.07922524 0.02749748]
 [0.03156994 0.03674477]
 [0.0196204  0.04638747]
 [0.07922524 0.02749748]
 [0.04732033 0.05858356]
 [0.13234812 0.02843951]
 [0.07922524 0.02749748]
 [0.03201822 0.02300175]
 [0.07922524 0.02749748]
 [0.01221319 0.05803979]]
Epoch 1392: at batch 1: Training dataset Loss=0.055729, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.007559871901490212 0.00418169697777764
0.046915241482977876 0.021190947321399863
0.13152158586888163 0.047657031661206
0.016837896345456294 0.009891740653005708
0.3171948537377034 0.153113536205475
0.01565232757944557 0.006443116414820073
0.008228024383217125 0.00486153380524743
0.22139257402970713 0.14126022948214403
0.004901244949738981 0.004839459425733122
0.01585520573026855 0.02057786247592187
0.022106525178774206 0.015553145029162605
0.03655459384481308 0.022117035254576114
0.044967254301006676 0.03727897419975604
0.19513722059616612 0.07731191248017726
0.13152158586888163 0.047657031661206
0.025525497653580587 0.016518946859265488
[[0.01360973 0.0126016 ]
 [0.00932518 0.00781299]]
Epoch 1397: at batch 1: Training dataset Loss=0.054131, Batch Time=0.125
Epoch 1402: at batch 1: Training dataset Loss=0.057936, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.033048693062326606 0.013037474835935143
0.16809062420747978 0.07973632348146112
0.035693221706310396 0.01840324492611169
0.004937994488716235 0.003965972629766596
0.6472527890918229 0.3686227442229776
0.04141868161838769 0.09921152299279144
0.02211541644073023 0.014244284388630861
0.1193885286666756 0.05911692692754374
0.09278183268052942 0.08594141117656752
0.012000217021146398 0.0069448304652695424
0.18794043304507113 0.07974556494399582
0.022126369010263325 0.01626228097239668
0.03260512584367525 0.02104348737775421
0.028616831587656932 0.014198115756901258
0.030496809406919567 0.021134916742463233
0.16423298395690722 0.16030363085391744
[[0.02948775 0.02898368]
 [0.03352026 0.03604058]]
Epoch 1407: at batch 1: Training dataset Loss=0.061858, Batch Time=0.121
Loss vector (slice for the first 10 images)
[[0.01313386 0.02209049]
 [0.02083368 0.04301884]
 [0.06719566 0.00270735]
 [0.11611494 0.00919063]
 [0.01313386 0.02209049]
 [0.05853514 0.03629009]
 [0.10858226 0.03990847]
 [0.09354842 0.00350689]
 [0.08263371 0.05056672]
 [0.15827264 0.03202424]]
Epoch 1412: at batch 1: Training dataset Loss=0.044034, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.04915655577985589 0.0240199578226078
0.05369392027980169 0.034471681793099146
0.06646152429495089 0.030018794850799627
0.01632102229111876 0.01230351044311149
0.009165787583375717 0.0047072390843554135
0.04993316477444054 0.038634406036767856
0.1563151683076569 0.07940810316980104
0.20289033523964672 0.14109095419401455
0.13597665620443422 0.12224386316350275
0.20124255141143976 0.0900044649644291
0.03706218057288879 0.015703244544044634
0.08459039148772263 0.03710130320736562
0.07138223523686804 0.03514596632762232
0.049139903853486544 0.02427964673770823
0.014955963003272288 0.009132294893336012
0.013964592629351458 0.00991419143124892
[[0.06464621 0.06792263]
 [0.06395312 0.06395312]]
Epoch 1417: at batch 1: Training dataset Loss=0.059962, Batch Time=0.124
Epoch 1422: at batch 1: Training dataset Loss=0.055986, Batch Time=0.118


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.05556106207346012 0.02832791533890833
0.0009409810900402604 0.0014046367384544845
0.026327794046480335 0.016188671162116686
0.044414950753463245 0.01864296466093555
0.0699583146077174 0.03694615621187458
0.03484246947499159 0.051359985022782405
0.012554079037023769 0.005687365010745855
0.012822086102282526 0.007519903981014578
0.039290838643889714 0.04654001349651435
0.01319595768670645 0.008663570607685625
0.010753505399370589 0.00859794949562926
0.008908010091232654 0.004627539594251773
0.03819870698524852 0.019505428210104685
0.014054787834705706 0.028185647522750157
0.15420622028890918 0.07432846779216673
0.07346173870847839 0.09507823736595317
[[0.07485351 0.08039821]
 [0.09192868 0.09715834]]
Epoch 1427: at batch 1: Training dataset Loss=0.044596, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.08114302 0.04590678]
 [0.06399673 0.00144492]
 [0.1231436  0.01736469]
 [0.09990073 0.00815226]
 [0.14541891 0.04706327]
 [0.1231436  0.01736469]
 [0.08076605 0.01949203]
 [0.14541891 0.04706327]
 [0.06417164 0.0017385 ]
 [0.07812665 0.05005465]]
Epoch 1432: at batch 1: Training dataset Loss=0.056368, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.1851536634038098 0.12246989714308183
0.030097746555977523 0.016988270805078826
0.18754136057778226 0.11582262749933045
0.009351177403767785 0.012375026294958964
0.01049123613697045 0.008798847953530416
0.06866019746547636 0.03844510042000179
0.04561319400727992 0.035294964881310356
0.03924081278196301 0.03765403134956829
0.07946029738540972 0.11744968880212105
0.017779951344857103 0.010496169813300892
0.012697181501208021 0.012599745829273212
0.09688250798433273 0.11803839609481645
0.04606359604918708 0.02346445722156369
0.04979965349123816 0.03017553342080321
0.15145293134396987 0.14909152610353374
0.05680663247573037 0.08803007321160948
[[0.14819482 0.16230862]
 [0.16860941 0.23766619]]
Epoch 1437: at batch 1: Training dataset Loss=0.051581, Batch Time=0.124
Epoch 1442: at batch 1: Training dataset Loss=0.049584, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.13205009704310022 0.10888585150092062
0.054526876545409664 0.0333491058047542
0.05129182481704042 0.05255682224950475
0.05875173724709848 0.08643721118162412
0.04237913158768869 0.050920057042807006
0.002154364883044213 0.004654899011472857
0.002154364883044213 0.004654899011472857
0.03698722298676671 0.030901470816228797
0.16043858615001394 0.17389525206642323
0.27429339299158073 0.11537773467726548
0.1245946705521428 0.13822165591023058
0.00840776102509233 0.005805547463224425
0.0956358098952137 0.035299268460403675
0.019892838399401036 0.011799571223022965
0.06175277026684789 0.03642486218140696
0.014312650886324718 0.013540250903644144
[[0.07888602 0.08014618]
 [0.06174784 0.06174784]]
Epoch 1447: at batch 1: Training dataset Loss=0.050918, Batch Time=0.121
Loss vector (slice for the first 10 images)
[[0.14775141 0.01021362]
 [0.11429503 0.02735381]
 [0.07393627 0.02904676]
 [0.01006466 0.00050336]
 [0.01892302 0.03733131]
 [0.0645848  0.01827831]
 [0.06313397 0.04494784]
 [0.03309809 0.03231559]
 [0.08893029 0.03342305]
 [0.10464715 0.02928732]]
Epoch 1452: at batch 1: Training dataset Loss=0.052740, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.11561865306232733 0.09527628696032542
0.06258763975810844 0.06542634711519042
0.06325723916504344 0.04478344053681617
0.1502480186726487 0.13053624521358387
0.17036332721818326 0.08457462649932274
0.06950934506897966 0.059697931140823106
0.0241040680500193 0.029778485774494084
0.19807676853085354 0.1717240925443339
0.06950934506897966 0.059697931140823106
0.024669186450018366 0.039438542701849225
0.13102854942221143 0.07402046802203874
0.017117915464694988 0.009495353083975791
0.10887465095616022 0.05539237486375744
0.04414058577896718 0.023762050848240398
0.04199553327352756 0.03549937924809726
0.034914434119841076 0.040129744540728006
[[0.0637641  0.06073971]
 [0.07208116 0.07762586]]
Epoch 1457: at batch 1: Training dataset Loss=0.057340, Batch Time=0.122
Epoch 1462: at batch 1: Training dataset Loss=0.045705, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.009782034614804758 0.012568295734273297
0.20799251044472555 0.083568283578088
0.026606255651145716 0.01663211331250649
0.024788852252576987 0.018923373075349442
0.013044490809999587 0.006597434033643836
0.16364950613304075 0.07819271514142334
0.19032144632441828 0.10939738490641657
0.1129154803400354 0.0649246433408765
0.09951854718735831 0.05911563422849275
0.05517866169114516 0.05847705993186489
0.02772915098880091 0.016747634112287777
0.05325957116771729 0.03718649986821343
0.029942168613953157 0.019664262211465458
0.04966097361369748 0.032468725695190076
0.015641836484142857 0.00770824940474522
0.014719567624937646 0.010566043590243414
[[0.00579674 0.00378048]
 [0.00529267 0.00604877]]
Epoch 1467: at batch 1: Training dataset Loss=0.052687, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.07651307 0.03241653]
 [0.03282351 0.04841961]
 [0.00263833 0.0110017 ]
 [0.00263833 0.0110017 ]
 [0.05039217 0.0199246 ]
 [0.0444787  0.01679411]
 [0.0444787  0.01679411]
 [0.04526198 0.03865374]
 [0.09807475 0.05020622]
 [0.04526198 0.03865374]]
Epoch 1472: at batch 1: Training dataset Loss=0.048898, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.1156409024035625 0.08507292433442176
0.2779642571183274 0.1393561043907562
0.0295561638493389 0.020770108250689766
0.0201016591286991 0.01348735182855645
0.03477578304376827 0.03151295748931558
0.03409035951551331 0.040646605267781966
0.0010417471715644666 0.0007049899344107792
0.030911494472297463 0.01794042076597398
0.04217875890645928 0.05545617298323508
0.028301621417679 0.02863079592843914
0.01573674459962593 0.008645512685387938
0.01607896219416105 0.01244030568765942
0.13193159939226007 0.05606364087918631
0.0035358039810171604 0.004259255172472362
0.014029171599039714 0.007988279979601144
0.061579569349561325 0.023431388115494774
[[0.23199546 0.21586542]
 [0.24579422 0.26293239]]
Epoch 1477: at batch 1: Training dataset Loss=0.042371, Batch Time=0.119
Epoch 1482: at batch 1: Training dataset Loss=0.050796, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.01013887357022547 0.015038691099523294
0.3289879949992951 0.19904848654628424
0.13308238378087367 0.0596711790550224
0.035824817818735255 0.01871731446789519
0.33454038870358005 0.34225688660445297
0.020917076080065655 0.013566571123341076
0.04673518563614465 0.03163009254566139
0.05203048440003144 0.06514391442139861
0.05380512358785694 0.03553186752191589
0.0249379319006664 0.00892455068274492
0.08974105825012657 0.04031140431966771
0.08069461822144319 0.19588015419841248
0.33454038870358005 0.34225688660445297
0.15138640931161262 0.06364662807430344
0.16536017872985553 0.112563052968813
0.2783711949484182 0.20257920275699262
[[0.00680486 0.00655283]
 [0.00655283 0.0055447 ]]
heteroschedastic_sigma_s [0.09738135]
sigma_c 0.0112888717234
[[ 0.00671302 -0.00977293]
 [-0.00991777 -0.01003778]]
[[-0.00691018 -0.01128254]
 [-0.00353983  0.00053439]] 

Epoch 1487: at batch 1: Training dataset Loss=0.044610, Batch Time=0.121
		Epoch 1487:  Time = 1527.479, Avg epoch time=1.222, Current epoch Time=1.271

Loss vector (slice for the first 10 images)
[[0.03275857 0.00713209]
 [0.05876356 0.01327214]
 [0.10006335 0.03584639]
 [0.10639141 0.0263846 ]
 [0.1592105  0.0594488 ]
 [0.01965527 0.01814153]
 [0.09466445 0.02979372]
 [0.11736922 0.01953792]
 [0.10006335 0.03584639]
 [0.09614167 0.01209433]]
Epoch 1492: at batch 1: Training dataset Loss=0.057953, Batch Time=0.121


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.022743308261210515 0.026379216252506606
0.08400028470100551 0.058893333418789155
0.003545396123001865 0.004294583529222573
0.0038428439650957102 0.0025859330846637256
0.09592175324664254 0.050502008865392266
0.0426070385323527 0.024148106390368112
0.014180673075132155 0.008273531746148819
0.00866875735902184 0.003880051365315918
0.04652879826124501 0.07218334967317556
0.00866875735902184 0.003880051365315918
0.07557442574700701 0.03614264897049231
0.031090655214748963 0.02609273981726275
0.14175353607248198 0.1504253144401397
0.06156833226879321 0.046476854333651146
0.006184440743295383 0.002963742757101779
0.056800625498921065 0.030466869237406113
[[0.02054061 0.01688614]
 [0.01682314 0.0134207 ]]
Epoch 1497: at batch 1: Training dataset Loss=0.063429, Batch Time=0.122
Epoch 1502: at batch 1: Training dataset Loss=0.052832, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.11255575425343523 0.07235299658765533
0.0174634749838134 0.007987292473215937
0.15084557173549484 0.0945114069002623
0.07654715798280165 0.09771573572124516
0.026961614007607437 0.02183807446849085
0.06539816911111984 0.025917826636360438
0.05815219018751794 0.058632246095457745
0.008305267260467097 0.011548096968298893
0.02853561420148054 0.020785246494262607
0.0315696856327925 0.018319410553856868
0.026961614007607437 0.02183807446849085
0.15084557173549484 0.0945114069002623
0.0125082844101021 0.007096127408094614
0.041104953016205314 0.04352271364599727
0.0315696856327925 0.018319410553856868
0.07784814781543936 0.11518423325253863
[[0.1629387  0.1622456 ]
 [0.15695293 0.1622456 ]]
Epoch 1507: at batch 1: Training dataset Loss=0.055591, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.0245198  0.04643682]
 [0.06182573 0.01987096]
 [0.03882564 0.05220819]
 [0.15735655 0.04133535]
 [0.14774737 0.03380121]
 [0.13019088 0.02625533]
 [0.03882564 0.05220819]
 [0.15289923 0.04892178]
 [0.15406048 0.02685687]
 [0.14481021 0.03867493]]
Epoch 1512: at batch 1: Training dataset Loss=0.054824, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.09388577626165784 0.18783268168761277
0.03277871320126735 0.020068652899811273
0.17568530837570506 0.14506982946352537
0.02314211635108876 0.020159996770472425
0.09435886326144605 0.04861032186135164
0.1945527928706241 0.13259972476428428
0.03689263117209052 0.02236096307737671
0.07212873720152402 0.054417071110697206
0.03321467557631319 0.08379865833208797
0.0438328679957678 0.0759576214076542
0.03956312597267786 0.030203607937525426
0.013071160769276169 0.008133481423114384
0.03277871320126735 0.020068652899811273
0.0406876403716101 0.07120049865530843
0.025614637185615408 0.012103944727701594
0.12410827758868948 0.0952133281189478
[[0.99955893 0.99955893]
 [0.99955893 0.99955893]]
Epoch 1517: at batch 1: Training dataset Loss=0.039866, Batch Time=0.122
Epoch 1522: at batch 1: Training dataset Loss=0.048645, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.01608271559279828 0.014124036992317957
0.034256444862545976 0.041158652350814735
0.027737134677123798 0.028845014410239577
0.037571005539465574 0.04343554566328027
0.06595742567328511 0.05249739959349362
0.003502981861084997 0.0035748955607498335
0.02265774520218322 0.04377115577149773
0.02041292362561009 0.011020731029350922
0.1816914036986219 0.12156494188996286
0.01013123985329667 0.01789855747514818
0.037571005539465574 0.04343554566328027
0.021877493369564238 0.03387382915679732
0.09729250705004233 0.04420458916016049
0.014435925863836019 0.013791704932067536
0.03232531248124815 0.016751678289584137
0.08361675270936075 0.076951655191186
[[0.03629261 0.04133325]
 [0.03906496 0.03730074]]
Epoch 1527: at batch 1: Training dataset Loss=0.052785, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.05260234 0.04169987]
 [0.07491818 0.03050298]
 [0.02379349 0.01934004]
 [0.02646213 0.01796156]
 [0.07491818 0.03050298]
 [0.00522188 0.0496098 ]
 [0.06710596 0.00570046]
 [0.0589369  0.02025346]
 [0.12171595 0.03482213]
 [0.00911875 0.01455653]]
Epoch 1532: at batch 1: Training dataset Loss=0.059944, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.003534355110517806 0.0014686632539006547
0.11445950523562942 0.05183429782232129
0.01092852719945192 0.006136686341996326
0.009050885659605434 0.0051337239548116444
0.03678446306463812 0.046049473616256706
0.061100288060945473 0.08251142881271775
0.020012212875100133 0.028292381938234173
0.08379120533949891 0.06411014873018425
0.018666251347307927 0.01924230156693418
0.14484584872174722 0.12173986023858686
0.2428416781611169 0.3595961440867191
0.11323703595235557 0.11536656400665156
0.28763441353112285 0.15752159884948427
0.05361743498176352 0.08040789270507699
0.011245950593988363 0.017490836860762485
0.050431443870412096 0.09539369056456762
[[0.00516666 0.00516666]
 [0.00573373 0.0054817 ]]
Epoch 1537: at batch 1: Training dataset Loss=0.052467, Batch Time=0.123
Epoch 1542: at batch 1: Training dataset Loss=0.047240, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.4232368472026593 0.4048412417938053
0.04399740065589697 0.020751491588528945
0.061956267539786225 0.14253771585107589
0.07376523967814919 0.06175152182596558
0.08728846480011043 0.03871037145581961
0.0958545293909232 0.0763006893004919
0.04826448720102583 0.025380065360009876
0.4232368472026593 0.4048412417938053
0.014888905473743108 0.009076938128852527
0.0025563004782984677 0.0018879651926081704
0.07376523967814919 0.06175152182596558
0.056752928205407116 0.12081689278513061
0.06968995466160699 0.07564110390360672
0.13901831908994744 0.09112421132982118
0.07254741508436702 0.08366879175778923
0.09970464846674787 0.0986299292734128
[[0.18410937 0.20792641]
 [0.5819419  0.80480123]]
Epoch 1547: at batch 1: Training dataset Loss=0.059293, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.1570565  0.02055925]
 [0.10477765 0.04308138]
 [0.1570565  0.02055925]
 [0.1570565  0.02055925]
 [0.02816236 0.05616475]
 [0.13919999 0.02244216]
 [0.06537061 0.04755371]
 [0.1103196  0.00265877]
 [0.09432269 0.03522118]
 [0.07915714 0.00226201]]
Epoch 1552: at batch 1: Training dataset Loss=0.052310, Batch Time=0.119


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.12469085163560578 0.06508131058661584
0.06515991527012943 0.02369940903575669
0.07890849665953681 0.03724573482753915
0.09619330221545397 0.04417364525097643
0.035145009012868655 0.040072739738830895
0.01338818902740968 0.006525691753015428
0.02456871554221962 0.022731050999796772
0.017383328595066416 0.009964915680041065
0.09619330221545397 0.04417364525097643
0.0361969895958234 0.04254139709177837
0.0367518928520667 0.027523267746041497
0.011225030916158119 0.00685343437273232
0.02112615640183968 0.01311642332928765
0.035364822682930175 0.022392665229934546
0.0785537267119345 0.09860918377387082
0.08829162418469139 0.04148380865499794
[[0.18524353 0.16684519]
 [0.17818663 0.16634113]]
Epoch 1557: at batch 1: Training dataset Loss=0.048447, Batch Time=0.123
Epoch 1562: at batch 1: Training dataset Loss=0.056638, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.02587852548358427 0.012082657565886265
0.09703312205982684 0.04344757374249707
0.052125188694802205 0.04581002660919021
0.06070452962906181 0.08745480895645222
0.0015323396815922763 0.0005296592469845918
0.034790373637631156 0.05761639875964574
0.15624254707645768 0.11951665797641695
0.2146818036447371 0.08964501403135457
0.0335983248529379 0.05542808819242649
0.03741115119067828 0.03149020511066931
0.18506140884917244 0.2014491058124042
0.014369923980177113 0.014945182590307804
0.0773587272859686 0.06426468599217405
0.11228016250620954 0.06071601439198564
0.06426830052121346 0.04906148875811906
0.1396914555780313 0.07433331994554689
[[0.02999181 0.03175603]
 [0.03099994 0.03200807]]
Epoch 1567: at batch 1: Training dataset Loss=0.063057, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.08866    0.04462625]
 [0.15683942 0.0450399 ]
 [0.06032047 0.05413712]
 [0.15683942 0.0450399 ]
 [0.13479942 0.01054396]
 [0.11392307 0.04318139]
 [0.06032047 0.05413712]
 [0.12616278 0.02676138]
 [0.0589056  0.00815332]
 [0.11392307 0.04318139]]
Epoch 1572: at batch 1: Training dataset Loss=0.047475, Batch Time=0.123
^CTraceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 270, in <module>
    heteroschedastic_noise_s = np.random.normal(0, 1, (BATCH_SIZE,ps,ps,D)) * heteroschedastic_sigma_s
KeyboardInterrupt
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ vi train_heteroscedastic_sigma_map.py
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ python train_heteroscedastic_sigma_map.py




Current date and time : 
2020-12-17 13:16:59
./dataset/Sony/long/00018_00_10s.ARW 18
./dataset/Sony/long/00148_00_30s.ARW 148
./dataset/Sony/long/00029_00_10s.ARW 29
./dataset/Sony/long/00132_00_30s.ARW 132
./dataset/Sony/long/00073_00_30s.ARW 73
./dataset/Sony/long/00033_00_10s.ARW 33
./dataset/Sony/long/00118_00_30s.ARW 118
Found 161 images to train with

Training on files with indices: 0 to 161
Training on 161 images only

2020-12-17 13:16:59.524486: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-17 13:16:59.661223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-17 13:16:59.661258: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-17 13:16:59.947017: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-17 13:16:59.947057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-17 13:16:59.947078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-17 13:16:59.947247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)

Loaded ./heteroscedastic_sigma_map_checkpoint/model.ckpt

last epoch of previous run: 1575


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 10 ,ps 128 ,len(train_fns)= 161 ,
result_dir= ./heteroscedastic_sigma_map_checkpoint/

Cleared all images in memory.

loading ./dataset/Sony/long/00059_00_10s.ARW; images_in_memory= 14
Epoch 1576: at batch 1: Training dataset Loss=0.908571, Batch Time=1.221
Epoch 1576: at batch 1: Training dataset Loss=0.908571, Batch Time=1.221; Early rounds
Epoch 1576: at batch 2: Training dataset Loss=0.817818, Batch Time=0.121; Early rounds
loading ./dataset/Sony/long/00072_00_30s.ARW; images_in_memory= 31
loading ./dataset/Sony/long/00114_00_30s.ARW; images_in_memory= 33
loading ./dataset/Sony/long/00156_00_30s.ARW; images_in_memory= 40
loading ./dataset/Sony/long/00090_00_30s.ARW; images_in_memory= 42
Epoch 1576: at batch 3: Training dataset Loss=0.747322, Batch Time=0.122; Early rounds
loading ./dataset/Sony/long/00026_00_10s.ARW; images_in_memory= 46
loading ./dataset/Sony/long/00039_00_10s.ARW; images_in_memory= 48
loading ./dataset/Sony/long/00024_00_10s.ARW; images_in_memory= 53
Epoch 1576: at batch 4: Training dataset Loss=0.669121, Batch Time=0.125; Early rounds
Epoch 1576: at batch 5: Training dataset Loss=0.602407, Batch Time=0.125; Early rounds
loading ./dataset/Sony/long/00012_00_10s.ARW; images_in_memory= 67
Epoch 1576: at batch 6: Training dataset Loss=0.566010, Batch Time=0.126; Early rounds
Epoch 1576: at batch 7: Training dataset Loss=0.503802, Batch Time=0.124; Early rounds
loading ./dataset/Sony/long/00164_00_30s.ARW; images_in_memory= 84
loading ./dataset/Sony/long/00057_00_10s.ARW; images_in_memory= 88
Epoch 1576: at batch 8: Training dataset Loss=0.474643, Batch Time=0.125; Early rounds
loading ./dataset/Sony/long/00200_00_10s.ARW; images_in_memory= 94
Epoch 1576: at batch 9: Training dataset Loss=0.437800, Batch Time=0.121; Early rounds
loading ./dataset/Sony/long/00018_00_10s.ARW; images_in_memory= 103
Epoch 1576: at batch 10: Training dataset Loss=0.377861, Batch Time=0.123; Early rounds
		Epoch 1576:  Time = 19.637, Avg epoch time=19.637, Current epoch Time=9.819

Loss vector (slice for the first 10 images)
[[0.06917382 0.01370312]
 [0.01633689 0.0571277 ]
 [0.08746558 0.00934033]
 [0.01633689 0.0571277 ]
 [0.00740616 0.02793433]
 [0.14060933 0.04764681]
 [1.         1.        ]
 [0.06351457 0.01369144]
 [1.         1.        ]
 [0.00740616 0.02793433]]
loading ./dataset/Sony/long/00128_00_30s.ARW; images_in_memory= 110
Epoch 1577: at batch 1: Training dataset Loss=0.327427, Batch Time=0.125
Epoch 1577: at batch 1: Training dataset Loss=0.327427, Batch Time=0.125; Early rounds
Epoch 1577: at batch 2: Training dataset Loss=0.318006, Batch Time=0.126; Early rounds
loading ./dataset/Sony/long/00084_00_30s.ARW; images_in_memory= 119
Epoch 1577: at batch 3: Training dataset Loss=0.286858, Batch Time=0.120; Early rounds
Epoch 1577: at batch 4: Training dataset Loss=0.260589, Batch Time=0.134; Early rounds
Epoch 1577: at batch 5: Training dataset Loss=0.249539, Batch Time=0.129; Early rounds
Epoch 1577: at batch 6: Training dataset Loss=0.236299, Batch Time=0.126; Early rounds
loading ./dataset/Sony/long/00219_00_10s.ARW; images_in_memory= 131
Epoch 1577: at batch 7: Training dataset Loss=0.204622, Batch Time=0.123; Early rounds
Epoch 1577: at batch 8: Training dataset Loss=0.186120, Batch Time=0.121; Early rounds
Epoch 1577: at batch 9: Training dataset Loss=0.176434, Batch Time=0.124; Early rounds
Epoch 1577: at batch 10: Training dataset Loss=0.169004, Batch Time=0.122; Early rounds
		Epoch 1577:  Time = 26.874, Avg epoch time=7.197, Current epoch Time=8.958

Loss vector (slice for the first 10 images)
[[0.06917382 0.01370312]
 [0.00446238 0.05878406]
 [0.03079008 0.00549657]
 [0.03079008 0.00549657]
 [0.11805403 0.01363321]
 [0.14060933 0.04764681]
 [1.         1.        ]
 [0.03256583 0.01206446]
 [1.         1.        ]
 [0.12773186 0.00792495]]
Epoch 1578: at batch 1: Training dataset Loss=0.161053, Batch Time=0.123
loading ./dataset/Sony/long/00038_00_10s.ARW; images_in_memory= 153
Epoch 1579: at batch 1: Training dataset Loss=0.067399, Batch Time=0.125
Epoch 1580: at batch 1: Training dataset Loss=0.058984, Batch Time=0.121
Epoch 1581: at batch 1: Training dataset Loss=0.052646, Batch Time=0.123
Epoch 1582: at batch 1: Training dataset Loss=0.059939, Batch Time=0.121
Epoch 1583: at batch 1: Training dataset Loss=0.058643, Batch Time=0.121
Epoch 1584: at batch 1: Training dataset Loss=0.061248, Batch Time=0.125
Epoch 1586: at batch 1: Training dataset Loss=0.076949, Batch Time=0.125
Epoch 1591: at batch 1: Training dataset Loss=0.050215, Batch Time=0.124
Epoch 1596: at batch 1: Training dataset Loss=0.045398, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.13696462 0.01294389]
 [0.01979579 0.00817169]
 [0.02128755 0.05168032]
 [0.01147616 0.00415138]
 [0.09538309 0.03995208]
 [0.0120465  0.02630366]
 [0.06359576 0.03355062]
 [0.07500166 0.03301763]
 [0.10273793 0.00138436]
 [0.08678449 0.029806  ]]
Epoch 1601: at batch 1: Training dataset Loss=0.057424, Batch Time=0.125
Epoch 1606: at batch 1: Training dataset Loss=0.052764, Batch Time=0.122
Epoch 1611: at batch 1: Training dataset Loss=0.046653, Batch Time=0.124
Epoch 1616: at batch 1: Training dataset Loss=0.047411, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.10860312 0.0432558 ]
 [0.02889939 0.04540634]
 [0.05659263 0.04074925]
 [0.02889939 0.04540634]
 [0.08819861 0.025857  ]
 [0.13493889 0.02072644]
 [0.14321116 0.04398263]
 [0.13493889 0.02072644]
 [0.12882084 0.04834318]
 [0.13493889 0.02072644]]
Epoch 1621: at batch 1: Training dataset Loss=0.050810, Batch Time=0.126
Epoch 1626: at batch 1: Training dataset Loss=0.058836, Batch Time=0.121
Epoch 1631: at batch 1: Training dataset Loss=0.057311, Batch Time=0.122
Epoch 1636: at batch 1: Training dataset Loss=0.057014, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.09515127 0.01446011]
 [0.0571147  0.00919389]
 [0.03621399 0.00511646]
 [0.0571147  0.00919389]
 [0.07797194 0.01703024]
 [0.04503151 0.04971938]
 [0.12926878 0.01193996]
 [0.12327819 0.02143309]
 [0.15647495 0.0363477 ]
 [0.04503151 0.04971938]]
Epoch 1641: at batch 1: Training dataset Loss=0.054935, Batch Time=0.124
Epoch 1646: at batch 1: Training dataset Loss=0.048128, Batch Time=0.122
Epoch 1651: at batch 1: Training dataset Loss=0.045966, Batch Time=0.124
Epoch 1656: at batch 1: Training dataset Loss=0.064845, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.13888458 0.04877335]
 [0.13974945 0.04813585]
 [0.12754671 0.01984867]
 [0.13888458 0.04877335]
 [0.12754671 0.01984867]
 [0.13888458 0.04877335]
 [0.1244576  0.0284944 ]
 [0.01197485 0.05105743]
 [0.13888458 0.04877335]
 [0.06053925 0.00884074]]
Epoch 1661: at batch 1: Training dataset Loss=0.063147, Batch Time=0.123
Epoch 1666: at batch 1: Training dataset Loss=0.060632, Batch Time=0.123
Epoch 1671: at batch 1: Training dataset Loss=0.051900, Batch Time=0.125
Epoch 1676: at batch 1: Training dataset Loss=0.045298, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.05533694 0.01391409]
 [0.11560629 0.01939698]
 [0.10623638 0.02823897]
 [0.04811606 0.01525936]
 [0.11560629 0.01939698]
 [0.13416801 0.04210041]
 [0.03705816 0.00985339]
 [0.13198582 0.02340001]
 [0.06551496 0.0575036 ]
 [0.06416971 0.04312535]]
Epoch 1681: at batch 1: Training dataset Loss=0.051561, Batch Time=0.124
Epoch 1686: at batch 1: Training dataset Loss=0.043015, Batch Time=0.121
Epoch 1691: at batch 1: Training dataset Loss=0.065159, Batch Time=0.122
Epoch 1696: at batch 1: Training dataset Loss=0.051551, Batch Time=0.126
Loss vector (slice for the first 10 images)
[[0.11589667 0.02988059]
 [0.07076605 0.00973236]
 [0.03008167 0.03184291]
 [0.12004444 0.00054571]
 [0.1146782  0.00935253]
 [0.10129893 0.03026768]
 [0.04872322 0.03024082]
 [0.1365885  0.02434664]
 [0.0411984  0.01061164]
 [0.11218023 0.01097181]]
Epoch 1701: at batch 1: Training dataset Loss=0.056843, Batch Time=0.120
Epoch 1706: at batch 1: Training dataset Loss=0.055389, Batch Time=0.123
Epoch 1711: at batch 1: Training dataset Loss=0.056449, Batch Time=0.122
Epoch 1716: at batch 1: Training dataset Loss=0.043726, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.09083413 0.04226007]
 [0.07659524 0.04319089]
 [0.07659524 0.04319089]
 [0.14082043 0.04916249]
 [0.07659524 0.04319089]
 [0.11072475 0.05832427]
 [0.09760275 0.05815585]
 [0.00499437 0.02110629]
 [0.06880036 0.02504661]
 [0.00499437 0.02110629]]
Epoch 1721: at batch 1: Training dataset Loss=0.057061, Batch Time=0.119
Epoch 1726: at batch 1: Training dataset Loss=0.048672, Batch Time=0.124
Epoch 1731: at batch 1: Training dataset Loss=0.049892, Batch Time=0.125
Epoch 1736: at batch 1: Training dataset Loss=0.057961, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.03162737 0.03159047]
 [0.044416   0.00306574]
 [0.12226057 0.05614717]
 [0.04663553 0.03231824]
 [0.1302609  0.01338103]
 [0.04584961 0.01297634]
 [0.12226057 0.05614717]
 [0.14623972 0.01631188]
 [0.11178628 0.019967  ]
 [0.12226057 0.05614717]]
Epoch 1741: at batch 1: Training dataset Loss=0.067108, Batch Time=0.122
Epoch 1746: at batch 1: Training dataset Loss=0.055666, Batch Time=0.121
Epoch 1751: at batch 1: Training dataset Loss=0.056078, Batch Time=0.123
Epoch 1756: at batch 1: Training dataset Loss=0.046471, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.0601435  0.04446754]
 [0.06859607 0.04932011]
 [0.02032182 0.0092754 ]
 [0.12035171 0.05978451]
 [0.01824243 0.03076502]
 [0.02367912 0.01859721]
 [0.1238179  0.0059681 ]
 [0.01824243 0.03076502]
 [0.03417709 0.05338636]
 [0.12868401 0.03597991]]
Epoch 1761: at batch 1: Training dataset Loss=0.053599, Batch Time=0.121
Epoch 1766: at batch 1: Training dataset Loss=0.049965, Batch Time=0.120
Epoch 1771: at batch 1: Training dataset Loss=0.060982, Batch Time=0.126


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.04933105935323567 0.020274720079920823
0.07769944988918454 0.04938229482727311
0.013552692801786392 0.009910226057721854
0.01956834180462863 0.017468755033580908
0.020646696220405047 0.013099802997618131
0.02376601919581489 0.03958396901392201
0.1496122316149524 0.1114413050113867
0.015658927761704522 0.012550692759764392
0.0031555764108199824 0.0019456894928966638
0.014159560149597183 0.006255952132947926
0.022091611546062317 0.034623364636987564
0.060283013632760785 0.09553660137000049
0.020678434812084312 0.04433848423821801
0.1149177698270023 0.06580640529483349
0.001160056384854613 0.0004516519204170233
0.029029998571655113 0.03246157756642844
[[0.07019091 0.07271124]
 [0.06918278 0.07472749]]
heteroschedastic_sigma_s [0.07161831]
sigma_c 0.0575648154345
[[ 0.03589478  0.00062929]
 [ 0.00855542 -0.00071406]]
[[ 0.15138216 -0.01935749]
 [-0.00416797  0.03498899]] 

Epoch 1776: at batch 1: Training dataset Loss=0.060058, Batch Time=0.120
		Epoch 1776:  Time = 279.968, Avg epoch time=1.228, Current epoch Time=1.386

Loss vector (slice for the first 10 images)
[[0.06350398 0.02798297]
 [0.13751239 0.0163551 ]
 [0.08241282 0.03702702]
 [0.00567668 0.0081924 ]
 [0.00271075 0.0368081 ]
 [0.08241282 0.03702702]
 [0.14903506 0.04765972]
 [0.14903506 0.04765972]
 [0.00271075 0.0368081 ]
 [0.11665084 0.05692681]]
Epoch 1781: at batch 1: Training dataset Loss=0.060529, Batch Time=0.120
Epoch 1786: at batch 1: Training dataset Loss=0.061543, Batch Time=0.125
Epoch 1791: at batch 1: Training dataset Loss=0.046733, Batch Time=0.122
Epoch 1796: at batch 1: Training dataset Loss=0.061623, Batch Time=0.122
Loss vector (slice for the first 10 images)
[[0.01321734 0.00222183]
 [0.08887879 0.02211086]
 [0.01321734 0.00222183]
 [0.0685751  0.0246664 ]
 [0.14780905 0.04042358]
 [0.06958815 0.03189417]
 [0.07897903 0.03459025]
 [0.11915952 0.03805637]
 [0.07289124 0.00364892]
 [0.12724625 0.04696944]]
Epoch 1801: at batch 1: Training dataset Loss=0.049748, Batch Time=0.124
Epoch 1806: at batch 1: Training dataset Loss=0.050419, Batch Time=0.125
Epoch 1811: at batch 1: Training dataset Loss=0.043952, Batch Time=0.121
Epoch 1816: at batch 1: Training dataset Loss=0.056198, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[9.77860783e-02 2.25420376e-02]
 [3.42322312e-02 2.32286064e-02]
 [1.24954082e-01 4.38887514e-03]
 [9.58731809e-02 5.44103806e-02]
 [1.26880745e-01 2.25577179e-02]
 [5.87089180e-02 4.83478958e-02]
 [1.14830771e-02 2.74645777e-02]
 [9.58731809e-02 5.44103806e-02]
 [7.70660971e-02 2.58450675e-02]
 [4.82180644e-05 2.92403212e-02]]
Epoch 1821: at batch 1: Training dataset Loss=0.045309, Batch Time=0.126
Epoch 1826: at batch 1: Training dataset Loss=0.057441, Batch Time=0.120
Epoch 1831: at batch 1: Training dataset Loss=0.049815, Batch Time=0.123
Epoch 1836: at batch 1: Training dataset Loss=0.046761, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.13834793 0.00777815]
 [0.01666941 0.01489429]
 [0.09746309 0.04805063]
 [0.08843171 0.04519249]
 [0.0832086  0.01639382]
 [0.04857633 0.00975409]
 [0.01666941 0.01489429]
 [0.01352379 0.04534134]
 [0.05790706 0.02782626]
 [0.00258673 0.01952482]]
Epoch 1841: at batch 1: Training dataset Loss=0.056957, Batch Time=0.121
Epoch 1846: at batch 1: Training dataset Loss=0.059827, Batch Time=0.122
Epoch 1851: at batch 1: Training dataset Loss=0.046380, Batch Time=0.126
Epoch 1856: at batch 1: Training dataset Loss=0.058718, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.15852233 0.03947254]
 [0.15794167 0.04521634]
 [0.14493867 0.01897934]
 [0.08108146 0.006731  ]
 [0.07919543 0.01310997]
 [0.10140658 0.03354186]
 [0.01383102 0.02589556]
 [0.02238313 0.02004547]
 [0.02238313 0.02004547]
 [0.10140658 0.03354186]]
Epoch 1861: at batch 1: Training dataset Loss=0.053766, Batch Time=0.120
Epoch 1866: at batch 1: Training dataset Loss=0.055381, Batch Time=0.121
Epoch 1871: at batch 1: Training dataset Loss=0.043358, Batch Time=0.121
Epoch 1876: at batch 1: Training dataset Loss=0.049324, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.0039421  0.01266441]
 [0.0327147  0.0032354 ]
 [0.06972233 0.03137486]
 [0.06972233 0.03137486]
 [0.06972233 0.03137486]
 [0.08509942 0.03191471]
 [0.08397081 0.02457219]
 [0.01421839 0.01566824]
 [0.06946847 0.05931932]
 [0.06289656 0.01365961]]
Epoch 1881: at batch 1: Training dataset Loss=0.046371, Batch Time=0.119
Epoch 1886: at batch 1: Training dataset Loss=0.052994, Batch Time=0.120
Epoch 1891: at batch 1: Training dataset Loss=0.049630, Batch Time=0.122
Epoch 1896: at batch 1: Training dataset Loss=0.057191, Batch Time=0.122
Loss vector (slice for the first 10 images)
[[0.12050448 0.02789779]
 [0.04854939 0.04332877]
 [0.05384056 0.04471964]
 [0.00947165 0.03472537]
 [0.07877291 0.03424117]
 [0.00947165 0.03472537]
 [0.04003152 0.0319117 ]
 [0.10031364 0.0212099 ]
 [0.15651155 0.02372458]
 [0.11801168 0.01925699]]
Epoch 1901: at batch 1: Training dataset Loss=0.063596, Batch Time=0.120
Epoch 1906: at batch 1: Training dataset Loss=0.059401, Batch Time=0.124
Epoch 1911: at batch 1: Training dataset Loss=0.061879, Batch Time=0.123
Epoch 1916: at batch 1: Training dataset Loss=0.057881, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.11877263 0.00226563]
 [0.00178644 0.03419559]
 [0.00178644 0.03419559]
 [0.01155359 0.00935226]
 [0.03996876 0.00445307]
 [0.0455747  0.04532418]
 [0.01605935 0.05540747]
 [0.0455747  0.04532418]
 [0.05559867 0.04383674]
 [0.11695762 0.01315826]]
Epoch 1921: at batch 1: Training dataset Loss=0.055307, Batch Time=0.124
Epoch 1926: at batch 1: Training dataset Loss=0.055659, Batch Time=0.122
Epoch 1931: at batch 1: Training dataset Loss=0.046817, Batch Time=0.126
Epoch 1936: at batch 1: Training dataset Loss=0.054561, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.10451336 0.0051657 ]
 [0.08416142 0.02019757]
 [0.00359315 0.02895473]
 [0.09090509 0.02537034]
 [0.09090509 0.02537034]
 [0.09294394 0.01428365]
 [0.04428093 0.05756222]
 [0.15931347 0.03601271]
 [0.02792789 0.05419852]
 [0.04428093 0.05756222]]
Epoch 1941: at batch 1: Training dataset Loss=0.065818, Batch Time=0.124
Epoch 1946: at batch 1: Training dataset Loss=0.049089, Batch Time=0.126
Epoch 1951: at batch 1: Training dataset Loss=0.060176, Batch Time=0.122
Epoch 1956: at batch 1: Training dataset Loss=0.048265, Batch Time=0.118
Loss vector (slice for the first 10 images)
[[0.12483557 0.00054447]
 [0.11235846 0.04502678]
 [0.08601681 0.03039881]
 [0.0794985  0.03485397]
 [0.11235846 0.04502678]
 [0.1265844  0.0554115 ]
 [0.02262719 0.04541739]
 [0.04664787 0.01193696]
 [0.02262719 0.04541739]
 [0.11875976 0.04304668]]
Epoch 1961: at batch 1: Training dataset Loss=0.056527, Batch Time=0.121
Epoch 1966: at batch 1: Training dataset Loss=0.048479, Batch Time=0.126
Epoch 1971: at batch 1: Training dataset Loss=0.063753, Batch Time=0.125


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.030109422106420425 0.019418057142332672
0.06018590962482051 0.028778353597263023
0.029284584138252967 0.021990870674638358
0.008702141910833738 0.010388495314920088
0.0882162512863971 0.08232009262340781
0.019540145111745844 0.009371535465840464
0.00763333446096226 0.008550879500363368
0.023502957722115525 0.012014884872597527
0.07092489463015283 0.06857876338473724
0.16920863647979445 0.11793454002512917
0.011068613677865935 0.015862760249599466
0.057445207666189546 0.05128787453649697
0.008946511360635734 0.006329819545715181
0.15357432124943493 0.15099788675627057
0.004209564456006731 0.0038801056792454737
0.031344388939853474 0.014152020371756745
[[0.02740848 0.02973978]
 [0.02665238 0.03188205]]
heteroschedastic_sigma_s [0.08657234]
sigma_c 0.0054522756032
[[-0.00274121 -0.00106031]
 [ 0.00284736  0.00260738]]
[[ 0.00271259 -0.00184341]
 [-0.00090428 -0.00424747]] 

Epoch 1976: at batch 1: Training dataset Loss=0.052565, Batch Time=0.124
		Epoch 1976:  Time = 530.805, Avg epoch time=1.225, Current epoch Time=1.320

Loss vector (slice for the first 10 images)
[[0.07490966 0.04030479]
 [0.00290131 0.01572361]
 [0.14822573 0.04768119]
 [0.01415644 0.01039036]
 [0.1459093  0.05514015]
 [0.07497581 0.03314121]
 [0.14822573 0.04768119]
 [0.08763327 0.03608127]
 [0.04399507 0.00444437]
 [0.02987946 0.05409821]]
Epoch 1981: at batch 1: Training dataset Loss=0.058432, Batch Time=0.122
Epoch 1986: at batch 1: Training dataset Loss=0.060188, Batch Time=0.124
Epoch 1991: at batch 1: Training dataset Loss=0.049700, Batch Time=0.125
Epoch 1996: at batch 1: Training dataset Loss=0.049848, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.13401072 0.04412094]
 [0.1092463  0.04935785]
 [0.00145203 0.03455237]
 [0.08327546 0.03629752]
 [0.14115409 0.02352712]
 [0.12638447 0.03918058]
 [0.03703272 0.01645288]
 [0.04578657 0.0124215 ]
 [0.00029537 0.01989479]
 [0.09846825 0.02200388]]
Epoch 2001: at batch 1: Training dataset Loss=0.052789, Batch Time=0.123
Epoch 2006: at batch 1: Training dataset Loss=0.047242, Batch Time=0.124
Epoch 2011: at batch 1: Training dataset Loss=0.055018, Batch Time=0.120
Epoch 2016: at batch 1: Training dataset Loss=0.051606, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.05960964 0.02906096]
 [0.02171286 0.0557426 ]
 [0.09913588 0.02835237]
 [0.01732141 0.03760242]
 [0.01732141 0.03760242]
 [0.11131298 0.03010311]
 [0.0628618  0.00731612]
 [0.11131298 0.03010311]
 [0.00720904 0.03628962]
 [0.05479263 0.0077943 ]]
Epoch 2021: at batch 1: Training dataset Loss=0.056824, Batch Time=0.120
Epoch 2026: at batch 1: Training dataset Loss=0.050875, Batch Time=0.122
Epoch 2031: at batch 1: Training dataset Loss=0.050380, Batch Time=0.122
Epoch 2036: at batch 1: Training dataset Loss=0.053384, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.10849153 0.05938672]
 [0.02348508 0.05505903]
 [0.14553744 0.00355627]
 [0.06788198 0.01505739]
 [0.03471144 0.00237293]
 [0.00750762 0.0168395 ]
 [0.07559301 0.01426949]
 [0.1515334  0.02840918]
 [0.15486763 0.04386611]
 [0.06788198 0.01505739]]
Epoch 2041: at batch 1: Training dataset Loss=0.054652, Batch Time=0.124
Epoch 2046: at batch 1: Training dataset Loss=0.049468, Batch Time=0.125
Epoch 2051: at batch 1: Training dataset Loss=0.055336, Batch Time=0.125
Epoch 2056: at batch 1: Training dataset Loss=0.052070, Batch Time=0.122
Loss vector (slice for the first 10 images)
[[9.11417830e-03 8.30285819e-03]
 [1.43333273e-01 2.65790114e-02]
 [1.30765895e-01 3.73991208e-02]
 [1.43333273e-01 2.65790114e-02]
 [4.81371607e-02 7.32171639e-06]
 [1.28205706e-01 1.77454591e-02]
 [2.80489397e-02 5.68870231e-02]
 [7.50602256e-02 2.58032332e-02]
 [5.91066788e-02 4.58549150e-02]
 [2.80489397e-02 5.68870231e-02]]
Epoch 2061: at batch 1: Training dataset Loss=0.062755, Batch Time=0.122
Epoch 2066: at batch 1: Training dataset Loss=0.057385, Batch Time=0.121
Epoch 2071: at batch 1: Training dataset Loss=0.046056, Batch Time=0.121
Epoch 2076: at batch 1: Training dataset Loss=0.053020, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.14518533 0.02181296]
 [0.02993869 0.05550926]
 [0.05914757 0.03839761]
 [0.09379737 0.03032029]
 [0.14518533 0.02181296]
 [0.11435022 0.02756069]
 [0.06184617 0.0317846 ]
 [0.06158594 0.04938236]
 [0.06184617 0.0317846 ]
 [0.13527773 0.01970733]]
Epoch 2081: at batch 1: Training dataset Loss=0.057333, Batch Time=0.120
Epoch 2086: at batch 1: Training dataset Loss=0.054853, Batch Time=0.120
Epoch 2091: at batch 1: Training dataset Loss=0.049305, Batch Time=0.122
Epoch 2096: at batch 1: Training dataset Loss=0.056734, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.15669449 0.05027106]
 [0.07633296 0.03725651]
 [0.08358922 0.03409855]
 [0.11712743 0.03583568]
 [0.083853   0.04024498]
 [0.07106263 0.00805243]
 [0.10631227 0.02697028]
 [0.00305451 0.04177977]
 [0.07633296 0.03725651]
 [0.07106263 0.00805243]]
Epoch 2101: at batch 1: Training dataset Loss=0.061490, Batch Time=0.123
Epoch 2106: at batch 1: Training dataset Loss=0.048374, Batch Time=0.123
Epoch 2111: at batch 1: Training dataset Loss=0.048495, Batch Time=0.125
Epoch 2116: at batch 1: Training dataset Loss=0.059928, Batch Time=0.122
Loss vector (slice for the first 10 images)
[[0.13209939 0.01376406]
 [0.02772286 0.01798407]
 [0.12039599 0.04725672]
 [0.02772286 0.01798407]
 [0.14139192 0.0319743 ]
 [0.13209939 0.01376406]
 [0.10776058 0.04663251]
 [0.05900706 0.03281838]
 [0.13209939 0.01376406]
 [0.14463407 0.01121562]]
Epoch 2121: at batch 1: Training dataset Loss=0.057647, Batch Time=0.124
Epoch 2126: at batch 1: Training dataset Loss=0.046071, Batch Time=0.120
Epoch 2131: at batch 1: Training dataset Loss=0.054159, Batch Time=0.126
Epoch 2136: at batch 1: Training dataset Loss=0.045292, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.03979735 0.01315721]
 [0.13391966 0.03071726]
 [0.06405853 0.03137774]
 [0.15254043 0.00586468]
 [0.15278217 0.01727874]
 [0.06405853 0.03137774]
 [0.03938247 0.0303478 ]
 [0.08176409 0.00882995]
 [0.14565268 0.03899502]
 [0.08176409 0.00882995]]
Epoch 2141: at batch 1: Training dataset Loss=0.054955, Batch Time=0.122
Epoch 2146: at batch 1: Training dataset Loss=0.053158, Batch Time=0.125
Epoch 2151: at batch 1: Training dataset Loss=0.053639, Batch Time=0.120
Epoch 2156: at batch 1: Training dataset Loss=0.058634, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.13022836 0.00733054]
 [0.0032678  0.04538266]
 [0.08703906 0.0396585 ]
 [0.00034647 0.02701326]
 [0.00445368 0.04379522]
 [0.08703906 0.0396585 ]
 [0.1188555  0.04860547]
 [0.13987265 0.02922069]
 [0.07141577 0.05084753]
 [0.04236165 0.00565467]]
Epoch 2161: at batch 1: Training dataset Loss=0.068618, Batch Time=0.123
Epoch 2166: at batch 1: Training dataset Loss=0.061794, Batch Time=0.118
Epoch 2171: at batch 1: Training dataset Loss=0.054777, Batch Time=0.124


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.08469871441329246 0.05227575924378882
0.16248916516437362 0.11675308680758735
0.012676030135986949 0.007587297983695619
0.15554581714032167 0.07630536049004388
0.0848551845458303 0.0486468016679697
0.01722047842769303 0.048865512368895254
0.10758385892333955 0.139763850733435
0.06832212938676463 0.051541038518664525
0.08443624515963677 0.06705578558870107
0.027539503975056334 0.009921125777812699
0.031157183961937562 0.03332883879061227
0.047678924084662455 0.03889272894160825
0.21575459522730966 0.1237467914934891
0.0739297954263698 0.03876618025410951
0.0012337977455114668 0.0007637766027170275
0.06187117464566683 0.04936434694998719
[[0.11694285 0.10585345]
 [0.12147943 0.10686157]]
heteroschedastic_sigma_s [0.05055368]
sigma_c 0.0461215506471
[[ 1.45269356e-02  1.25909691e-02]
 [-9.04370499e-06 -1.31198099e-02]]
[[0.0132667  0.0128607 ]
 [0.0499543  0.07045859]] 

Epoch 2176: at batch 1: Training dataset Loss=0.055254, Batch Time=0.120
		Epoch 2176:  Time = 781.538, Avg epoch time=1.227, Current epoch Time=1.298

Loss vector (slice for the first 10 images)
[[0.06183355 0.00776005]
 [0.06183355 0.00776005]
 [0.13518717 0.04888856]
 [0.02243719 0.05364106]
 [0.06183355 0.00776005]
 [0.13518717 0.04888856]
 [0.06593303 0.02293617]
 [0.01481368 0.05026891]
 [0.02555244 0.00556893]
 [0.05257276 0.0008762 ]]
Epoch 2181: at batch 1: Training dataset Loss=0.057382, Batch Time=0.125
Epoch 2186: at batch 1: Training dataset Loss=0.052231, Batch Time=0.121
Epoch 2191: at batch 1: Training dataset Loss=0.056626, Batch Time=0.125
Epoch 2196: at batch 1: Training dataset Loss=0.055421, Batch Time=0.119
Loss vector (slice for the first 10 images)
[[0.04635549 0.0055499 ]
 [0.13875545 0.03343109]
 [0.00846232 0.0329665 ]
 [0.04635549 0.0055499 ]
 [0.09447857 0.03708721]
 [0.04635549 0.0055499 ]
 [0.09291957 0.05984127]
 [0.09291957 0.05984127]
 [0.02856424 0.04483112]
 [0.09491502 0.02136174]]
Epoch 2201: at batch 1: Training dataset Loss=0.042180, Batch Time=0.123
Epoch 2206: at batch 1: Training dataset Loss=0.054864, Batch Time=0.120
Epoch 2211: at batch 1: Training dataset Loss=0.057942, Batch Time=0.120
Epoch 2216: at batch 1: Training dataset Loss=0.051664, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.06841224 0.01932464]
 [0.12859126 0.02151759]
 [0.03347727 0.02848728]
 [0.0568849  0.02646778]
 [0.06841224 0.01932464]
 [0.08812419 0.03113189]
 [0.08055288 0.04857136]
 [0.15986987 0.0321952 ]
 [0.08055288 0.04857136]
 [0.06841224 0.01932464]]
Epoch 2221: at batch 1: Training dataset Loss=0.049403, Batch Time=0.120
Epoch 2226: at batch 1: Training dataset Loss=0.054120, Batch Time=0.125
Epoch 2231: at batch 1: Training dataset Loss=0.055883, Batch Time=0.124
Epoch 2236: at batch 1: Training dataset Loss=0.067897, Batch Time=0.121
Loss vector (slice for the first 10 images)
[[0.04139621 0.05588718]
 [0.15465641 0.04202608]
 [0.15613078 0.02890997]
 [0.11230482 0.00666947]
 [0.11230482 0.00666947]
 [0.07076348 0.00158176]
 [0.03955178 0.04946415]
 [0.09242939 0.05588762]
 [0.09827616 0.02924267]
 [0.11230482 0.00666947]]
Epoch 2241: at batch 1: Training dataset Loss=0.052698, Batch Time=0.119
Epoch 2246: at batch 1: Training dataset Loss=0.055579, Batch Time=0.124
Epoch 2251: at batch 1: Training dataset Loss=0.051221, Batch Time=0.125
Epoch 2256: at batch 1: Training dataset Loss=0.056275, Batch Time=0.121
Loss vector (slice for the first 10 images)
[[0.15794991 0.0257201 ]
 [0.13763197 0.04733077]
 [0.04905471 0.03907705]
 [0.02577614 0.05888236]
 [0.0650757  0.02683403]
 [0.04079725 0.03310284]
 [0.08411713 0.00915115]
 [0.0650757  0.02683403]
 [0.00104996 0.00490642]
 [0.07054428 0.03732673]]
Epoch 2261: at batch 1: Training dataset Loss=0.056847, Batch Time=0.125
Epoch 2266: at batch 1: Training dataset Loss=0.054049, Batch Time=0.123
Epoch 2271: at batch 1: Training dataset Loss=0.049333, Batch Time=0.121
Epoch 2276: at batch 1: Training dataset Loss=0.060444, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.06707959 0.0175821 ]
 [0.08345828 0.04423834]
 [0.08345828 0.04423834]
 [0.07539388 0.02686537]
 [0.06707959 0.0175821 ]
 [0.07252527 0.05073265]
 [0.03513895 0.02245521]
 [0.03419382 0.02741546]
 [0.03513895 0.02245521]
 [0.01355699 0.04977248]]
Epoch 2281: at batch 1: Training dataset Loss=0.050983, Batch Time=0.122
Epoch 2286: at batch 1: Training dataset Loss=0.044695, Batch Time=0.124
Epoch 2291: at batch 1: Training dataset Loss=0.055393, Batch Time=0.123
Epoch 2296: at batch 1: Training dataset Loss=0.060703, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[1.47170604e-01 2.89277701e-02]
 [1.06083774e-01 1.18256666e-02]
 [1.12957068e-01 5.68630451e-02]
 [9.91912637e-05 3.75412953e-02]
 [2.34296490e-02 4.57841627e-02]
 [4.43031697e-02 2.32720141e-02]
 [1.74369739e-02 5.53701965e-02]
 [9.75341362e-02 3.12319398e-02]
 [9.63078688e-02 9.18021197e-03]
 [1.54446776e-01 1.29333762e-02]]
Epoch 2301: at batch 1: Training dataset Loss=0.049229, Batch Time=0.119
Epoch 2306: at batch 1: Training dataset Loss=0.062024, Batch Time=0.121
Epoch 2311: at batch 1: Training dataset Loss=0.062188, Batch Time=0.118
Epoch 2316: at batch 1: Training dataset Loss=0.046912, Batch Time=0.118
Loss vector (slice for the first 10 images)
[[0.02655234 0.04067198]
 [0.09914342 0.03881346]
 [0.00848768 0.01851264]
 [0.02526906 0.01439057]
 [0.13092675 0.05439102]
 [0.02655234 0.04067198]
 [0.11176506 0.05566206]
 [0.12536276 0.01596835]
 [0.09010472 0.05718363]
 [0.02655234 0.04067198]]
Epoch 2321: at batch 1: Training dataset Loss=0.062642, Batch Time=0.120
Epoch 2326: at batch 1: Training dataset Loss=0.056889, Batch Time=0.124
Epoch 2331: at batch 1: Training dataset Loss=0.058478, Batch Time=0.124
Epoch 2336: at batch 1: Training dataset Loss=0.052815, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.03002501 0.01771688]
 [0.02234654 0.05956429]
 [0.02234654 0.05956429]
 [0.05147975 0.04818068]
 [0.07429312 0.00318016]
 [0.02234654 0.05956429]
 [0.14940025 0.05580037]
 [0.04009721 0.02600636]
 [0.04035704 0.04431444]
 [0.11882422 0.01515492]]
Epoch 2341: at batch 1: Training dataset Loss=0.050168, Batch Time=0.124
Epoch 2346: at batch 1: Training dataset Loss=0.054160, Batch Time=0.124
Epoch 2351: at batch 1: Training dataset Loss=0.056731, Batch Time=0.124
Epoch 2356: at batch 1: Training dataset Loss=0.040675, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.06504582 0.01411695]
 [0.12536475 0.04587194]
 [0.15784787 0.0484938 ]
 [0.03325029 0.0211001 ]
 [0.11491222 0.03866785]
 [0.00743627 0.00485771]
 [0.11491222 0.03866785]
 [0.11467996 0.03445512]
 [0.05005286 0.02231361]
 [0.08691453 0.05291968]]
Epoch 2361: at batch 1: Training dataset Loss=0.052980, Batch Time=0.125
Epoch 2366: at batch 1: Training dataset Loss=0.053912, Batch Time=0.121
Epoch 2371: at batch 1: Training dataset Loss=0.064214, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.10624184020913319 0.21178829511505085
0.18824146317312795 0.09975526568981716
0.02993536171641331 0.02406095593638376
0.058240349085096454 0.14111671798545614
0.04806449907185595 0.020193288925597404
0.08298187421959824 0.03763085911111919
0.05888681085584935 0.063272070002679
0.20605984077681683 0.18875626943199772
0.04036663565947318 0.027581573677094025
0.07511781101899118 0.1862141517395447
0.04806449907185595 0.020193288925597404
0.06418767150000804 0.035881046021305804
0.16183531868780676 0.10216888257998051
0.04122167297401802 0.04283708351219311
0.6592715834434983 0.3433060267658541
0.020038771302204772 0.019435448499311007
[[0.03200807 0.02545523]
 [0.02646336 0.02822758]]
heteroschedastic_sigma_s [0.01049822]
sigma_c 0.0152063155649
[[ 9.48034300e-04  4.97768873e-04]
 [-4.78445327e-05  9.04726248e-04]]
[[0.0163179  0.02009985]
 [0.00358572 0.02464475]] 

Epoch 2376: at batch 1: Training dataset Loss=0.055250, Batch Time=0.125
		Epoch 2376:  Time = 1032.091, Avg epoch time=1.225, Current epoch Time=1.287

Loss vector (slice for the first 10 images)
[[0.04863173 0.02316293]
 [0.05433815 0.05980449]
 [0.02776888 0.03680518]
 [0.08838615 0.05169166]
 [0.115875   0.02480743]
 [0.02726867 0.04421451]
 [0.05433815 0.05980449]
 [0.05239855 0.00947335]
 [0.02726867 0.04421451]
 [0.03317014 0.02049729]]
Epoch 2381: at batch 1: Training dataset Loss=0.058179, Batch Time=0.124
Epoch 2386: at batch 1: Training dataset Loss=0.059588, Batch Time=0.121
Epoch 2391: at batch 1: Training dataset Loss=0.060222, Batch Time=0.124
Epoch 2396: at batch 1: Training dataset Loss=0.061219, Batch Time=0.118
Loss vector (slice for the first 10 images)
[[0.1287923  0.00615881]
 [0.13037022 0.01026846]
 [0.06377667 0.00704834]
 [0.11559251 0.00563777]
 [0.11559251 0.00563777]
 [0.14382386 0.01121071]
 [0.07991688 0.03529547]
 [0.10612487 0.00597975]
 [0.0896104  0.00038103]
 [0.08625182 0.0097902 ]]
Epoch 2401: at batch 1: Training dataset Loss=0.052451, Batch Time=0.123
Epoch 2406: at batch 1: Training dataset Loss=0.040613, Batch Time=0.121
Epoch 2411: at batch 1: Training dataset Loss=0.058986, Batch Time=0.121
Epoch 2416: at batch 1: Training dataset Loss=0.059087, Batch Time=0.121
Loss vector (slice for the first 10 images)
[[0.12183616 0.0335677 ]
 [0.05540162 0.01550427]
 [0.02598135 0.02631298]
 [0.15407682 0.0138791 ]
 [0.00883947 0.01069879]
 [0.02598135 0.02631295]
 [0.04353657 0.01433872]
 [0.13488313 0.04834918]
 [0.02598135 0.02631298]
 [0.02598135 0.02631298]]
Epoch 2421: at batch 1: Training dataset Loss=0.055521, Batch Time=0.119
Epoch 2426: at batch 1: Training dataset Loss=0.059215, Batch Time=0.119
Epoch 2431: at batch 1: Training dataset Loss=0.048656, Batch Time=0.125
Epoch 2436: at batch 1: Training dataset Loss=0.054889, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.0619654  0.01528748]
 [0.00468    0.0266764 ]
 [0.01207128 0.01004438]
 [0.0568901  0.00660285]
 [0.08979516 0.04061451]
 [0.08682449 0.05236667]
 [0.08253715 0.05376171]
 [0.05427995 0.01105168]
 [0.08979516 0.04061451]
 [0.08979516 0.04061451]]
Epoch 2441: at batch 1: Training dataset Loss=0.055760, Batch Time=0.124
Epoch 2446: at batch 1: Training dataset Loss=0.058338, Batch Time=0.120
Epoch 2451: at batch 1: Training dataset Loss=0.055830, Batch Time=0.119
Epoch 2456: at batch 1: Training dataset Loss=0.049206, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.13969422 0.04170065]
 [0.055625   0.00532788]
 [0.13969422 0.04170051]
 [0.08033979 0.03512753]
 [0.08227127 0.05527821]
 [0.01518449 0.00368623]
 [0.15525352 0.01676298]
 [0.01518449 0.00368623]
 [0.1147401  0.00670285]
 [0.08695917 0.05279231]]
Epoch 2461: at batch 1: Training dataset Loss=0.059461, Batch Time=0.124
Epoch 2466: at batch 1: Training dataset Loss=0.042104, Batch Time=0.120
Epoch 2471: at batch 1: Training dataset Loss=0.061255, Batch Time=0.124
Epoch 2476: at batch 1: Training dataset Loss=0.062563, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.09636513 0.045271  ]
 [0.14282589 0.04074754]
 [0.15891013 0.00096218]
 [0.04019422 0.05728678]
 [0.03865242 0.01672621]
 [0.04019422 0.05728678]
 [0.12228418 0.03321789]
 [0.03865242 0.01672621]
 [0.10351096 0.03318479]
 [0.12816533 0.05963192]]
Epoch 2481: at batch 1: Training dataset Loss=0.051600, Batch Time=0.121
Epoch 2486: at batch 1: Training dataset Loss=0.058245, Batch Time=0.123
Epoch 2491: at batch 1: Training dataset Loss=0.051903, Batch Time=0.123
Epoch 2496: at batch 1: Training dataset Loss=0.058667, Batch Time=0.120
Loss vector (slice for the first 10 images)
[[0.07424052 0.01133067]
 [0.12661908 0.00314279]
 [0.15364579 0.05051432]
 [0.10248681 0.01565298]
 [0.07424052 0.01133067]
 [0.1047616  0.00627349]
 [0.14595174 0.04524737]
 [0.15364579 0.05051432]
 [0.01127429 0.03335646]
 [0.1285391  0.05902856]]
Epoch 2501: at batch 1: Training dataset Loss=0.059571, Batch Time=0.126
Epoch 2506: at batch 1: Training dataset Loss=0.052189, Batch Time=0.124
Epoch 2511: at batch 1: Training dataset Loss=0.067874, Batch Time=0.120
Epoch 2516: at batch 1: Training dataset Loss=0.056429, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.15405346 0.04938902]
 [0.08754972 0.05400479]
 [0.14708936 0.0413639 ]
 [0.0056195  0.03011184]
 [0.09173945 0.03099689]
 [0.12678027 0.02714147]
 [0.15196635 0.0344062 ]
 [0.14779708 0.03181521]
 [0.12678027 0.02714147]
 [0.08754972 0.05400479]]
Epoch 2521: at batch 1: Training dataset Loss=0.052409, Batch Time=0.124
Epoch 2526: at batch 1: Training dataset Loss=0.057548, Batch Time=0.122
Epoch 2531: at batch 1: Training dataset Loss=0.063579, Batch Time=0.122
Epoch 2536: at batch 1: Training dataset Loss=0.051062, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.0682587  0.02942944]
 [0.0596964  0.03639818]
 [0.0682587  0.02942944]
 [0.15095578 0.04026219]
 [0.15704339 0.02043035]
 [0.12464846 0.05762241]
 [0.09809375 0.04940443]
 [0.14468764 0.04472165]
 [0.0999362  0.03708898]
 [0.11914819 0.04639757]]
Epoch 2541: at batch 1: Training dataset Loss=0.054147, Batch Time=0.121
Epoch 2546: at batch 1: Training dataset Loss=0.050738, Batch Time=0.123
Epoch 2551: at batch 1: Training dataset Loss=0.057531, Batch Time=0.123
Epoch 2556: at batch 1: Training dataset Loss=0.060172, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.01286976 0.0279295 ]
 [0.07523506 0.01164515]
 [0.13281791 0.02975586]
 [0.08839739 0.03105128]
 [0.10746176 0.03224367]
 [0.10789814 0.03530449]
 [0.07523506 0.01164515]
 [0.02345641 0.05311328]
 [0.04294817 0.02294969]
 [0.07523506 0.01164515]]
Epoch 2561: at batch 1: Training dataset Loss=0.062320, Batch Time=0.125
Epoch 2566: at batch 1: Training dataset Loss=0.059298, Batch Time=0.122
Epoch 2571: at batch 1: Training dataset Loss=0.054892, Batch Time=0.122


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.0239136864881635 0.03683474235291936
0.04646846687396078 0.08080936341420537
0.07823434874025903 0.04335681334932071
0.12024277669607919 0.12734762066728167
0.07823434874025903 0.04335681334932071
0.02158107521686503 0.023419331571152252
0.013608982365188815 0.01743091215330599
0.008056986722692727 0.004038508823648807
0.012779629547833515 0.015477924610495348
0.004724869469701964 0.0026702941305482815
0.0867055676810562 0.08225887622004749
0.09223278930997214 0.14088175111182563
0.030608117494092646 0.042665623139593704
0.02316963811346362 0.028743440767415165
0.0986300657128929 0.06692410648947676
0.0349154551429649 0.07623257249284376
[[0.00730893 0.00604877]
 [0.0055447  0.0055447 ]]
heteroschedastic_sigma_s [0.06346956]
sigma_c 0.00226646244887
[[ 3.64464224e-05 -1.08463749e-04]
 [-3.38442164e-04 -1.57159945e-04]]
[[ 0.00271783 -0.00397092]
 [-0.00236701 -0.00272273]] 

Epoch 2576: at batch 1: Training dataset Loss=0.062283, Batch Time=0.126
		Epoch 2576:  Time = 1282.859, Avg epoch time=1.223, Current epoch Time=1.280

Loss vector (slice for the first 10 images)
[[0.10373933 0.04871188]
 [0.01140273 0.04078208]
 [0.15491261 0.0581482 ]
 [0.10632964 0.04404092]
 [0.07762502 0.02424713]
 [0.03431513 0.02994172]
 [0.01414273 0.0294716 ]
 [0.01871864 0.05603835]
 [0.00033646 0.03560124]
 [0.01140273 0.04078208]]
Epoch 2581: at batch 1: Training dataset Loss=0.059807, Batch Time=0.119
Epoch 2586: at batch 1: Training dataset Loss=0.064022, Batch Time=0.125
Epoch 2591: at batch 1: Training dataset Loss=0.062279, Batch Time=0.124
Epoch 2596: at batch 1: Training dataset Loss=0.060527, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.04399948 0.01581683]
 [0.03570698 0.05427692]
 [0.00268888 0.00498318]
 [0.03570698 0.05427692]
 [0.14933503 0.00739538]
 [0.07280122 0.01249384]
 [0.06514254 0.02788756]
 [0.14327118 0.024919  ]
 [0.12078523 0.0504809 ]
 [0.05562769 0.01710633]]
Epoch 2601: at batch 1: Training dataset Loss=0.057776, Batch Time=0.120
Epoch 2606: at batch 1: Training dataset Loss=0.062770, Batch Time=0.125
Epoch 2611: at batch 1: Training dataset Loss=0.052992, Batch Time=0.126
Epoch 2616: at batch 1: Training dataset Loss=0.047920, Batch Time=0.122
Loss vector (slice for the first 10 images)
[[0.04916687 0.02445952]
 [0.13373871 0.01223947]
 [0.00724464 0.04142707]
 [0.12136622 0.01380034]
 [0.02609342 0.04243892]
 [0.04376001 0.04016848]
 [0.13777803 0.0009975 ]
 [0.03324452 0.00567529]
 [0.00024379 0.01637238]
 [0.04376001 0.04016848]]
Epoch 2621: at batch 1: Training dataset Loss=0.053458, Batch Time=0.121
Epoch 2626: at batch 1: Training dataset Loss=0.054232, Batch Time=0.121
Epoch 2631: at batch 1: Training dataset Loss=0.063733, Batch Time=0.121
Epoch 2636: at batch 1: Training dataset Loss=0.046199, Batch Time=0.122
Loss vector (slice for the first 10 images)
[[0.11647328 0.05178516]
 [0.13880631 0.01507424]
 [0.08542108 0.00314413]
 [0.13880631 0.01507424]
 [0.03251746 0.04471292]
 [0.02225382 0.04118959]
 [0.11647328 0.05178516]
 [0.02722024 0.04077731]
 [0.00723534 0.03746655]
 [0.0644597  0.03634042]]
Epoch 2641: at batch 1: Training dataset Loss=0.063787, Batch Time=0.125
Epoch 2646: at batch 1: Training dataset Loss=0.052980, Batch Time=0.120
Epoch 2651: at batch 1: Training dataset Loss=0.054746, Batch Time=0.121
Epoch 2656: at batch 1: Training dataset Loss=0.048917, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.13844282 0.03973777]
 [0.08470672 0.02527051]
 [0.11572895 0.0168668 ]
 [0.11018734 0.01998426]
 [0.1269065  0.04394316]
 [0.08841178 0.04133579]
 [0.0226957  0.01362888]
 [0.1310462  0.00442699]
 [0.1310462  0.00442699]
 [0.02632028 0.01228495]]
Epoch 2661: at batch 1: Training dataset Loss=0.058721, Batch Time=0.121
Epoch 2666: at batch 1: Training dataset Loss=0.053232, Batch Time=0.124
Epoch 2671: at batch 1: Training dataset Loss=0.048038, Batch Time=0.122
Epoch 2676: at batch 1: Training dataset Loss=0.057669, Batch Time=0.125
Loss vector (slice for the first 10 images)
[[0.11831502 0.01987611]
 [0.0609994  0.0373365 ]
 [0.11831502 0.01987611]
 [0.05779439 0.05022831]
 [0.13890506 0.01081695]
 [0.02073246 0.03933294]
 [0.0609994  0.0373365 ]
 [0.02073246 0.03933294]
 [0.02073246 0.03933294]
 [0.14004524 0.02181447]]
Epoch 2681: at batch 1: Training dataset Loss=0.053716, Batch Time=0.121
Epoch 2686: at batch 1: Training dataset Loss=0.057510, Batch Time=0.121
Epoch 2691: at batch 1: Training dataset Loss=0.051477, Batch Time=0.120
Epoch 2696: at batch 1: Training dataset Loss=0.040608, Batch Time=0.124
Loss vector (slice for the first 10 images)
[[0.08304458 0.03215362]
 [0.02129632 0.00397712]
 [0.08304458 0.03215362]
 [0.00309071 0.03562981]
 [0.00658642 0.01177889]
 [0.08304458 0.03215362]
 [0.00658642 0.01177889]
 [0.10636788 0.047344  ]
 [0.08781526 0.0420402 ]
 [0.02129632 0.00397712]]
Epoch 2701: at batch 1: Training dataset Loss=0.062062, Batch Time=0.138
Epoch 2706: at batch 1: Training dataset Loss=0.047413, Batch Time=0.124
Epoch 2711: at batch 1: Training dataset Loss=0.050277, Batch Time=0.125
Epoch 2716: at batch 1: Training dataset Loss=0.054065, Batch Time=0.123
Loss vector (slice for the first 10 images)
[[0.02676373 0.04368164]
 [0.06353757 0.04282774]
 [0.04629271 0.05898513]
 [0.06353757 0.04282774]
 [0.1353825  0.03588142]
 [0.00701485 0.01795558]
 [0.04052377 0.05164873]
 [0.14328318 0.02810081]
 [0.12901595 0.03499737]
 [0.0621142  0.01313721]]
Epoch 2721: at batch 1: Training dataset Loss=0.065722, Batch Time=0.123
Epoch 2726: at batch 1: Training dataset Loss=0.058110, Batch Time=0.121
Epoch 2731: at batch 1: Training dataset Loss=0.068013, Batch Time=0.125
Epoch 2736: at batch 1: Training dataset Loss=0.054265, Batch Time=0.119
Loss vector (slice for the first 10 images)
[[0.12215288 0.05275781]
 [0.12215288 0.05275781]
 [0.01708543 0.0141353 ]
 [0.14821905 0.04551572]
 [0.03022157 0.05117003]
 [0.05054938 0.04564661]
 [0.05054938 0.04564661]
 [0.08352966 0.01618484]
 [0.04328986 0.01115075]
 [0.09135779 0.04347835]]
Epoch 2741: at batch 1: Training dataset Loss=0.050248, Batch Time=0.124
Epoch 2746: at batch 1: Training dataset Loss=0.044612, Batch Time=0.125
Epoch 2751: at batch 1: Training dataset Loss=0.055207, Batch Time=0.123
Epoch 2756: at batch 1: Training dataset Loss=0.050421, Batch Time=0.122
Loss vector (slice for the first 10 images)
[[0.02564836 0.01705019]
 [0.02517858 0.04348052]
 [0.07891379 0.05653933]
 [0.13461872 0.02196616]
 [0.01055002 0.00969516]
 [0.01499392 0.00872742]
 [0.01499392 0.00872742]
 [0.07891379 0.05653933]
 [0.14500099 0.0592438 ]
 [0.09127    0.02522642]]
Epoch 2761: at batch 1: Training dataset Loss=0.056512, Batch Time=0.124
Epoch 2766: at batch 1: Training dataset Loss=0.058173, Batch Time=0.119
Epoch 2771: at batch 1: Training dataset Loss=0.060170, Batch Time=0.123


(mean,stddev), image[0], hetero noise[0], const noise[0], image[0]
0.0014930250609959117 0.0010310461683860675
0.012319659402662886 0.04069879298428326
0.17885585901626655 0.1628027775871937
0.031436413741506186 0.015495266145050852
0.1356405167505379 0.2134744540872263
0.13141823645775474 0.07253089413959579
0.008061790027452886 0.004230175457741802
0.15241102964705533 0.07747433939486581
0.01792565734740137 0.012476298156156748
0.11516386211596696 0.0993478522431062
0.16281484240704458 0.0943227584694283
0.0028058519992644193 0.001408262897439562
0.16226758714441303 0.07263283618517008
0.14675417510767375 0.05938202761175909
0.06621519547020682 0.03455144015958027
0.01991852003838801 0.014073079224887462
[[0.00283536 0.00220528]
 [0.00056707 0.00220528]]
heteroschedastic_sigma_s [0.06106571]
sigma_c 0.0277423797731
[[ 0.00045777  0.00033163]
 [ 0.0004798  -0.00021113]]
[[ 0.00481119  0.00104227]
 [ 0.01304924 -0.04284487]] 

Epoch 2776: at batch 1: Training dataset Loss=0.042126, Batch Time=0.124
		Epoch 2776:  Time = 1533.548, Avg epoch time=1.230, Current epoch Time=1.276

Loss vector (slice for the first 10 images)
[[0.05258696 0.04996822]
 [0.12370822 0.04723245]
 [0.04256665 0.01130373]
 [0.13632556 0.05543426]
 [0.13089996 0.036018  ]
 [0.04256665 0.01130373]
 [0.11097232 0.01219209]
 [0.15183102 0.03747057]
 [0.0980061  0.05355857]
 [0.13089996 0.036018  ]]
Epoch 2781: at batch 1: Training dataset Loss=0.061159, Batch Time=0.121
Epoch 2786: at batch 1: Training dataset Loss=0.052369, Batch Time=0.119
Epoch 2791: at batch 1: Training dataset Loss=0.048907, Batch Time=0.125
^CTraceback (most recent call last):
  File "train_heteroscedastic_sigma_map.py", line 270, in <module>
    heteroschedastic_noise_s = np.random.normal(0, 1, (BATCH_SIZE,ps,ps,D)) * heteroschedastic_sigma_s
KeyboardInterrupt
(sid2) [ir967@gr013 Learning-to-See-in-the-Dark]$ 