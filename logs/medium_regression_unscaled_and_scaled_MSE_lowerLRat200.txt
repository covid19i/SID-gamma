(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ mkdir gt_Sony_medium_regression_MSE_lowerLRat200_BNeverywhere
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ vi medium_regression_square_loss.py
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ python medium_regression_square_loss.py 




Found 161 images to train with

Training on 161 images only

2020-12-12 14:29:00.761563: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-12 14:29:00.903866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-12 14:29:00.903900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-12 14:29:01.190677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-12 14:29:01.190712: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-12 14:29:01.190729: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-12 14:29:01.190826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./gt_Sony_medium_regression_MSE_lowerLRat200_BNeverywhere/. Hence, will create the folder.
Gamma curve:
Every 8K, 0 to 64K
[[0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]]
Every 125, 64536 to 65536
[[14104 17115 20761 25175 30515 36975 44786 54227]
 [ 1408  2284  3702  5994  9696 15670 25302 40817]
 [  653  1167  2083  3715  6616 11770 20917 37129]]
Every 1, 65526 to 65536
[[64641 64739 64838 64937 65036 65136 65235 65335 65435 65535]
 [63323 63565 63808 64051 64296 64542 64789 65036 65285 65535]
 [62889 63178 63468 63759 64051 64345 64641 64937 65235 65535]]
last epoch of previous run: 0
rawpy read the 0th file at location: ./dataset/Sony/long/00018_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 300.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.87565, 0.00000, 300.00000, 3485919
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.01631, 300.00000, 1
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00164, 300.00000, 1177951
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00201, 100.00000, 803131
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.02266, 300.00000, 3700
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00467, 100.00000, 2882267
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00001, 300.00000, 5540515
rawpy read the 10th file at location: ./dataset/Sony/long/00038_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00894, 100.00000, 2598691
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00228, 300.00000, 4653941
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00017, 250.00000, 6102869
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00613, 100.00000, 1458187
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00012, 300.00000, 3426263
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00116, 250.00000, 4586373
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00146, 250.00000, 909101
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 250.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00327, 100.00000, 3630407
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00002, 300.00000, 6735543
rawpy read the 20th file at location: ./dataset/Sony/long/00072_00_30s.ARW
rawpy read the 30th file at location: ./dataset/Sony/long/00039_00_10s.ARW
rawpy read the 40th file at location: ./dataset/Sony/long/00200_00_10s.ARW
rawpy read the 50th file at location: ./dataset/Sony/long/00024_00_10s.ARW
rawpy read the 60th file at location: ./dataset/Sony/long/00084_00_30s.ARW
rawpy read the 70th file at location: ./dataset/Sony/long/00164_00_30s.ARW
rawpy read the 80th file at location: ./dataset/Sony/long/00128_00_30s.ARW
rawpy read the 90th file at location: ./dataset/Sony/long/00057_00_10s.ARW
rawpy read the 100th file at location: ./dataset/Sony/long/00059_00_10s.ARW
rawpy read the 110th file at location: ./dataset/Sony/long/00026_00_10s.ARW
rawpy read the 120th file at location: ./dataset/Sony/long/00012_00_10s.ARW
rawpy read the 130th file at location: ./dataset/Sony/long/00090_00_30s.ARW
rawpy read the 140th file at location: ./dataset/Sony/long/00114_00_30s.ARW
rawpy read the 150th file at location: ./dataset/Sony/long/00156_00_30s.ARW
rawpy read the 160th file at location: ./dataset/Sony/long/00219_00_10s.ARW
161 images loaded to CPU RAM in Time=41.134 seconds.

Moved images data to a numpy array.
BATCH_SIZE 16 ,final_epoch 4001 ,no_of_batches 10 ,ps 128 ,result_dir ./gt_Sony_medium_regression_MSE_lowerLRat200_BNeverywhere/ ,len(train_ids) 161
Starting Training on index [ 84  29  28  21  76 109  62 160   4 128 143 125  47  42  12 130], dataset index: [222 231  49 143  60 152   4 219  73  71 205 209 160 223  46  90]
Starting Training on gammas [300 250 100 250 300 100 100 250 100 100 250 100 300 300 250 300]
Epoch 0: at batch 1: Training dataset Loss=1.532, Batch Time=1.274
[[2.5       ]
 [0.        ]
 [0.        ]
 [1.01272857]
 [1.5317241 ]
 [2.5       ]
 [0.62981975]
 [1.01272857]
 [0.        ]
 [1.02486002]
 [0.        ]
 [3.38286567]
 [2.24651289]
 [0.        ]
 [1.02486002]
 [0.        ]
 [0.50575036]
 [0.50575036]
 [0.        ]
 [0.        ]
 [0.62981975]
 [1.01272857]
 [0.        ]
 [0.        ]
 [0.        ]
 [2.24651289]
 [0.        ]
 [0.62981975]
 [3.38286567]
 [0.62981975]
 [0.50575036]
 [1.02486002]
 [0.        ]
 [0.50575036]
 [3.38286567]
 [3.38286567]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.        ]
 [1.01272857]
 [0.        ]
 [1.01272857]
 [1.01272857]
 [2.6404047 ]
 [0.        ]
 [2.6404047 ]
 [0.62981975]
 [0.        ]
 [4.76340103]
 [3.38286567]
 [0.62981975]
 [0.50575036]
 [0.        ]
 [1.02486002]
 [0.50575036]
 [0.50575036]
 [2.6404047 ]
 [2.6404047 ]
 [0.62981975]
 [0.        ]
 [2.5       ]
 [0.50575036]
 [0.        ]
 [2.5       ]
 [0.        ]
 [1.02486002]
 [0.        ]
 [3.38286567]
 [1.02486002]
 [0.        ]
 [3.38286567]
 [0.        ]
 [0.        ]
 [3.38286567]
 [2.24651289]
 [1.02486002]
 [0.50575036]
 [0.        ]
 [0.        ]
 [0.        ]
 [1.01272857]
 [1.01272857]
 [2.24651289]
 [1.02486002]
 [0.62981975]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.62981975]
 [0.        ]
 [0.        ]
 [0.50575036]
 [2.5       ]
 [0.        ]
 [0.62981975]
 [2.6404047 ]
 [2.6404047 ]
 [1.02486002]
 [0.        ]
 [0.50575036]
 [0.        ]
 [3.38286567]
 [3.38286567]
 [2.5       ]
 [0.        ]
 [0.        ]
 [1.01272857]
 [1.01272857]
 [1.02486002]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.        ]
 [3.38286567]
 [0.        ]
 [1.02486002]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.        ]
 [3.38286567]
 [1.01272857]
 [1.02486002]
 [2.24651289]
 [0.        ]
 [2.24651289]
 [0.62981975]
 [0.50575036]
 [2.6404047 ]
 [2.6404047 ]
 [0.        ]
 [0.        ]
 [0.        ]
 [1.01272857]
 [0.62981975]
 [0.        ]
 [4.76340103]
 [3.38286567]
 [2.24651289]
 [4.76340103]
 [2.6404047 ]
 [2.24651289]
 [0.        ]
 [0.        ]
 [0.        ]
 [1.02486002]
 [0.        ]
 [0.        ]
 [1.01272857]
 [4.76340103]
 [0.        ]
 [1.01272857]
 [1.01272857]
 [1.02486002]
 [3.38286567]
 [0.        ]
 [0.        ]
 [0.50575036]
 [2.6404047 ]]
Epoch 1: at batch 1: Training dataset Loss=1.752, Batch Time=0.029
Epoch 1: Epoch time = 2.041, Avg epoch time=0.270, Total Time=1.020

[[2.5       ]
 [0.80085415]
 [1.43741155]
 [2.7385478 ]
 [3.70869541]
 [2.5       ]
 [3.70869541]
 [1.01272857]
 [0.64178234]
 [2.7385478 ]
 [1.43741155]
 [1.14322805]
 [1.43741155]
 [0.71418667]
 [1.02486002]
 [0.        ]
 [1.29295099]
 [1.43741155]
 [1.14322805]
 [0.64178234]
 [0.62981975]
 [1.29295099]
 [1.14322805]
 [0.80085415]
 [0.        ]
 [2.7385478 ]
 [1.43741155]
 [1.14322805]
 [1.14322805]
 [0.71418667]
 [0.80085415]
 [0.71418667]
 [0.71418667]
 [0.50575036]
 [3.38286567]
 [3.38286567]
 [0.        ]
 [0.        ]
 [1.5996269 ]
 [0.        ]
 [1.5996269 ]
 [0.61758208]
 [1.43741155]
 [1.01272857]
 [2.6404047 ]
 [1.14322805]
 [2.6404047 ]
 [0.62981975]
 [0.        ]
 [4.76340103]
 [3.38286567]
 [1.43741155]
 [0.80085415]
 [0.        ]
 [1.02486002]
 [1.43741155]
 [3.70869541]
 [2.6404047 ]
 [1.29295099]
 [0.71418667]
 [1.14322805]
 [2.5       ]
 [1.43741155]
 [2.7385478 ]
 [0.80085415]
 [1.14322805]
 [0.64178234]
 [1.14322805]
 [0.61758208]
 [0.71418667]
 [0.        ]
 [3.38286567]
 [0.        ]
 [0.64178234]
 [3.38286567]
 [1.14322805]
 [1.14322805]
 [3.70869541]
 [0.71418667]
 [0.        ]
 [0.71418667]
 [1.01272857]
 [0.71418667]
 [0.64178234]
 [1.29295099]
 [0.64178234]
 [0.        ]
 [0.        ]
 [0.71418667]
 [1.29295099]
 [0.        ]
 [0.80085415]
 [0.50575036]
 [2.5       ]
 [1.5996269 ]
 [0.61758208]
 [0.61758208]
 [1.5996269 ]
 [2.7385478 ]
 [0.        ]
 [0.71418667]
 [1.14322805]
 [0.61758208]
 [1.14322805]
 [2.5       ]
 [0.        ]
 [0.71418667]
 [1.01272857]
 [1.01272857]
 [1.02486002]
 [0.64178234]
 [2.7385478 ]
 [0.        ]
 [1.43741155]
 [1.5996269 ]
 [3.38286567]
 [1.29295099]
 [1.5996269 ]
 [0.        ]
 [0.71418667]
 [3.70869541]
 [0.        ]
 [0.80085415]
 [0.64178234]
 [1.02486002]
 [0.64178234]
 [0.61758208]
 [1.43741155]
 [0.62981975]
 [0.71418667]
 [1.5996269 ]
 [3.70869541]
 [3.70869541]
 [1.14322805]
 [1.29295099]
 [0.80085415]
 [0.62981975]
 [0.80085415]
 [4.76340103]
 [0.61758208]
 [2.24651289]
 [1.43741155]
 [1.29295099]
 [2.24651289]
 [3.70869541]
 [0.        ]
 [0.61758208]
 [1.02486002]
 [2.7385478 ]
 [0.        ]
 [2.7385478 ]
 [0.71418667]
 [0.80085415]
 [1.43741155]
 [1.14322805]
 [1.02486002]
 [0.64178234]
 [0.        ]
 [3.70869541]
 [0.50575036]
 [0.71418667]]
Epoch 2: at batch 1: Training dataset Loss=1.489, Batch Time=0.032
[[1.43211353]
 [0.80085415]
 [5.76848221]
 [0.86157918]
 [5.76848221]
 [1.5       ]
 [1.        ]
 [1.43211353]
 [1.43211353]
 [1.06140351]
 [1.14923978]
 [5.76848221]
 [1.43741155]
 [1.5       ]
 [1.02486002]
 [0.        ]
 [1.29295099]
 [1.        ]
 [1.06140351]
 [0.64178234]
 [0.62981975]
 [1.29295099]
 [1.        ]
 [0.80085415]
 [0.        ]
 [0.86157918]
 [1.14923978]
 [0.75324804]
 [1.14322805]
 [1.43211353]
 [0.80085415]
 [1.06140351]
 [0.75324804]
 [0.50575036]
 [5.76848221]
 [3.38286567]
 [0.        ]
 [0.75324804]
 [1.27789366]
 [0.        ]
 [1.5996269 ]
 [5.76848221]
 [5.76848221]
 [1.01272857]
 [0.75324804]
 [1.14322805]
 [1.14923978]
 [1.        ]
 [1.14923978]
 [1.06140351]
 [0.75324804]
 [1.43741155]
 [0.80085415]
 [0.        ]
 [1.43211353]
 [1.43741155]
 [3.70869541]
 [1.14923978]
 [1.29295099]
 [1.43211353]
 [5.76848221]
 [1.5       ]
 [1.14923978]
 [1.43211353]
 [0.80085415]
 [1.43211353]
 [0.64178234]
 [1.06140351]
 [0.86157918]
 [5.76848221]
 [1.5       ]
 [3.38286567]
 [1.43211353]
 [0.64178234]
 [3.38286567]
 [1.14322805]
 [1.14322805]
 [5.76848221]
 [1.43211353]
 [0.        ]
 [0.71418667]
 [1.14923978]
 [0.86157918]
 [1.5       ]
 [1.29295099]
 [0.64178234]
 [1.27789366]
 [0.86157918]
 [0.71418667]
 [1.29295099]
 [0.        ]
 [0.80085415]
 [1.06140351]
 [1.43211353]
 [1.5996269 ]
 [0.61758208]
 [5.76848221]
 [0.75324804]
 [5.76848221]
 [0.        ]
 [1.5       ]
 [1.14322805]
 [1.06140351]
 [1.14322805]
 [2.5       ]
 [1.14923978]
 [0.71418667]
 [1.14923978]
 [1.01272857]
 [1.14923978]
 [1.14923978]
 [0.86157918]
 [1.06140351]
 [5.76848221]
 [1.5996269 ]
 [1.43211353]
 [1.7858386 ]
 [1.43211353]
 [0.        ]
 [1.06140351]
 [1.        ]
 [0.        ]
 [0.80085415]
 [0.64178234]
 [1.43211353]
 [0.64178234]
 [1.14923978]
 [1.14923978]
 [5.76848221]
 [1.43211353]
 [1.5996269 ]
 [3.70869541]
 [0.75324804]
 [1.27789366]
 [1.27789366]
 [0.80085415]
 [1.        ]
 [0.80085415]
 [1.5       ]
 [1.14923978]
 [1.06140351]
 [1.43211353]
 [1.27789366]
 [2.24651289]
 [1.7858386 ]
 [0.        ]
 [1.5       ]
 [1.        ]
 [2.7385478 ]
 [0.        ]
 [1.27789366]
 [0.75324804]
 [0.80085415]
 [1.43741155]
 [1.14322805]
 [1.06140351]
 [0.64178234]
 [0.        ]
 [1.5       ]
 [0.86157918]
 [0.71418667]]
Epoch 3: at batch 1: Training dataset Loss=1.500, Batch Time=0.031
[[0.80194235]
 [1.        ]
 [1.57308233]
 [0.86157918]
 [1.        ]
 [0.74996531]
 [0.8125    ]
 [2.1875    ]
 [1.43211353]
 [0.67106438]
 [0.8125    ]
 [5.76848221]
 [1.43741155]
 [1.        ]
 [0.80194235]
 [0.67554128]
 [0.67106438]
 [1.        ]
 [0.80194235]
 [0.64178234]
 [0.62981975]
 [1.125     ]
 [1.        ]
 [1.54948843]
 [1.54948843]
 [0.86157918]
 [0.67106438]
 [1.        ]
 [1.14322805]
 [0.8125    ]
 [0.67554128]
 [1.06140351]
 [0.75324804]
 [1.        ]
 [5.76848221]
 [0.67106438]
 [0.80194235]
 [0.75324804]
 [0.67554128]
 [0.        ]
 [0.67554128]
 [1.125     ]
 [2.1875    ]
 [1.        ]
 [1.125     ]
 [1.        ]
 [0.80194235]
 [1.        ]
 [1.        ]
 [1.54948843]
 [0.67106438]
 [0.67554128]
 [0.67554128]
 [0.        ]
 [0.80194235]
 [1.43741155]
 [3.70869541]
 [1.14923978]
 [1.29295099]
 [1.43211353]
 [1.125     ]
 [0.80194235]
 [1.        ]
 [1.43211353]
 [0.8125    ]
 [1.57308233]
 [0.64178234]
 [2.1875    ]
 [0.86157918]
 [1.57308233]
 [1.5       ]
 [0.74996531]
 [1.57308233]
 [0.64178234]
 [1.        ]
 [1.        ]
 [1.14322805]
 [0.80194235]
 [1.125     ]
 [1.57308233]
 [0.71418667]
 [1.14923978]
 [0.86157918]
 [1.        ]
 [2.1875    ]
 [2.1875    ]
 [0.80194235]
 [0.67106438]
 [0.8125    ]
 [1.125     ]
 [1.57308233]
 [1.125     ]
 [0.67106438]
 [1.43211353]
 [1.5996269 ]
 [0.67106438]
 [0.67106438]
 [0.75324804]
 [5.76848221]
 [2.1875    ]
 [1.        ]
 [1.57308233]
 [1.06140351]
 [2.1875    ]
 [2.5       ]
 [0.80194235]
 [1.54948843]
 [1.14923978]
 [1.01272857]
 [1.54948843]
 [0.67106438]
 [0.80194235]
 [1.06140351]
 [5.76848221]
 [1.5996269 ]
 [0.67106438]
 [2.1875    ]
 [0.80194235]
 [0.80194235]
 [1.06140351]
 [1.57308233]
 [2.1875    ]
 [2.1875    ]
 [0.74996531]
 [0.8125    ]
 [0.64178234]
 [0.67106438]
 [1.14923978]
 [5.76848221]
 [1.43211353]
 [1.5996269 ]
 [1.57308233]
 [0.67554128]
 [1.54948843]
 [2.1875    ]
 [0.8125    ]
 [1.57308233]
 [0.80085415]
 [0.67106438]
 [1.        ]
 [1.06140351]
 [0.80194235]
 [1.27789366]
 [2.24651289]
 [1.7858386 ]
 [0.        ]
 [1.5       ]
 [1.        ]
 [1.        ]
 [1.57308233]
 [1.54948843]
 [2.1875    ]
 [1.54948843]
 [1.43741155]
 [0.80194235]
 [1.06140351]
 [0.8125    ]
 [0.        ]
 [1.5       ]
 [0.67106438]
 [0.74996531]]
Epoch 4: at batch 1: Training dataset Loss=1.240, Batch Time=0.033
[[0.62679613]
 [1.49082363]
 [1.57308233]
 [0.86157918]
 [1.5625    ]
 [0.74996531]
 [0.61202288]
 [2.1875    ]
 [1.3125    ]
 [1.49082363]
 [0.62679613]
 [5.76848221]
 [1.3125    ]
 [1.        ]
 [1.5625    ]
 [1.3125    ]
 [0.61202288]
 [1.49082363]
 [0.80194235]
 [1.2226367 ]
 [0.62981975]
 [0.61202288]
 [0.9172765 ]
 [1.3125    ]
 [1.54948843]
 [0.86157918]
 [1.55362678]
 [0.62679613]
 [1.55362678]
 [0.8125    ]
 [1.55362678]
 [1.49082363]
 [0.75324804]
 [1.49082363]
 [5.76848221]
 [0.67106438]
 [1.3125    ]
 [0.9172765 ]
 [0.67554128]
 [0.61202288]
 [0.67554128]
 [1.125     ]
 [2.1875    ]
 [0.62679613]
 [1.55362678]
 [1.55362678]
 [1.5625    ]
 [1.49082363]
 [0.62679613]
 [0.61202288]
 [0.62679613]
 [0.61202288]
 [0.9172765 ]
 [1.3125    ]
 [0.80194235]
 [0.61202288]
 [1.55362678]
 [1.2226367 ]
 [1.2226367 ]
 [1.43211353]
 [1.125     ]
 [0.80194235]
 [1.2226367 ]
 [1.43211353]
 [0.62679613]
 [0.62679613]
 [0.64178234]
 [2.1875    ]
 [0.86157918]
 [1.57308233]
 [0.9172765 ]
 [0.74996531]
 [1.57308233]
 [0.9172765 ]
 [1.3125    ]
 [1.55362678]
 [1.14322805]
 [1.3125    ]
 [1.3125    ]
 [1.49082363]
 [0.71418667]
 [1.14923978]
 [0.86157918]
 [1.        ]
 [2.1875    ]
 [2.1875    ]
 [0.80194235]
 [0.67106438]
 [0.9172765 ]
 [1.125     ]
 [0.61202288]
 [1.125     ]
 [0.9172765 ]
 [0.62679613]
 [0.62679613]
 [1.49082363]
 [0.67106438]
 [0.62679613]
 [0.82202107]
 [1.49082363]
 [1.        ]
 [1.49082363]
 [1.06140351]
 [2.1875    ]
 [2.5       ]
 [1.55362678]
 [1.54948843]
 [1.14923978]
 [1.01272857]
 [1.54948843]
 [0.61202288]
 [1.3125    ]
 [1.55362678]
 [0.9172765 ]
 [0.82202107]
 [0.67106438]
 [2.1875    ]
 [1.3125    ]
 [0.61202288]
 [1.06140351]
 [1.3125    ]
 [2.1875    ]
 [2.1875    ]
 [1.3125    ]
 [0.8125    ]
 [0.61202288]
 [1.55362678]
 [1.3125    ]
 [1.2226367 ]
 [1.43211353]
 [1.5996269 ]
 [1.2226367 ]
 [0.67554128]
 [1.54948843]
 [0.62679613]
 [0.8125    ]
 [0.62679613]
 [1.55362678]
 [0.62679613]
 [1.5625    ]
 [1.49082363]
 [0.62679613]
 [0.9172765 ]
 [1.49082363]
 [1.2226367 ]
 [1.3125    ]
 [1.55362678]
 [1.3125    ]
 [1.49082363]
 [1.57308233]
 [1.54948843]
 [0.62679613]
 [1.54948843]
 [0.9172765 ]
 [0.61202288]
 [1.06140351]
 [0.61202288]
 [0.61202288]
 [1.5       ]
 [0.67106438]
 [0.74996531]]
Epoch 5: at batch 1: Training dataset Loss=1.181, Batch Time=0.032
[[0.62679613]
 [1.49082363]
 [0.54259044]
 [0.86157918]
 [0.93650091]
 [0.54259044]
 [0.84927487]
 [2.1875    ]
 [1.08762014]
 [0.93650091]
 [1.12524807]
 [5.76848221]
 [1.39640152]
 [0.84927487]
 [1.5625    ]
 [1.3125    ]
 [0.61202288]
 [1.18275714]
 [0.80194235]
 [1.08762014]
 [0.62981975]
 [1.18275714]
 [0.9172765 ]
 [1.0812062 ]
 [1.18275714]
 [0.86157918]
 [1.18275714]
 [1.08762014]
 [1.55362678]
 [1.0812062 ]
 [1.10867798]
 [1.39497566]
 [0.75324804]
 [1.49082363]
 [1.18275714]
 [1.0812062 ]
 [1.3125    ]
 [1.18275714]
 [1.39497566]
 [1.08762014]
 [0.67554128]
 [1.0812062 ]
 [2.1875    ]
 [0.62679613]
 [1.0812062 ]
 [1.12524807]
 [1.39497566]
 [1.49082363]
 [1.18275714]
 [0.54259044]
 [0.62679613]
 [1.0812062 ]
 [0.9172765 ]
 [0.93650091]
 [0.80194235]
 [0.54259044]
 [1.39497566]
 [1.08762014]
 [1.2226367 ]
 [1.43211353]
 [1.125     ]
 [1.0812062 ]
 [1.18275714]
 [0.93650091]
 [1.10867798]
 [0.62679613]
 [0.64178234]
 [1.08762014]
 [0.86157918]
 [1.18275714]
 [1.18275714]
 [0.74996531]
 [1.57308233]
 [1.18275714]
 [1.3125    ]
 [0.84927487]
 [1.14322805]
 [1.39497566]
 [1.3125    ]
 [1.39640152]
 [0.71418667]
 [1.0812062 ]
 [1.10867798]
 [1.        ]
 [2.1875    ]
 [1.08762014]
 [0.80194235]
 [0.67106438]
 [1.39640152]
 [1.125     ]
 [0.61202288]
 [1.10867798]
 [0.9172765 ]
 [0.93650091]
 [1.0812062 ]
 [1.39640152]
 [0.67106438]
 [0.62679613]
 [1.39497566]
 [1.49082363]
 [0.54259044]
 [1.39640152]
 [0.54259044]
 [2.1875    ]
 [2.5       ]
 [0.93650091]
 [1.54948843]
 [0.93650091]
 [0.93650091]
 [1.54948843]
 [0.93650091]
 [0.84927487]
 [1.18275714]
 [0.9172765 ]
 [1.18275714]
 [1.39497566]
 [2.1875    ]
 [1.3125    ]
 [1.18275714]
 [1.06140351]
 [0.84927487]
 [0.93650091]
 [1.08762014]
 [1.39497566]
 [0.8125    ]
 [0.54259044]
 [1.08762014]
 [1.3125    ]
 [1.2226367 ]
 [1.08762014]
 [1.5996269 ]
 [1.2226367 ]
 [1.0812062 ]
 [1.54948843]
 [0.54259044]
 [1.18275714]
 [0.62679613]
 [0.84927487]
 [0.62679613]
 [1.39497566]
 [1.0812062 ]
 [1.08762014]
 [1.39640152]
 [1.0812062 ]
 [1.39640152]
 [1.39497566]
 [0.93650091]
 [1.3125    ]
 [1.49082363]
 [1.57308233]
 [1.54948843]
 [1.12524807]
 [0.54259044]
 [0.93650091]
 [1.12524807]
 [0.93650091]
 [1.18275714]
 [0.93650091]
 [1.5       ]
 [0.84927487]
 [0.74996531]]
Epoch 6: at batch 1: Training dataset Loss=1.093, Batch Time=0.030
[[2.18977237]
 [1.49082363]
 [0.54259044]
 [1.08304214]
 [0.93650091]
 [1.03409326]
 [0.93319273]
 [1.37782574]
 [1.08762014]
 [1.40251482]
 [1.08304214]
 [1.22922468]
 [0.7293697 ]
 [0.68905866]
 [0.93319273]
 [1.03409326]
 [0.61202288]
 [1.18275714]
 [1.40251482]
 [1.08762014]
 [1.03409326]
 [1.03409326]
 [0.93319273]
 [1.40251482]
 [1.40251482]
 [0.93319273]
 [1.03409326]
 [0.68905866]
 [1.55362678]
 [1.0812062 ]
 [1.12729776]
 [1.39497566]
 [0.75324804]
 [1.22922468]
 [1.18275714]
 [1.0812062 ]
 [0.93319273]
 [1.08304214]
 [1.39497566]
 [0.68905866]
 [0.67554128]
 [1.0812062 ]
 [2.1875    ]
 [1.03409326]
 [1.0812062 ]
 [2.18977237]
 [1.22922468]
 [1.49082363]
 [0.7293697 ]
 [0.54259044]
 [1.08304214]
 [1.0812062 ]
 [0.7293697 ]
 [0.93650091]
 [2.18977237]
 [1.12729776]
 [1.39497566]
 [0.7293697 ]
 [1.12729776]
 [1.43211353]
 [1.125     ]
 [0.7293697 ]
 [1.37782574]
 [2.18977237]
 [1.10867798]
 [0.62679613]
 [1.37782574]
 [1.22922468]
 [0.93319273]
 [0.7293697 ]
 [1.18275714]
 [1.37782574]
 [0.68905866]
 [2.18977237]
 [2.18977237]
 [2.18977237]
 [1.03409326]
 [1.39497566]
 [1.37782574]
 [1.08304214]
 [0.71418667]
 [1.0812062 ]
 [1.10867798]
 [1.        ]
 [0.93319273]
 [1.22922468]
 [0.93319273]
 [0.67106438]
 [0.93319273]
 [0.7293697 ]
 [0.61202288]
 [1.10867798]
 [1.12729776]
 [0.93650091]
 [1.03409326]
 [1.39640152]
 [1.22922468]
 [0.93319273]
 [0.93319273]
 [1.22922468]
 [0.54259044]
 [0.93319273]
 [0.54259044]
 [0.93319273]
 [2.5       ]
 [1.03409326]
 [1.54948843]
 [0.68905866]
 [1.22922468]
 [1.12729776]
 [1.40251482]
 [0.68905866]
 [1.18275714]
 [0.9172765 ]
 [0.7293697 ]
 [2.18977237]
 [2.1875    ]
 [1.3125    ]
 [1.22922468]
 [0.93319273]
 [0.84927487]
 [0.93650091]
 [1.08762014]
 [1.08304214]
 [0.8125    ]
 [0.68905866]
 [0.7293697 ]
 [1.40251482]
 [1.12729776]
 [1.22922468]
 [1.40251482]
 [0.68905866]
 [1.0812062 ]
 [1.12729776]
 [2.18977237]
 [1.22922468]
 [0.93319273]
 [0.84927487]
 [1.40251482]
 [0.7293697 ]
 [1.0812062 ]
 [1.40251482]
 [1.39640152]
 [1.03409326]
 [0.93319273]
 [1.40251482]
 [1.08304214]
 [1.37782574]
 [1.49082363]
 [2.18977237]
 [1.54948843]
 [1.37782574]
 [0.68905866]
 [0.93650091]
 [2.18977237]
 [0.93650091]
 [1.18275714]
 [0.93650091]
 [1.03409326]
 [1.40251482]
 [0.74996531]]
Epoch 7: at batch 1: Training dataset Loss=1.137, Batch Time=0.023
[[1.69708848]
 [1.49082363]
 [1.03955615]
 [0.97916234]
 [0.93650091]
 [1.01091838]
 [0.97916234]
 [1.3583616 ]
 [0.89200544]
 [1.40251482]
 [1.17461467]
 [1.22922468]
 [0.7293697 ]
 [0.68905866]
 [1.3583616 ]
 [0.91911852]
 [1.69708848]
 [1.03955615]
 [1.17461467]
 [1.08762014]
 [1.17461467]
 [1.03409326]
 [0.93319273]
 [0.87994289]
 [0.89200544]
 [1.03955615]
 [1.03409326]
 [0.68905866]
 [1.55362678]
 [0.91911852]
 [1.12729776]
 [1.01091838]
 [1.03955615]
 [1.69708848]
 [0.91911852]
 [1.0812062 ]
 [1.01091838]
 [1.17461467]
 [1.3583616 ]
 [1.03955615]
 [0.67554128]
 [1.2978369 ]
 [2.1875    ]
 [1.03409326]
 [1.17461467]
 [1.03955615]
 [1.22922468]
 [0.91911852]
 [1.69708848]
 [1.2978369 ]
 [1.2978369 ]
 [1.0812062 ]
 [1.69708848]
 [1.3583616 ]
 [1.69708848]
 [1.69708848]
 [1.3583616 ]
 [0.7293697 ]
 [1.12729776]
 [1.17461467]
 [0.97916234]
 [0.91911852]
 [1.37782574]
 [2.18977237]
 [1.10867798]
 [1.3583616 ]
 [1.03955615]
 [1.22922468]
 [1.01091838]
 [0.7293697 ]
 [0.87994289]
 [0.97916234]
 [0.68905866]
 [2.18977237]
 [2.18977237]
 [2.18977237]
 [0.97916234]
 [1.39497566]
 [1.37782574]
 [0.89200544]
 [0.97916234]
 [1.0812062 ]
 [1.3583616 ]
 [1.01091838]
 [0.97916234]
 [1.2978369 ]
 [0.93319273]
 [1.2978369 ]
 [0.91911852]
 [1.3583616 ]
 [1.17461467]
 [1.3583616 ]
 [1.01091838]
 [1.2978369 ]
 [1.03409326]
 [1.69708848]
 [1.22922468]
 [0.93319273]
 [0.93319273]
 [1.22922468]
 [0.54259044]
 [1.2978369 ]
 [1.3583616 ]
 [1.01091838]
 [1.17461467]
 [1.03409326]
 [1.69708848]
 [0.87994289]
 [1.03955615]
 [1.2978369 ]
 [1.40251482]
 [1.2978369 ]
 [0.91911852]
 [0.9172765 ]
 [1.3583616 ]
 [0.89200544]
 [2.1875    ]
 [1.69708848]
 [1.22922468]
 [0.93319273]
 [1.03955615]
 [0.93650091]
 [1.2978369 ]
 [0.97916234]
 [1.17461467]
 [0.91911852]
 [1.3583616 ]
 [1.40251482]
 [1.17461467]
 [1.01091838]
 [1.17461467]
 [0.87994289]
 [1.2978369 ]
 [0.97916234]
 [0.91911852]
 [1.22922468]
 [0.91911852]
 [1.2978369 ]
 [1.3583616 ]
 [0.7293697 ]
 [1.01091838]
 [1.3583616 ]
 [1.39640152]
 [0.97916234]
 [0.91911852]
 [1.40251482]
 [1.08304214]
 [1.3583616 ]
 [1.49082363]
 [2.18977237]
 [1.54948843]
 [1.37782574]
 [0.68905866]
 [0.91911852]
 [1.01091838]
 [0.93650091]
 [0.87994289]
 [0.93650091]
 [1.3583616 ]
 [1.2978369 ]
 [0.91911852]]
Epoch 8: at batch 1: Training dataset Loss=1.137, Batch Time=0.027
[[1.69708848]
 [1.49082363]
 [1.37541628]
 [0.97916234]
 [0.86512673]
 [0.86512673]
 [0.97916234]
 [1.3583616 ]
 [2.98964   ]
 [0.94909889]
 [0.94909889]
 [1.37541628]
 [2.98964   ]
 [0.68905866]
 [0.86512673]
 [1.1506592 ]
 [1.69708848]
 [1.03955615]
 [1.17461467]
 [4.71592426]
 [1.17461467]
 [1.03409326]
 [0.6065954 ]
 [0.87994289]
 [0.89200544]
 [1.03955615]
 [1.03409326]
 [1.37541628]
 [0.6065954 ]
 [0.91911852]
 [1.1506592 ]
 [0.86037004]
 [1.1506592 ]
 [0.86512673]
 [0.86037004]
 [1.0812062 ]
 [2.4375    ]
 [1.37541628]
 [4.71592426]
 [1.4375    ]
 [1.4375    ]
 [0.6065954 ]
 [4.71592426]
 [1.4375    ]
 [0.86037004]
 [2.4375    ]
 [1.22922468]
 [0.91911852]
 [1.69708848]
 [1.2978369 ]
 [1.2978369 ]
 [1.0812062 ]
 [0.86512673]
 [4.71592426]
 [1.37541628]
 [1.69708848]
 [1.4375    ]
 [2.4375    ]
 [1.37541628]
 [1.17461467]
 [4.71592426]
 [2.4375    ]
 [1.37782574]
 [4.71592426]
 [2.98964   ]
 [2.4375    ]
 [1.03955615]
 [4.71592426]
 [1.01091838]
 [1.1506592 ]
 [0.87994289]
 [0.97916234]
 [0.68905866]
 [2.4375    ]
 [0.94909889]
 [0.6065954 ]
 [2.4375    ]
 [2.4375    ]
 [1.37782574]
 [0.89200544]
 [0.86037004]
 [1.37541628]
 [4.71592426]
 [2.4375    ]
 [0.97916234]
 [0.94909889]
 [0.86512673]
 [0.86512673]
 [2.98964   ]
 [1.1506592 ]
 [1.17461467]
 [0.94909889]
 [1.01091838]
 [2.98964   ]
 [1.1506592 ]
 [1.69708848]
 [1.22922468]
 [1.37541628]
 [0.93319273]
 [1.4375    ]
 [4.71592426]
 [1.2978369 ]
 [1.3583616 ]
 [1.01091838]
 [1.17461467]
 [1.03409326]
 [2.4375    ]
 [1.1506592 ]
 [1.03955615]
 [1.2978369 ]
 [1.1506592 ]
 [4.71592426]
 [0.86037004]
 [2.98964   ]
 [4.71592426]
 [4.71592426]
 [1.1506592 ]
 [1.4375    ]
 [2.98964   ]
 [0.86037004]
 [1.03955615]
 [0.93650091]
 [0.94909889]
 [1.4375    ]
 [1.17461467]
 [0.91911852]
 [0.86037004]
 [2.98964   ]
 [0.86037004]
 [1.37541628]
 [2.98964   ]
 [0.87994289]
 [1.2978369 ]
 [2.4375    ]
 [0.91911852]
 [1.22922468]
 [1.37541628]
 [2.98964   ]
 [1.37541628]
 [4.71592426]
 [0.94909889]
 [0.6065954 ]
 [1.37541628]
 [2.4375    ]
 [2.4375    ]
 [1.40251482]
 [1.1506592 ]
 [1.3583616 ]
 [1.4375    ]
 [0.86037004]
 [1.54948843]
 [2.98964   ]
 [0.68905866]
 [0.91911852]
 [2.4375    ]
 [0.6065954 ]
 [1.1506592 ]
 [1.1506592 ]
 [1.37541628]
 [4.71592426]
 [0.91911852]]
Epoch 9: at batch 1: Training dataset Loss=1.571, Batch Time=0.031
[[1.10187364]
 [2.1875    ]
 [1.51670742]
 [0.97916234]
 [2.1875    ]
 [0.86512673]
 [0.60507923]
 [1.3583616 ]
 [0.73438454]
 [1.51670742]
 [0.94909889]
 [0.63926744]
 [2.98964   ]
 [0.7910828 ]
 [0.63926744]
 [1.51670742]
 [0.55365252]
 [1.03955615]
 [0.87840742]
 [4.71592426]
 [1.10187364]
 [2.1875    ]
 [0.87840742]
 [0.7910828 ]
 [1.51670742]
 [1.76483774]
 [1.10187364]
 [1.37541628]
 [0.55365252]
 [0.91911852]
 [1.51670742]
 [0.86037004]
 [2.1875    ]
 [0.7910828 ]
 [0.87840742]
 [1.0812062 ]
 [0.7910828 ]
 [1.37541628]
 [1.51670742]
 [0.63926744]
 [0.87840742]
 [0.6065954 ]
 [1.76483774]
 [0.55365252]
 [0.86037004]
 [2.4375    ]
 [1.22922468]
 [2.1875    ]
 [0.60507923]
 [0.60507923]
 [1.2978369 ]
 [0.63926744]
 [0.60507923]
 [4.71592426]
 [0.55365252]
 [0.7910828 ]
 [2.1875    ]
 [0.60507923]
 [1.37541628]
 [0.55365252]
 [1.10187364]
 [2.4375    ]
 [1.37782574]
 [4.71592426]
 [0.7910828 ]
 [2.4375    ]
 [1.51670742]
 [0.73438454]
 [0.7910828 ]
 [0.63926744]
 [1.10187364]
 [0.87840742]
 [0.68905866]
 [2.1875    ]
 [0.7910828 ]
 [1.51670742]
 [0.87840742]
 [2.1875    ]
 [0.7910828 ]
 [0.87840742]
 [2.1875    ]
 [1.37541628]
 [4.71592426]
 [0.55365252]
 [1.76483774]
 [0.94909889]
 [0.86512673]
 [0.87840742]
 [2.98964   ]
 [1.1506592 ]
 [1.10187364]
 [0.60507923]
 [1.01091838]
 [0.60507923]
 [1.1506592 ]
 [1.76483774]
 [1.76483774]
 [0.60507923]
 [0.93319273]
 [1.4375    ]
 [0.55365252]
 [1.2978369 ]
 [0.63926744]
 [1.01091838]
 [0.55365252]
 [0.55365252]
 [0.60507923]
 [0.73438454]
 [0.60507923]
 [1.2978369 ]
 [0.7910828 ]
 [0.63926744]
 [1.10187364]
 [2.1875    ]
 [0.7910828 ]
 [1.51670742]
 [1.10187364]
 [0.87840742]
 [2.98964   ]
 [0.7910828 ]
 [0.7910828 ]
 [1.76483774]
 [0.55365252]
 [1.4375    ]
 [1.17461467]
 [0.91911852]
 [1.51670742]
 [2.98964   ]
 [0.55365252]
 [1.37541628]
 [2.98964   ]
 [0.7910828 ]
 [1.2978369 ]
 [0.60507923]
 [0.73438454]
 [0.87840742]
 [1.37541628]
 [2.98964   ]
 [1.37541628]
 [0.60507923]
 [0.94909889]
 [1.10187364]
 [2.1875    ]
 [2.4375    ]
 [1.76483774]
 [1.51670742]
 [1.76483774]
 [1.51670742]
 [1.4375    ]
 [0.55365252]
 [0.60507923]
 [0.7910828 ]
 [0.73438454]
 [0.7910828 ]
 [2.4375    ]
 [2.1875    ]
 [1.1506592 ]
 [1.51670742]
 [0.55365252]
 [0.73438454]
 [0.55365252]]
Epoch 11: at batch 1: Training dataset Loss=1.288, Batch Time=0.029
Epoch 21: at batch 1: Training dataset Loss=1.439, Batch Time=0.029
Epoch 31: at batch 1: Training dataset Loss=1.079, Batch Time=0.026
Epoch 41: at batch 1: Training dataset Loss=1.040, Batch Time=0.029
Epoch 51: at batch 1: Training dataset Loss=1.184, Batch Time=0.029
Epoch 61: at batch 1: Training dataset Loss=1.129, Batch Time=0.027
Epoch 71: at batch 1: Training dataset Loss=1.131, Batch Time=0.032
Epoch 81: at batch 1: Training dataset Loss=1.379, Batch Time=0.032
Epoch 91: at batch 1: Training dataset Loss=1.142, Batch Time=0.030
Epoch 101: at batch 1: Training dataset Loss=1.191, Batch Time=0.031
Epoch 101: Epoch time = 42.815, Avg epoch time=0.278, Total Time=0.420

[[0.81503522]
 [2.125     ]
 [1.20849538]
 [2.5       ]
 [1.20849538]
 [1.61098456]
 [1.61098456]
 [1.20849538]
 [3.87055111]
 [0.5343883 ]
 [1.61098456]
 [0.81503522]
 [0.72051656]
 [3.87055111]
 [0.65607393]
 [1.38643193]
 [1.61098456]
 [1.375     ]
 [1.38643193]
 [1.38643193]
 [0.84110129]
 [3.87055111]
 [1.38643193]
 [1.20849538]
 [1.375     ]
 [0.88819808]
 [3.87055111]
 [0.65607393]
 [0.88819808]
 [1.52043653]
 [0.88819808]
 [1.20849538]
 [1.22311664]
 [1.15676355]
 [2.125     ]
 [1.22311664]
 [0.81503522]
 [0.88819808]
 [0.84110129]
 [0.81503522]
 [1.20849538]
 [1.375     ]
 [2.5       ]
 [2.125     ]
 [1.22311664]
 [0.84110129]
 [1.20849538]
 [1.61098456]
 [1.24256825]
 [1.1875    ]
 [0.9642421 ]
 [1.15676355]
 [0.88819808]
 [0.9642421 ]
 [1.15676355]
 [1.1875    ]
 [0.88819808]
 [0.88819808]
 [2.5       ]
 [2.5       ]
 [1.15676355]
 [0.88819808]
 [1.375     ]
 [0.78740108]
 [1.38643193]
 [1.1875    ]
 [0.72051656]
 [1.22311664]
 [0.72051656]
 [1.375     ]
 [0.65607393]
 [0.9207052 ]
 [1.15676355]
 [1.61098456]
 [3.87055111]
 [1.20849538]
 [1.38643193]
 [0.81503522]
 [1.38643193]
 [1.15676355]
 [1.38643193]
 [1.44792163]
 [0.88819808]
 [0.79908729]
 [3.87055111]
 [1.44792163]
 [0.9642421 ]
 [0.5343883 ]
 [1.375     ]
 [1.61098456]
 [1.375     ]
 [1.15676355]
 [0.88819808]
 [1.22311664]
 [1.61098456]
 [0.65607393]
 [1.15676355]
 [1.61098456]
 [1.47235966]
 [1.61098456]
 [2.125     ]
 [1.24256825]
 [3.87055111]
 [1.625     ]
 [1.375     ]
 [1.38643193]
 [1.22311664]
 [0.9642421 ]
 [1.61098456]
 [1.375     ]
 [1.44792163]
 [2.5       ]
 [1.52043653]
 [1.24256825]
 [1.38643193]
 [0.95662105]
 [1.37290883]
 [1.61098456]
 [3.87055111]
 [1.38643193]
 [1.15676355]
 [1.38643193]
 [1.20849538]
 [1.44792163]
 [0.81503522]
 [1.38643193]
 [0.84110129]
 [1.38643193]
 [1.20849538]
 [1.22311664]
 [1.20849538]
 [1.61098456]
 [1.47235966]
 [3.87055111]
 [1.4375    ]
 [0.88819808]
 [0.65607393]
 [2.125     ]
 [1.4375    ]
 [1.20849538]
 [1.52043653]
 [1.375     ]
 [2.5       ]
 [1.15676355]
 [0.5343883 ]
 [1.61098456]
 [0.43898818]
 [0.43898818]
 [0.5343883 ]
 [1.24256825]
 [0.88819808]
 [1.375     ]
 [1.375     ]
 [1.1875    ]
 [2.125     ]
 [1.38643193]
 [0.87787479]
 [0.72018868]
 [3.87055111]
 [1.38643193]
 [0.9642421 ]]
Epoch 111: at batch 1: Training dataset Loss=1.316, Batch Time=0.033
Epoch 121: at batch 1: Training dataset Loss=0.947, Batch Time=0.033
Epoch 131: at batch 1: Training dataset Loss=1.043, Batch Time=0.029
Epoch 141: at batch 1: Training dataset Loss=1.004, Batch Time=0.023
Epoch 151: at batch 1: Training dataset Loss=1.034, Batch Time=0.035
Epoch 161: at batch 1: Training dataset Loss=1.223, Batch Time=0.033
Epoch 171: at batch 1: Training dataset Loss=1.076, Batch Time=0.035
Epoch 181: at batch 1: Training dataset Loss=0.971, Batch Time=0.034
Epoch 191: at batch 1: Training dataset Loss=1.029, Batch Time=0.026
Epoch 201: at batch 1: Training dataset Loss=0.884, Batch Time=0.024
Epoch 201: Epoch time = 83.528, Avg epoch time=0.267, Total Time=0.414

[[0.84609795]
 [0.88114411]
 [0.65873981]
 [4.64930677]
 [1.27208459]
 [4.64930677]
 [0.65873981]
 [1.36451578]
 [0.70404834]
 [1.51148784]
 [0.48893055]
 [0.73798859]
 [0.73798859]
 [1.36451578]
 [4.64930677]
 [0.66526508]
 [0.88114411]
 [1.27208459]
 [0.84609795]
 [1.07022941]
 [4.64930677]
 [1.51148784]
 [1.03186274]
 [0.96291482]
 [0.96291482]
 [1.27208459]
 [0.94223142]
 [0.65873981]
 [1.51148784]
 [0.66526508]
 [0.77614212]
 [0.29700163]
 [0.94736487]
 [0.81493485]
 [0.88114411]
 [0.94355422]
 [0.70404834]
 [0.94223142]
 [0.73798859]
 [0.77507347]
 [1.46834791]
 [0.77614212]
 [4.64930677]
 [0.70404834]
 [0.65873981]
 [1.27208459]
 [1.27208459]
 [0.63272166]
 [0.65873981]
 [1.27208459]
 [0.84609795]
 [0.76631498]
 [0.73798859]
 [0.6966337 ]
 [1.27208459]
 [0.65873981]
 [1.51148784]
 [0.94355422]
 [0.77614212]
 [0.84609795]
 [0.89408976]
 [1.27208459]
 [1.09377813]
 [4.64930677]
 [0.81493485]
 [0.88114411]
 [0.73798859]
 [0.48893055]
 [0.73798859]
 [0.84609795]
 [1.07022941]
 [0.58590305]
 [1.27208459]
 [1.09377813]
 [4.64930677]
 [0.94736487]
 [0.73798859]
 [1.07022941]
 [0.94223142]
 [0.77507347]
 [0.78588253]
 [0.73798859]
 [1.02946126]
 [0.29700163]
 [0.48893055]
 [0.58590305]
 [1.27208459]
 [0.48893055]
 [0.73798859]
 [1.36451578]
 [0.6966337 ]
 [0.65873981]
 [0.94223142]
 [0.73798859]
 [0.66526508]
 [0.77614212]
 [1.02946126]
 [1.27208459]
 [4.64930677]
 [1.27208459]
 [0.73798859]
 [0.84609795]
 [0.60248125]
 [0.58590305]
 [0.77614212]
 [0.81493485]
 [0.84609795]
 [0.77614212]
 [0.65873981]
 [0.84609795]
 [0.65873981]
 [0.48893055]
 [0.29700163]
 [1.46834791]
 [0.73798859]
 [0.70404834]
 [0.94355422]
 [0.73798859]
 [1.07022941]
 [4.64930677]
 [0.88114411]
 [2.03654313]
 [0.70404834]
 [0.84609795]
 [1.27208459]
 [0.94355422]
 [0.84609795]
 [1.44845033]
 [0.94355422]
 [0.62936854]
 [4.64930677]
 [1.02946126]
 [0.73798859]
 [1.36451578]
 [0.48893055]
 [0.65873981]
 [1.07022941]
 [0.58590305]
 [0.77222192]
 [0.48893055]
 [0.84609795]
 [0.73798859]
 [1.27208459]
 [0.94223142]
 [0.90123487]
 [1.02946126]
 [0.56988913]
 [0.94223142]
 [0.73798859]
 [0.65873981]
 [0.66526508]
 [1.02983236]
 [0.58590305]
 [1.27208459]
 [1.27208459]
 [1.02983236]
 [0.94223142]
 [1.07022941]
 [1.02983236]
 [1.46834791]
 [0.72485948]]
Epoch 211: at batch 1: Training dataset Loss=0.988, Batch Time=0.024
Epoch 221: at batch 1: Training dataset Loss=0.870, Batch Time=0.028
Epoch 231: at batch 1: Training dataset Loss=0.897, Batch Time=0.026
Epoch 241: at batch 1: Training dataset Loss=0.873, Batch Time=0.033
Epoch 251: at batch 1: Training dataset Loss=0.881, Batch Time=0.035
Epoch 261: at batch 1: Training dataset Loss=0.871, Batch Time=0.025
Epoch 271: at batch 1: Training dataset Loss=0.903, Batch Time=0.034
Epoch 281: at batch 1: Training dataset Loss=1.095, Batch Time=0.027
Epoch 291: at batch 1: Training dataset Loss=0.878, Batch Time=0.026
Epoch 301: at batch 1: Training dataset Loss=1.004, Batch Time=0.024
Epoch 301: Epoch time = 124.421, Avg epoch time=0.272, Total Time=0.412

[[1.16819096]
 [0.65614492]
 [1.40667486]
 [1.17294192]
 [1.16819096]
 [1.16819096]
 [1.40667486]
 [1.04909444]
 [0.99870157]
 [0.6423682 ]
 [0.62438816]
 [1.24392724]
 [0.87360078]
 [1.6129719 ]
 [0.65614492]
 [0.87360078]
 [1.6129719 ]
 [0.79993469]
 [1.54841208]
 [1.54841208]
 [0.91995251]
 [0.91995251]
 [1.6129719 ]
 [0.65614492]
 [0.62438816]
 [0.87360078]
 [0.91995251]
 [0.74957287]
 [0.6557216 ]
 [0.99870157]
 [1.56606913]
 [1.56606913]
 [0.99847996]
 [1.17294192]
 [0.87550509]
 [1.16819096]
 [1.00288653]
 [0.8416748 ]
 [1.54841208]
 [1.16819096]
 [0.72402453]
 [1.17294192]
 [1.56606913]
 [0.6557216 ]
 [0.91995251]
 [0.80053645]
 [1.16819096]
 [1.54841208]
 [0.65614492]
 [0.91995251]
 [1.97616196]
 [1.16819096]
 [0.62438816]
 [0.87360078]
 [0.99847996]
 [1.54841208]
 [1.16819096]
 [0.65614492]
 [1.40667486]
 [0.65614492]
 [0.62438816]
 [1.16819096]
 [1.40667486]
 [1.56606913]
 [0.87460178]
 [0.99847996]
 [1.04909444]
 [1.54841208]
 [1.56606913]
 [1.17294192]
 [1.04909444]
 [0.6423682 ]
 [0.65614492]
 [0.6423682 ]
 [1.17294192]
 [1.6129719 ]
 [1.17294192]
 [0.80404723]
 [0.99847996]
 [1.97616196]
 [0.62438816]
 [0.79993469]
 [0.91995251]
 [0.87360078]
 [0.6423682 ]
 [1.54841208]
 [0.6423682 ]
 [0.80404723]
 [0.80404723]
 [1.54841208]
 [0.62438816]
 [0.65614492]
 [0.91995251]
 [0.99870157]
 [1.54841208]
 [1.56606913]
 [1.56606913]
 [0.91995251]
 [1.54841208]
 [0.74957287]
 [1.16819096]
 [1.16819096]
 [1.97616196]
 [1.54841208]
 [0.62438816]
 [0.58732796]
 [1.17294192]
 [0.62438816]
 [0.91995251]
 [0.87360078]
 [0.55221277]
 [1.40667486]
 [0.99847996]
 [0.91995251]
 [0.88480192]
 [0.62438816]
 [0.55221277]
 [1.40667486]
 [1.16819096]
 [0.80404723]
 [1.40667486]
 [0.6423682 ]
 [0.62438816]
 [0.80357742]
 [0.6557216 ]
 [0.80404723]
 [0.80404723]
 [1.16819096]
 [1.40667486]
 [0.62438816]
 [0.6423682 ]
 [1.16819096]
 [1.54841208]
 [1.6129719 ]
 [1.12635422]
 [1.54841208]
 [1.6129719 ]
 [0.74957287]
 [1.6129719 ]
 [0.88480192]
 [1.16819096]
 [0.7496109 ]
 [0.80404723]
 [0.65614492]
 [1.17294192]
 [0.91995251]
 [1.04909444]
 [0.6423682 ]
 [1.04909444]
 [0.7496109 ]
 [0.6423682 ]
 [0.95158422]
 [0.91995251]
 [0.80404723]
 [0.62438816]
 [0.71172166]
 [1.16819096]
 [0.99870157]
 [0.91995251]
 [0.87360078]
 [0.87550509]]
Epoch 311: at batch 1: Training dataset Loss=1.013, Batch Time=0.026
Epoch 321: at batch 1: Training dataset Loss=0.945, Batch Time=0.027
Epoch 331: at batch 1: Training dataset Loss=1.033, Batch Time=0.027
Epoch 341: at batch 1: Training dataset Loss=1.083, Batch Time=0.036
Epoch 351: at batch 1: Training dataset Loss=0.864, Batch Time=0.029
Epoch 361: at batch 1: Training dataset Loss=0.974, Batch Time=0.029
Epoch 371: at batch 1: Training dataset Loss=0.870, Batch Time=0.033
Epoch 381: at batch 1: Training dataset Loss=0.760, Batch Time=0.035
Epoch 391: at batch 1: Training dataset Loss=0.905, Batch Time=0.034
Epoch 401: at batch 1: Training dataset Loss=0.780, Batch Time=0.027
Epoch 401: Epoch time = 165.467, Avg epoch time=0.277, Total Time=0.412

[[0.70079744]
 [0.75498712]
 [1.14580977]
 [0.72233784]
 [1.02118826]
 [0.81460518]
 [1.02528179]
 [1.02528179]
 [0.81460518]
 [1.02528179]
 [1.02118826]
 [0.75498712]
 [1.02528179]
 [0.84150422]
 [1.02118826]
 [0.817963  ]
 [0.81460518]
 [0.75498712]
 [1.02118826]
 [0.80784416]
 [0.71544749]
 [0.81460518]
 [0.70079744]
 [0.79250616]
 [0.635997  ]
 [1.02118826]
 [1.04003692]
 [1.04003692]
 [1.14580977]
 [0.67123055]
 [0.72226322]
 [1.02118826]
 [0.68309212]
 [0.81460518]
 [0.69467747]
 [0.55453217]
 [0.68309212]
 [0.79250616]
 [0.72226322]
 [1.20676184]
 [0.81592244]
 [0.71544749]
 [1.02528179]
 [0.68309212]
 [1.02528179]
 [0.68340319]
 [1.20676184]
 [1.14580977]
 [0.68309212]
 [0.70079744]
 [0.84150422]
 [0.84150422]
 [0.71544749]
 [0.75498712]
 [0.72226322]
 [0.71544749]
 [1.02118826]
 [1.20676184]
 [1.31012261]
 [0.72226322]
 [0.84150422]
 [0.68309212]
 [0.80784416]
 [0.81460518]
 [1.02118826]
 [0.85162544]
 [1.20676184]
 [0.70079744]
 [0.72226322]
 [1.30735135]
 [0.63294232]
 [0.72226322]
 [0.780209  ]
 [1.30735135]
 [0.68340319]
 [0.9436484 ]
 [0.68340319]
 [0.75498712]
 [1.04003692]
 [0.81460518]
 [0.70079744]
 [0.68309212]
 [0.84150422]
 [0.68309212]
 [1.14580977]
 [1.02118826]
 [1.14580977]
 [0.70079744]
 [0.68340319]
 [0.85162544]
 [0.80784416]
 [1.02118826]
 [1.04133344]
 [0.60182118]
 [0.70079744]
 [0.79250616]
 [1.02118826]
 [0.45695144]
 [1.04003692]
 [1.14580977]
 [0.635997  ]
 [1.04003692]
 [1.02528179]
 [0.81460518]
 [0.780209  ]
 [1.08304882]
 [1.02528179]
 [1.02528179]
 [0.81460518]
 [0.75498712]
 [1.02528179]
 [0.80784416]
 [1.20676184]
 [0.817963  ]
 [0.72226322]
 [0.84150422]
 [0.81460518]
 [0.71544749]
 [1.02528179]
 [1.02528179]
 [1.04003692]
 [1.02118826]
 [0.71544749]
 [1.08304882]
 [0.70079744]
 [1.20676184]
 [1.20676184]
 [1.04003692]
 [1.02118826]
 [1.20676184]
 [1.04003692]
 [1.02528179]
 [0.68309212]
 [1.04003692]
 [0.72226322]
 [0.49191529]
 [0.68309212]
 [1.02528179]
 [0.68309212]
 [0.70079744]
 [1.02528179]
 [0.68309212]
 [1.02118826]
 [1.02118826]
 [1.02118826]
 [0.81460518]
 [0.69467747]
 [0.780209  ]
 [0.80784416]
 [1.05060506]
 [1.20676184]
 [1.20676184]
 [0.75498712]
 [0.635997  ]
 [1.04003692]
 [0.72226322]
 [1.14580977]
 [1.30735135]
 [0.84150422]
 [0.68309212]
 [0.780209  ]]
Epoch 411: at batch 1: Training dataset Loss=0.868, Batch Time=0.029
^CTraceback (most recent call last):
  File "medium_regression_square_loss.py", line 312, in <module>
    input_patch = np.minimum(input_patch, 1.0)
KeyboardInterrupt
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ vi medium_regression_square_loss.py
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ python medium_regression_square_loss.py 




Found 161 images to train with

Training on 161 images only

2020-12-12 14:34:48.344166: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-12 14:34:48.481064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-12 14:34:48.481102: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-12 14:34:48.767159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-12 14:34:48.767193: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-12 14:34:48.767216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-12 14:34:48.767317: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)

Loaded ./gt_Sony_medium_regression_MSE_lowerLRat200_BNeverywhere/model.ckpt
Gamma curve:
Every 8K, 0 to 64K
[[0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]]
Every 125, 64536 to 65536
[[14104 17115 20761 25175 30515 36975 44786 54227]
 [ 1408  2284  3702  5994  9696 15670 25302 40817]
 [  653  1167  2083  3715  6616 11770 20917 37129]]
Every 1, 65526 to 65536
[[64641 64739 64838 64937 65036 65136 65235 65335 65435 65535]
 [63323 63565 63808 64051 64296 64542 64789 65036 65285 65535]
 [62889 63178 63468 63759 64051 64345 64641 64937 65235 65535]]
last epoch of previous run: 410
rawpy read the 0th file at location: ./dataset/Sony/long/00018_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 250.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.89525, 0.00000, 250.00000, 3485919
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.01631, 300.00000, 1
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00164, 300.00000, 1177951
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00187, 300.00000, 803131
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.02266, 300.00000, 3700
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00467, 100.00000, 2882267
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00001, 100.00000, 5540515
rawpy read the 10th file at location: ./dataset/Sony/long/00038_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00886, 300.00000, 2598691
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00228, 300.00000, 4653941
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00016, 300.00000, 6102869
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00611, 250.00000, 1458187
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00012, 250.00000, 3426263
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00117, 100.00000, 4586373
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00155, 100.00000, 909101
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 250.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00327, 100.00000, 3630407
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00003, 100.00000, 6735543
rawpy read the 20th file at location: ./dataset/Sony/long/00072_00_30s.ARW
^CTraceback (most recent call last):
  File "medium_regression_square_loss.py", line 242, in <module>
    input_image = np.multiply(input_image, 1./65535.)
KeyboardInterrupt
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ vi medium_regression_square_loss.py
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ python medium_regression_square_loss.py 




Found 161 images to train with

Training on 161 images only

2020-12-12 14:36:19.438884: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-12 14:36:19.575763: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-12 14:36:19.575796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-12 14:36:19.862313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-12 14:36:19.862347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-12 14:36:19.862363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-12 14:36:19.862463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)

Loaded ./gt_Sony_medium_regression_MSE_lowerLRat200_BNeverywhere/model.ckpt
Gamma curve:
Every 8K, 0 to 64K
[[0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]]
Every 125, 64536 to 65536
[[14104 17115 20761 25175 30515 36975 44786 54227]
 [ 1408  2284  3702  5994  9696 15670 25302 40817]
 [  653  1167  2083  3715  6616 11770 20917 37129]]
Every 1, 65526 to 65536
[[64641 64739 64838 64937 65036 65136 65235 65335 65435 65535]
 [63323 63565 63808 64051 64296 64542 64789 65036 65285 65535]
 [62889 63178 63468 63759 64051 64345 64641 64937 65235 65535]]
last epoch of previous run: 410
rawpy read the 0th file at location: ./dataset/Sony/long/00018_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 250.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.95669, 0.00000, 100.00000, 3485919
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.01631, 300.00000, 1
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 300.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00164, 300.00000, 1177951
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00187, 300.00000, 803131
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.02266, 300.00000, 3700
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00467, 100.00000, 2882267
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00001, 250.00000, 5540515
rawpy read the 10th file at location: ./dataset/Sony/long/00038_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00887, 250.00000, 2598691
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00228, 300.00000, 4653941
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00017, 250.00000, 6102869
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00611, 300.00000, 1458187
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00012, 250.00000, 3426263
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00117, 100.00000, 4586373
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00155, 100.00000, 909101
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00307, 250.00000, 3630407
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00002, 250.00000, 6735543
rawpy read the 20th file at location: ./dataset/Sony/long/00072_00_30s.ARW
rawpy read the 30th file at location: ./dataset/Sony/long/00039_00_10s.ARW
rawpy read the 40th file at location: ./dataset/Sony/long/00200_00_10s.ARW
rawpy read the 50th file at location: ./dataset/Sony/long/00024_00_10s.ARW
rawpy read the 60th file at location: ./dataset/Sony/long/00084_00_30s.ARW
rawpy read the 70th file at location: ./dataset/Sony/long/00164_00_30s.ARW
rawpy read the 80th file at location: ./dataset/Sony/long/00128_00_30s.ARW
rawpy read the 90th file at location: ./dataset/Sony/long/00057_00_10s.ARW
^CTraceback (most recent call last):
  File "medium_regression_square_loss.py", line 231, in <module>
    input_image = pack_raw(gt_raw)
  File "medium_regression_square_loss.py", line 95, in pack_raw
    im = np.maximum(im - 512, 0) / (16383 - 512)  # subtract the black level
KeyboardInterrupt
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ vi medium_regression_square_loss.py
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ python medium_regression_square_loss.py 




Found 161 images to train with

Training on 161 images only

2020-12-12 14:37:05.510220: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-12 14:37:05.650274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-12 14:37:05.650307: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-12 14:37:05.937524: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-12 14:37:05.937560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-12 14:37:05.937589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-12 14:37:05.937693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)

Loaded ./gt_Sony_medium_regression_MSE_lowerLRat200_BNeverywhere/model.ckpt
Gamma curve:
Every 8K, 0 to 64K
[[0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]]
Every 125, 64536 to 65536
[[14104 17115 20761 25175 30515 36975 44786 54227]
 [ 1408  2284  3702  5994  9696 15670 25302 40817]
 [  653  1167  2083  3715  6616 11770 20917 37129]]
Every 1, 65526 to 65536
[[64641 64739 64838 64937 65036 65136 65235 65335 65435 65535]
 [63323 63565 63808 64051 64296 64542 64789 65036 65285 65535]
 [62889 63178 63468 63759 64051 64345 64641 64937 65235 65535]]
last epoch of previous run: 410
rawpy read the 0th file at location: ./dataset/Sony/long/00018_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.87565, 0.00000, 300.00000, 3485919
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.01633, 250.00000, 1
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 300.00000, 0
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00164, 300.00000, 1177951
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00190, 250.00000, 803131
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.02465, 100.00000, 3700
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00427, 250.00000, 2882267
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00001, 250.00000, 5540515
rawpy read the 10th file at location: ./dataset/Sony/long/00038_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00886, 300.00000, 2598691
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00248, 100.00000, 4653941
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00017, 100.00000, 6102869
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00611, 300.00000, 1458187
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00012, 250.00000, 3426263
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00116, 250.00000, 4586373
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00146, 250.00000, 909101
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 300.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00307, 250.00000, 3630407
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00002, 250.00000, 6735543
rawpy read the 20th file at location: ./dataset/Sony/long/00072_00_30s.ARW
rawpy read the 30th file at location: ./dataset/Sony/long/00039_00_10s.ARW
rawpy read the 40th file at location: ./dataset/Sony/long/00200_00_10s.ARW
rawpy read the 50th file at location: ./dataset/Sony/long/00024_00_10s.ARW
rawpy read the 60th file at location: ./dataset/Sony/long/00084_00_30s.ARW
rawpy read the 70th file at location: ./dataset/Sony/long/00164_00_30s.ARW
rawpy read the 80th file at location: ./dataset/Sony/long/00128_00_30s.ARW
rawpy read the 90th file at location: ./dataset/Sony/long/00057_00_10s.ARW
rawpy read the 100th file at location: ./dataset/Sony/long/00059_00_10s.ARW
rawpy read the 110th file at location: ./dataset/Sony/long/00026_00_10s.ARW
rawpy read the 120th file at location: ./dataset/Sony/long/00012_00_10s.ARW
rawpy read the 130th file at location: ./dataset/Sony/long/00090_00_30s.ARW
rawpy read the 140th file at location: ./dataset/Sony/long/00114_00_30s.ARW
rawpy read the 150th file at location: ./dataset/Sony/long/00156_00_30s.ARW
rawpy read the 160th file at location: ./dataset/Sony/long/00219_00_10s.ARW
161 images loaded to CPU RAM in Time=41.258 seconds.

Moved images data to a numpy array.
BATCH_SIZE 16 ,final_epoch 4001 ,no_of_batches 10 ,ps 128 ,result_dir ./gt_Sony_medium_regression_MSE_lowerLRat200_BNeverywhere/ ,len(train_ids) 161
Epoch 411: at batch 1: Training dataset Loss=61101.020, Batch Time=1.321
Epoch 411: Epoch time = 1.578, Avg epoch time=1.578, Total Time=0.789

[[    0.        ]
 [    0.        ]
 [75850.546875  ]
 [    0.        ]
 [57702.703125  ]
 [    0.        ]
 [62649.71484375]
 [    0.        ]
 [    0.        ]
 [    0.        ]
 [61101.01953125]
 [54396.171875  ]
 [62649.71484375]
 [    0.        ]
 [51120.55078125]
 [    0.        ]
 [63972.53125   ]
 [    0.        ]
 [57702.703125  ]
 [75850.546875  ]]
Epoch 412: at batch 1: Training dataset Loss=57637.583, Batch Time=0.031
[[57339.015625  ]
 [52863.6171875 ]
 [51069.421875  ]
 [    0.        ]
 [52863.6171875 ]
 [    0.        ]
 [47879.921875  ]
 [47879.921875  ]
 [    0.        ]
 [51069.421875  ]
 [60833.546875  ]
 [52519.04296875]
 [47879.921875  ]
 [45990.234375  ]
 [52863.6171875 ]
 [    0.        ]
 [63972.53125   ]
 [47879.921875  ]
 [52519.04296875]
 [75850.546875  ]]
Epoch 413: at batch 1: Training dataset Loss=57614.876, Batch Time=0.032
[[57339.015625  ]
 [72799.6640625 ]
 [49019.8828125 ]
 [56128.859375  ]
 [62982.70703125]
 [    0.        ]
 [62982.70703125]
 [62982.70703125]
 [72305.6875    ]
 [68858.2421875 ]
 [54412.5703125 ]
 [52519.04296875]
 [62982.70703125]
 [72305.6875    ]
 [54412.5703125 ]
 [72799.6640625 ]
 [72035.96875   ]
 [47879.921875  ]
 [52519.04296875]
 [72799.6640625 ]]
Epoch 414: at batch 1: Training dataset Loss=58441.175, Batch Time=0.036
[[37050.71484375]
 [42477.6953125 ]
 [49019.8828125 ]
 [42477.6953125 ]
 [42477.6953125 ]
 [55572.046875  ]
 [62982.70703125]
 [55572.046875  ]
 [37050.71484375]
 [68858.2421875 ]
 [61210.09375   ]
 [55572.046875  ]
 [42477.6953125 ]
 [72305.6875    ]
 [54412.5703125 ]
 [37050.71484375]
 [36186.01953125]
 [47879.921875  ]
 [52519.04296875]
 [37050.71484375]]
Epoch 415: at batch 1: Training dataset Loss=52549.237, Batch Time=0.029
[[46306.8828125 ]
 [60310.6796875 ]
 [60310.6796875 ]
 [53840.0078125 ]
 [56934.8203125 ]
 [60310.6796875 ]
 [62982.70703125]
 [55572.046875  ]
 [55273.296875  ]
 [46931.875     ]
 [55434.734375  ]
 [55572.046875  ]
 [55273.296875  ]
 [56934.8203125 ]
 [46931.875     ]
 [37050.71484375]
 [36186.01953125]
 [55434.734375  ]
 [52519.04296875]
 [37270.9140625 ]]
Epoch 416: at batch 1: Training dataset Loss=53554.439, Batch Time=0.030
[[71051.0625    ]
 [60310.6796875 ]
 [56951.109375  ]
 [59532.8671875 ]
 [56934.8203125 ]
 [60310.6796875 ]
 [62982.70703125]
 [55572.046875  ]
 [55273.296875  ]
 [53646.95703125]
 [55434.734375  ]
 [55572.046875  ]
 [42866.5546875 ]
 [56934.8203125 ]
 [59532.8671875 ]
 [37050.71484375]
 [42866.5546875 ]
 [42866.5546875 ]
 [42190.5078125 ]
 [37270.9140625 ]]
Epoch 417: at batch 1: Training dataset Loss=55204.414, Batch Time=0.025
[[61948.3359375 ]
 [58625.13671875]
 [56951.109375  ]
 [59532.8671875 ]
 [56934.8203125 ]
 [67287.078125  ]
 [62793.765625  ]
 [42436.5625    ]
 [55273.296875  ]
 [53646.95703125]
 [55659.5703125 ]
 [48478.05078125]
 [42866.5546875 ]
 [62793.765625  ]
 [59532.8671875 ]
 [37050.71484375]
 [55659.5703125 ]
 [47028.8671875 ]
 [42190.5078125 ]
 [37270.9140625 ]]
Epoch 418: at batch 1: Training dataset Loss=52271.645, Batch Time=0.030
[[71961.6484375 ]
 [58625.13671875]
 [59520.71875   ]
 [59532.8671875 ]
 [54534.6484375 ]
 [52885.1015625 ]
 [52885.1015625 ]
 [48500.84765625]
 [54534.6484375 ]
 [53646.95703125]
 [55659.5703125 ]
 [48478.05078125]
 [42866.5546875 ]
 [48500.84765625]
 [61445.17578125]
 [52885.1015625 ]
 [55659.5703125 ]
 [47028.8671875 ]
 [60128.47265625]
 [71961.6484375 ]]
Epoch 419: at batch 1: Training dataset Loss=54945.312, Batch Time=0.033
[[45719.1640625 ]
 [45719.1640625 ]
 [45719.1640625 ]
 [59348.09765625]
 [48744.59765625]
 [61065.0546875 ]
 [59348.09765625]
 [48500.84765625]
 [59348.09765625]
 [59348.09765625]
 [55659.5703125 ]
 [45719.1640625 ]
 [57956.234375  ]
 [48500.84765625]
 [45719.1640625 ]
 [48620.8828125 ]
 [49847.6484375 ]
 [48744.59765625]
 [61065.0546875 ]
 [59348.09765625]]
Epoch 421: at batch 1: Training dataset Loss=51608.111, Batch Time=0.027
Epoch 431: at batch 1: Training dataset Loss=51021.453, Batch Time=0.032
Epoch 441: at batch 1: Training dataset Loss=53554.953, Batch Time=0.026
Epoch 451: at batch 1: Training dataset Loss=49840.138, Batch Time=0.030
Epoch 461: at batch 1: Training dataset Loss=50357.047, Batch Time=0.024
Epoch 471: at batch 1: Training dataset Loss=48906.593, Batch Time=0.024
Epoch 481: at batch 1: Training dataset Loss=51567.465, Batch Time=0.033
Epoch 491: at batch 1: Training dataset Loss=51458.099, Batch Time=0.035
Epoch 501: at batch 1: Training dataset Loss=51344.851, Batch Time=0.029
Epoch 511: at batch 1: Training dataset Loss=50848.214, Batch Time=0.026
Epoch 511: Epoch time = 42.715, Avg epoch time=0.271, Total Time=0.419

[[31589.16796875]
 [72061.0078125 ]
 [60685.390625  ]
 [45522.20703125]
 [67997.3125    ]
 [62213.74609375]
 [72061.0078125 ]
 [57027.49609375]
 [54665.37109375]
 [53042.9921875 ]
 [55631.7109375 ]
 [60946.5546875 ]
 [55728.40234375]
 [57911.0234375 ]
 [34999.76953125]
 [57911.0234375 ]
 [60685.390625  ]
 [44562.8359375 ]
 [37874.7734375 ]
 [54665.37109375]]
Epoch 521: at batch 1: Training dataset Loss=50650.160, Batch Time=0.028
Epoch 531: at batch 1: Training dataset Loss=48726.119, Batch Time=0.028
Epoch 541: at batch 1: Training dataset Loss=49849.555, Batch Time=0.034
Epoch 551: at batch 1: Training dataset Loss=51200.409, Batch Time=0.029
Epoch 561: at batch 1: Training dataset Loss=49452.490, Batch Time=0.027
Epoch 571: at batch 1: Training dataset Loss=49642.379, Batch Time=0.026
Epoch 581: at batch 1: Training dataset Loss=47913.946, Batch Time=0.027
Epoch 591: at batch 1: Training dataset Loss=46390.210, Batch Time=0.027
Epoch 601: at batch 1: Training dataset Loss=49635.033, Batch Time=0.025
Epoch 611: at batch 1: Training dataset Loss=49397.761, Batch Time=0.029
Epoch 611: Epoch time = 83.193, Avg epoch time=0.281, Total Time=0.412

[[65403.81640625]
 [49628.421875  ]
 [35364.421875  ]
 [48955.88671875]
 [35364.421875  ]
 [49709.421875  ]
 [40899.203125  ]
 [47318.83203125]
 [52098.9765625 ]
 [65403.81640625]
 [65403.81640625]
 [46815.23828125]
 [43462.2578125 ]
 [46815.23828125]
 [44482.921875  ]
 [44482.921875  ]
 [35364.421875  ]
 [47318.83203125]
 [47318.83203125]
 [64505.36328125]]
Epoch 621: at batch 1: Training dataset Loss=45533.219, Batch Time=0.029
Epoch 631: at batch 1: Training dataset Loss=47818.980, Batch Time=0.023
Epoch 641: at batch 1: Training dataset Loss=46867.741, Batch Time=0.035
Epoch 651: at batch 1: Training dataset Loss=49022.861, Batch Time=0.030
Epoch 661: at batch 1: Training dataset Loss=45268.883, Batch Time=0.031
Epoch 671: at batch 1: Training dataset Loss=50669.830, Batch Time=0.023
Epoch 681: at batch 1: Training dataset Loss=45316.290, Batch Time=0.032
Epoch 691: at batch 1: Training dataset Loss=47571.773, Batch Time=0.031
Epoch 701: at batch 1: Training dataset Loss=47976.836, Batch Time=0.026
Epoch 711: at batch 1: Training dataset Loss=48185.591, Batch Time=0.029
Epoch 711: Epoch time = 123.851, Avg epoch time=0.270, Total Time=0.410

[[56799.42578125]
 [52429.9375    ]
 [35810.0234375 ]
 [39497.9609375 ]
 [43034.2421875 ]
 [44700.50390625]
 [40037.91015625]
 [60014.25      ]
 [40245.4609375 ]
 [48274.4765625 ]
 [58323.21875   ]
 [38012.2265625 ]
 [43034.2421875 ]
 [52429.9375    ]
 [44100.1796875 ]
 [35810.0234375 ]
 [39123.4296875 ]
 [48222.49609375]
 [52429.9375    ]
 [39123.4296875 ]]
Epoch 721: at batch 1: Training dataset Loss=47247.091, Batch Time=0.027
Epoch 731: at batch 1: Training dataset Loss=47130.558, Batch Time=0.027
Epoch 741: at batch 1: Training dataset Loss=47847.371, Batch Time=0.027
Epoch 751: at batch 1: Training dataset Loss=48624.237, Batch Time=0.031
Epoch 761: at batch 1: Training dataset Loss=45211.180, Batch Time=0.028
Epoch 771: at batch 1: Training dataset Loss=46687.801, Batch Time=0.032
Epoch 781: at batch 1: Training dataset Loss=45783.154, Batch Time=0.026
Epoch 791: at batch 1: Training dataset Loss=46648.941, Batch Time=0.029
Epoch 801: at batch 1: Training dataset Loss=46268.258, Batch Time=0.031
Epoch 811: at batch 1: Training dataset Loss=46558.187, Batch Time=0.032
Epoch 811: Epoch time = 164.512, Avg epoch time=0.294, Total Time=0.409

[[49125.59375   ]
 [45549.09765625]
 [39957.15234375]
 [57564.28125   ]
 [51230.3359375 ]
 [43076.59765625]
 [40352.6875    ]
 [47992.2109375 ]
 [53456.94140625]
 [45688.5859375 ]
 [49125.59375   ]
 [38444.4609375 ]
 [48031.2421875 ]
 [44782.7890625 ]
 [54209.8125    ]
 [48031.2421875 ]
 [51230.3359375 ]
 [48031.2421875 ]
 [48031.2421875 ]
 [45688.5859375 ]]
Epoch 821: at batch 1: Training dataset Loss=47247.302, Batch Time=0.032
Epoch 831: at batch 1: Training dataset Loss=45069.322, Batch Time=0.034
Epoch 841: at batch 1: Training dataset Loss=46816.013, Batch Time=0.025
Epoch 851: at batch 1: Training dataset Loss=45480.643, Batch Time=0.027
Epoch 861: at batch 1: Training dataset Loss=47096.855, Batch Time=0.030
Epoch 871: at batch 1: Training dataset Loss=45142.807, Batch Time=0.027
Epoch 881: at batch 1: Training dataset Loss=47595.162, Batch Time=0.029
Epoch 891: at batch 1: Training dataset Loss=44983.825, Batch Time=0.028
Epoch 901: at batch 1: Training dataset Loss=45193.607, Batch Time=0.025
Epoch 911: at batch 1: Training dataset Loss=47451.019, Batch Time=0.031
Epoch 911: Epoch time = 205.141, Avg epoch time=0.283, Total Time=0.409

[[38918.890625  ]
 [40077.99609375]
 [43873.125     ]
 [40077.99609375]
 [54762.0234375 ]
 [44146.87109375]
 [50765.23046875]
 [38509.953125  ]
 [50765.23046875]
 [40077.99609375]
 [57269.12109375]
 [38026.3984375 ]
 [42661.1953125 ]
 [42086.41796875]
 [42086.41796875]
 [38918.890625  ]
 [38509.953125  ]
 [54762.0234375 ]
 [43873.125     ]
 [54762.0234375 ]]
Epoch 921: at batch 1: Training dataset Loss=44175.997, Batch Time=0.031
Epoch 931: at batch 1: Training dataset Loss=43901.495, Batch Time=0.030
Epoch 941: at batch 1: Training dataset Loss=43803.961, Batch Time=0.034
Epoch 951: at batch 1: Training dataset Loss=41798.151, Batch Time=0.024
Epoch 961: at batch 1: Training dataset Loss=46056.167, Batch Time=0.028
Epoch 971: at batch 1: Training dataset Loss=43686.392, Batch Time=0.031
Epoch 981: at batch 1: Training dataset Loss=43628.690, Batch Time=0.027
Epoch 991: at batch 1: Training dataset Loss=44215.795, Batch Time=0.026
Epoch 1001: at batch 1: Training dataset Loss=40090.533, Batch Time=0.032
Epoch 1011: at batch 1: Training dataset Loss=44607.778, Batch Time=0.032
Epoch 1011: Epoch time = 245.865, Avg epoch time=0.273, Total Time=0.408

[[50155.41015625]
 [41939.9921875 ]
 [38516.390625  ]
 [35661.32421875]
 [58297.46875   ]
 [44503.8046875 ]
 [44931.94921875]
 [53033.8046875 ]
 [46205.6796875 ]
 [42411.703125  ]
 [46205.6796875 ]
 [64551.703125  ]
 [47058.71875   ]
 [38712.6015625 ]
 [64551.703125  ]
 [30091.49609375]
 [53377.37109375]
 [38196.3203125 ]
 [58297.46875   ]
 [58297.46875   ]]
Epoch 1021: at batch 1: Training dataset Loss=46454.027, Batch Time=0.026
Epoch 1031: at batch 1: Training dataset Loss=43330.055, Batch Time=0.026
Epoch 1041: at batch 1: Training dataset Loss=41866.686, Batch Time=0.026
Epoch 1051: at batch 1: Training dataset Loss=42382.413, Batch Time=0.031
Epoch 1061: at batch 1: Training dataset Loss=46703.265, Batch Time=0.028
Epoch 1071: at batch 1: Training dataset Loss=43701.982, Batch Time=0.024
Epoch 1081: at batch 1: Training dataset Loss=40759.209, Batch Time=0.024
Epoch 1091: at batch 1: Training dataset Loss=42943.807, Batch Time=0.024
Epoch 1101: at batch 1: Training dataset Loss=42178.032, Batch Time=0.032
Epoch 1111: at batch 1: Training dataset Loss=41535.821, Batch Time=0.035
Epoch 1111: Epoch time = 286.536, Avg epoch time=0.264, Total Time=0.408

[[52530.33984375]
 [50661.30078125]
 [52530.33984375]
 [33891.390625  ]
 [35731.45703125]
 [60246.1015625 ]
 [53110.203125  ]
 [32841.5625    ]
 [34030.1640625 ]
 [35731.45703125]
 [42097.359375  ]
 [29878.24804688]
 [28548.52734375]
 [50602.0390625 ]
 [53109.84375   ]
 [52527.4140625 ]
 [52527.4140625 ]
 [42579.2109375 ]
 [52527.4140625 ]
 [35731.45703125]]
Epoch 1121: at batch 1: Training dataset Loss=43546.053, Batch Time=0.032
Epoch 1131: at batch 1: Training dataset Loss=42505.710, Batch Time=0.028
Epoch 1141: at batch 1: Training dataset Loss=44750.152, Batch Time=0.029
Epoch 1151: at batch 1: Training dataset Loss=46517.531, Batch Time=0.023
Epoch 1161: at batch 1: Training dataset Loss=39634.567, Batch Time=0.026
Epoch 1171: at batch 1: Training dataset Loss=41006.794, Batch Time=0.026
Epoch 1181: at batch 1: Training dataset Loss=43956.038, Batch Time=0.032
Epoch 1191: at batch 1: Training dataset Loss=44513.491, Batch Time=0.029
Epoch 1201: at batch 1: Training dataset Loss=39934.996, Batch Time=0.026
Epoch 1211: at batch 1: Training dataset Loss=44106.237, Batch Time=0.026
Epoch 1211: Epoch time = 327.146, Avg epoch time=0.282, Total Time=0.408

[[40636.8203125 ]
 [38151.62109375]
 [39112.8984375 ]
 [36148.21875   ]
 [38640.9140625 ]
 [62864.953125  ]
 [41135.84765625]
 [39112.8984375 ]
 [43715.0390625 ]
 [49987.921875  ]
 [62864.953125  ]
 [31301.359375  ]
 [46909.40625   ]
 [41135.84765625]
 [46136.28125   ]
 [62864.953125  ]
 [65949.6796875 ]
 [26343.05273438]
 [43715.0390625 ]
 [62864.953125  ]]
Epoch 1221: at batch 1: Training dataset Loss=42292.551, Batch Time=0.029
Epoch 1231: at batch 1: Training dataset Loss=40730.676, Batch Time=0.028
Epoch 1241: at batch 1: Training dataset Loss=38817.333, Batch Time=0.029
Epoch 1251: at batch 1: Training dataset Loss=42962.444, Batch Time=0.026
Epoch 1261: at batch 1: Training dataset Loss=43281.468, Batch Time=0.025
Epoch 1271: at batch 1: Training dataset Loss=39765.996, Batch Time=0.032
Epoch 1281: at batch 1: Training dataset Loss=41837.343, Batch Time=0.026
Epoch 1291: at batch 1: Training dataset Loss=36451.543, Batch Time=0.029
Epoch 1301: at batch 1: Training dataset Loss=38857.354, Batch Time=0.026
Epoch 1311: at batch 1: Training dataset Loss=40359.771, Batch Time=0.032
Epoch 1311: Epoch time = 367.915, Avg epoch time=0.274, Total Time=0.408

[[57300.984375  ]
 [44172.73046875]
 [28670.87304688]
 [43813.59375   ]
 [43813.59375   ]
 [47021.        ]
 [43753.4140625 ]
 [46230.6484375 ]
 [35837.3125    ]
 [36643.59765625]
 [47021.        ]
 [35837.3125    ]
 [36643.59765625]
 [47021.        ]
 [28670.87304688]
 [36811.48046875]
 [42429.3203125 ]
 [43753.4140625 ]
 [34097.94140625]
 [41547.02734375]]
Epoch 1321: at batch 1: Training dataset Loss=40309.468, Batch Time=0.033
Epoch 1331: at batch 1: Training dataset Loss=41858.268, Batch Time=0.029
Epoch 1341: at batch 1: Training dataset Loss=40625.971, Batch Time=0.029
Epoch 1351: at batch 1: Training dataset Loss=39322.970, Batch Time=0.029
Epoch 1361: at batch 1: Training dataset Loss=37986.142, Batch Time=0.025
Epoch 1371: at batch 1: Training dataset Loss=42404.016, Batch Time=0.028
Epoch 1381: at batch 1: Training dataset Loss=36976.741, Batch Time=0.026
Epoch 1391: at batch 1: Training dataset Loss=38758.984, Batch Time=0.035
Epoch 1401: at batch 1: Training dataset Loss=41004.899, Batch Time=0.032
Epoch 1411: at batch 1: Training dataset Loss=35851.718, Batch Time=0.027
Epoch 1411: Epoch time = 408.598, Avg epoch time=0.276, Total Time=0.408

[[29174.08203125]
 [27840.74023438]
 [29174.08203125]
 [34745.34375   ]
 [51617.578125  ]
 [54622.04296875]
 [26142.50390625]
 [23488.93359375]
 [29174.08203125]
 [44924.25390625]
 [23488.93359375]
 [24140.8046875 ]
 [60876.65234375]
 [22518.75390625]
 [59524.34375   ]
 [43749.171875  ]
 [35049.6640625 ]
 [51617.578125  ]
 [40779.4453125 ]
 [24140.8046875 ]]
Epoch 1421: at batch 1: Training dataset Loss=37876.236, Batch Time=0.023
Epoch 1431: at batch 1: Training dataset Loss=39944.696, Batch Time=0.035
Epoch 1441: at batch 1: Training dataset Loss=38383.393, Batch Time=0.031
Epoch 1451: at batch 1: Training dataset Loss=35825.301, Batch Time=0.037
Epoch 1461: at batch 1: Training dataset Loss=41277.035, Batch Time=0.031
Epoch 1471: at batch 1: Training dataset Loss=37235.445, Batch Time=0.024
Epoch 1481: at batch 1: Training dataset Loss=38123.167, Batch Time=0.031
Epoch 1491: at batch 1: Training dataset Loss=38656.256, Batch Time=0.026
Epoch 1501: at batch 1: Training dataset Loss=39774.173, Batch Time=0.026
Epoch 1511: at batch 1: Training dataset Loss=38539.119, Batch Time=0.035
Epoch 1511: Epoch time = 449.287, Avg epoch time=0.271, Total Time=0.408

[[40704.78515625]
 [37744.6796875 ]
 [33246.3828125 ]
 [30213.01757812]
 [50646.98828125]
 [56808.09765625]
 [50646.98828125]
 [51983.92578125]
 [51983.92578125]
 [25419.1015625 ]
 [56808.09765625]
 [29260.22851562]
 [50646.98828125]
 [51983.92578125]
 [40978.1171875 ]
 [40704.78515625]
 [56808.09765625]
 [56808.09765625]
 [33726.234375  ]
 [56808.09765625]]
Epoch 1521: at batch 1: Training dataset Loss=35852.582, Batch Time=0.030
Epoch 1531: at batch 1: Training dataset Loss=36521.468, Batch Time=0.025
Epoch 1541: at batch 1: Training dataset Loss=34845.824, Batch Time=0.035
Epoch 1551: at batch 1: Training dataset Loss=37766.231, Batch Time=0.033
Epoch 1561: at batch 1: Training dataset Loss=38477.146, Batch Time=0.026
Epoch 1571: at batch 1: Training dataset Loss=36944.983, Batch Time=0.026
Epoch 1581: at batch 1: Training dataset Loss=35400.538, Batch Time=0.032
Epoch 1591: at batch 1: Training dataset Loss=37920.985, Batch Time=0.027
Epoch 1601: at batch 1: Training dataset Loss=35101.391, Batch Time=0.029
Epoch 1611: at batch 1: Training dataset Loss=35771.840, Batch Time=0.024
Epoch 1611: Epoch time = 490.062, Avg epoch time=0.264, Total Time=0.408

[[28148.0390625 ]
 [33157.        ]
 [37236.04296875]
 [33091.3828125 ]
 [51177.6328125 ]
 [33157.        ]
 [36314.4140625 ]
 [33025.171875  ]
 [35204.26953125]
 [35204.26953125]
 [40281.1171875 ]
 [29710.25      ]
 [36073.7421875 ]
 [33816.328125  ]
 [36073.7421875 ]
 [24419.62304688]
 [37946.96484375]
 [17220.49609375]
 [31228.50390625]
 [31228.50390625]]
Epoch 1621: at batch 1: Training dataset Loss=35917.216, Batch Time=0.032
Epoch 1631: at batch 1: Training dataset Loss=36514.569, Batch Time=0.033
Epoch 1641: at batch 1: Training dataset Loss=35975.202, Batch Time=0.024
Epoch 1651: at batch 1: Training dataset Loss=37390.635, Batch Time=0.034
Epoch 1661: at batch 1: Training dataset Loss=35151.066, Batch Time=0.024
Epoch 1671: at batch 1: Training dataset Loss=35101.986, Batch Time=0.029
Epoch 1681: at batch 1: Training dataset Loss=37869.294, Batch Time=0.029
Epoch 1691: at batch 1: Training dataset Loss=36692.849, Batch Time=0.023
Epoch 1701: at batch 1: Training dataset Loss=32296.265, Batch Time=0.027
Epoch 1711: at batch 1: Training dataset Loss=33726.503, Batch Time=0.027
Epoch 1711: Epoch time = 530.793, Avg epoch time=0.281, Total Time=0.408

[[21638.140625  ]
 [38640.21875   ]
 [21638.140625  ]
 [26542.56835938]
 [41183.3203125 ]
 [21638.140625  ]
 [38640.21875   ]
 [45649.765625  ]
 [27310.9296875 ]
 [43740.6015625 ]
 [57372.20703125]
 [21638.140625  ]
 [26279.0546875 ]
 [29123.36328125]
 [41221.7109375 ]
 [43740.6015625 ]
 [39491.15625   ]
 [43740.6015625 ]
 [21638.140625  ]
 [43740.6015625 ]]
Epoch 1721: at batch 1: Training dataset Loss=31620.740, Batch Time=0.032
Epoch 1731: at batch 1: Training dataset Loss=33979.753, Batch Time=0.031
Epoch 1741: at batch 1: Training dataset Loss=29234.349, Batch Time=0.023
Epoch 1751: at batch 1: Training dataset Loss=34532.534, Batch Time=0.031
Epoch 1761: at batch 1: Training dataset Loss=34695.331, Batch Time=0.026
Epoch 1771: at batch 1: Training dataset Loss=35904.807, Batch Time=0.030
Epoch 1781: at batch 1: Training dataset Loss=33738.012, Batch Time=0.033
Epoch 1791: at batch 1: Training dataset Loss=33866.408, Batch Time=0.023
Epoch 1801: at batch 1: Training dataset Loss=32028.393, Batch Time=0.027
Epoch 1811: at batch 1: Training dataset Loss=33976.963, Batch Time=0.028
Epoch 1811: Epoch time = 571.641, Avg epoch time=0.290, Total Time=0.408

[[28279.05859375]
 [35677.15625   ]
 [30092.08203125]
 [25195.86132812]
 [31926.60546875]
 [43404.9609375 ]
 [31926.60546875]
 [30092.08203125]
 [32125.19921875]
 [35677.15625   ]
 [24671.59179688]
 [36085.234375  ]
 [31053.90234375]
 [24671.59179688]
 [51675.8203125 ]
 [35677.15625   ]
 [24671.59179688]
 [31053.90234375]
 [33432.11328125]
 [35677.15625   ]]
Epoch 1821: at batch 1: Training dataset Loss=32459.670, Batch Time=0.029
Epoch 1831: at batch 1: Training dataset Loss=28619.944, Batch Time=0.026
Epoch 1841: at batch 1: Training dataset Loss=33621.551, Batch Time=0.029
Epoch 1851: at batch 1: Training dataset Loss=34466.858, Batch Time=0.026
Epoch 1861: at batch 1: Training dataset Loss=32893.400, Batch Time=0.026
Epoch 1871: at batch 1: Training dataset Loss=32230.625, Batch Time=0.031
Epoch 1881: at batch 1: Training dataset Loss=30189.499, Batch Time=0.032
Epoch 1891: at batch 1: Training dataset Loss=30327.878, Batch Time=0.027
Epoch 1901: at batch 1: Training dataset Loss=34348.909, Batch Time=0.027
Epoch 1911: at batch 1: Training dataset Loss=32335.542, Batch Time=0.034
Epoch 1911: Epoch time = 612.744, Avg epoch time=0.287, Total Time=0.408

[[38740.52734375]
 [30980.56640625]
 [30980.56640625]
 [23456.3515625 ]
 [28189.84375   ]
 [34783.6875    ]
 [27652.10351562]
 [26920.67382812]
 [26920.67382812]
 [27652.10351562]
 [26920.67382812]
 [27652.10351562]
 [40367.4140625 ]
 [38580.8203125 ]
 [26920.67382812]
 [50432.421875  ]
 [30980.56640625]
 [27984.07421875]
 [34783.6875    ]
 [38580.8203125 ]]
Epoch 1921: at batch 1: Training dataset Loss=31319.383, Batch Time=0.024
Epoch 1931: at batch 1: Training dataset Loss=35830.308, Batch Time=0.028
Epoch 1941: at batch 1: Training dataset Loss=32031.278, Batch Time=0.033
Epoch 1951: at batch 1: Training dataset Loss=31294.141, Batch Time=0.029
Epoch 1961: at batch 1: Training dataset Loss=29633.047, Batch Time=0.025
Epoch 1971: at batch 1: Training dataset Loss=29507.434, Batch Time=0.029
Epoch 1981: at batch 1: Training dataset Loss=29998.669, Batch Time=0.026
Epoch 1991: at batch 1: Training dataset Loss=32574.419, Batch Time=0.028
Epoch 2001: at batch 1: Training dataset Loss=31297.221, Batch Time=0.035
Epoch 2011: at batch 1: Training dataset Loss=29190.974, Batch Time=0.029
Epoch 2011: Epoch time = 653.478, Avg epoch time=0.290, Total Time=0.408

[[26536.87890625]
 [31642.265625  ]
 [23370.82421875]
 [32307.609375  ]
 [31642.265625  ]
 [44986.73046875]
 [32411.34570312]
 [32304.1015625 ]
 [21258.13867188]
 [33210.65625   ]
 [32304.1015625 ]
 [19939.01757812]
 [26536.87890625]
 [26717.90039062]
 [21258.13867188]
 [32411.34570312]
 [21258.13867188]
 [26717.90039062]
 [23087.0625    ]
 [36687.4296875 ]]
Epoch 2021: at batch 1: Training dataset Loss=30546.911, Batch Time=0.033
Epoch 2031: at batch 1: Training dataset Loss=30859.974, Batch Time=0.034
Epoch 2041: at batch 1: Training dataset Loss=29479.014, Batch Time=0.023
Epoch 2051: at batch 1: Training dataset Loss=30426.899, Batch Time=0.035
Epoch 2061: at batch 1: Training dataset Loss=26370.334, Batch Time=0.027
Epoch 2071: at batch 1: Training dataset Loss=28209.896, Batch Time=0.027
Epoch 2081: at batch 1: Training dataset Loss=29739.513, Batch Time=0.032
Epoch 2091: at batch 1: Training dataset Loss=29061.615, Batch Time=0.031
Epoch 2101: at batch 1: Training dataset Loss=27636.056, Batch Time=0.030
Epoch 2111: at batch 1: Training dataset Loss=27063.857, Batch Time=0.029
Epoch 2111: Epoch time = 694.393, Avg epoch time=0.283, Total Time=0.408

[[16493.265625  ]
 [26069.8359375 ]
 [18552.80664062]
 [23276.6953125 ]
 [46535.5703125 ]
 [25998.83984375]
 [26534.90234375]
 [26378.50195312]
 [34589.46484375]
 [31184.84570312]
 [23322.96484375]
 [26534.90234375]
 [17497.78515625]
 [46535.5703125 ]
 [46535.5703125 ]
 [30129.93945312]
 [26534.90234375]
 [23276.6953125 ]
 [19579.84765625]
 [30226.02734375]]
Epoch 2121: at batch 1: Training dataset Loss=30272.424, Batch Time=0.024
Epoch 2131: at batch 1: Training dataset Loss=26176.541, Batch Time=0.025
Epoch 2141: at batch 1: Training dataset Loss=29925.152, Batch Time=0.029
Epoch 2151: at batch 1: Training dataset Loss=30942.972, Batch Time=0.028
Epoch 2161: at batch 1: Training dataset Loss=27166.854, Batch Time=0.036
Epoch 2171: at batch 1: Training dataset Loss=25091.063, Batch Time=0.025
Epoch 2181: at batch 1: Training dataset Loss=27858.237, Batch Time=0.033
Epoch 2191: at batch 1: Training dataset Loss=26445.225, Batch Time=0.023
Epoch 2201: at batch 1: Training dataset Loss=24716.866, Batch Time=0.025
Epoch 2211: at batch 1: Training dataset Loss=25453.499, Batch Time=0.029
Epoch 2211: Epoch time = 735.277, Avg epoch time=0.285, Total Time=0.408

[[30950.95507812]
 [42207.8984375 ]
 [30515.38867188]
 [23825.68554688]
 [15915.14453125]
 [19011.58398438]
 [20864.77734375]
 [19011.58398438]
 [23847.52734375]
 [22350.4609375 ]
 [19011.58398438]
 [42207.8984375 ]
 [25040.94140625]
 [34647.015625  ]
 [23847.52734375]
 [23715.61132812]
 [34145.109375  ]
 [19011.58398438]
 [42207.8984375 ]
 [29720.75585938]]
Epoch 2221: at batch 1: Training dataset Loss=27503.073, Batch Time=0.035
Epoch 2231: at batch 1: Training dataset Loss=26619.522, Batch Time=0.034
Epoch 2241: at batch 1: Training dataset Loss=30449.683, Batch Time=0.032
Epoch 2251: at batch 1: Training dataset Loss=27407.199, Batch Time=0.035
Epoch 2261: at batch 1: Training dataset Loss=30358.827, Batch Time=0.026
Epoch 2271: at batch 1: Training dataset Loss=29800.966, Batch Time=0.031
Epoch 2281: at batch 1: Training dataset Loss=28112.003, Batch Time=0.029
Epoch 2291: at batch 1: Training dataset Loss=26166.182, Batch Time=0.025
Epoch 2301: at batch 1: Training dataset Loss=26417.134, Batch Time=0.028
Epoch 2311: at batch 1: Training dataset Loss=25957.241, Batch Time=0.030
Epoch 2311: Epoch time = 776.040, Avg epoch time=0.276, Total Time=0.408

[[26460.74023438]
 [44773.5234375 ]
 [22461.32421875]
 [24947.28125   ]
 [24269.3984375 ]
 [24269.3984375 ]
 [13166.30175781]
 [26460.74023438]
 [35857.515625  ]
 [25088.41992188]
 [20338.96484375]
 [20338.96484375]
 [29355.578125  ]
 [27993.328125  ]
 [47718.83984375]
 [39383.12890625]
 [26460.74023438]
 [16630.7578125 ]
 [29355.578125  ]
 [44773.5234375 ]]
Epoch 2321: at batch 1: Training dataset Loss=24885.469, Batch Time=0.025
Epoch 2331: at batch 1: Training dataset Loss=26495.607, Batch Time=0.033
Epoch 2341: at batch 1: Training dataset Loss=27345.183, Batch Time=0.031
Epoch 2351: at batch 1: Training dataset Loss=24628.592, Batch Time=0.032
Epoch 2361: at batch 1: Training dataset Loss=23975.927, Batch Time=0.032
Epoch 2371: at batch 1: Training dataset Loss=23717.838, Batch Time=0.023
Epoch 2381: at batch 1: Training dataset Loss=29173.951, Batch Time=0.033
Epoch 2391: at batch 1: Training dataset Loss=24322.078, Batch Time=0.035
Epoch 2401: at batch 1: Training dataset Loss=26787.246, Batch Time=0.033
Epoch 2411: at batch 1: Training dataset Loss=28720.035, Batch Time=0.027
Epoch 2411: Epoch time = 816.666, Avg epoch time=0.297, Total Time=0.408

[[26336.2265625 ]
 [27472.84375   ]
 [23378.76953125]
 [28514.0234375 ]
 [26336.2265625 ]
 [36081.875     ]
 [19666.78125   ]
 [21460.66015625]
 [26336.2265625 ]
 [28318.1015625 ]
 [19039.01171875]
 [23439.0234375 ]
 [17244.72265625]
 [21460.66015625]
 [36621.234375  ]
 [50556.7265625 ]
 [18192.13867188]
 [28579.63671875]
 [27494.0625    ]
 [38048.58203125]]
Epoch 2421: at batch 1: Training dataset Loss=22538.634, Batch Time=0.025
Epoch 2431: at batch 1: Training dataset Loss=24068.607, Batch Time=0.027
Epoch 2441: at batch 1: Training dataset Loss=24763.480, Batch Time=0.029
Epoch 2451: at batch 1: Training dataset Loss=24428.394, Batch Time=0.024
Epoch 2461: at batch 1: Training dataset Loss=23986.219, Batch Time=0.032
Epoch 2471: at batch 1: Training dataset Loss=25601.343, Batch Time=0.028
Epoch 2481: at batch 1: Training dataset Loss=23027.498, Batch Time=0.035
Epoch 2491: at batch 1: Training dataset Loss=24607.402, Batch Time=0.034
Epoch 2501: at batch 1: Training dataset Loss=24163.060, Batch Time=0.034
Epoch 2511: at batch 1: Training dataset Loss=25051.471, Batch Time=0.026
Epoch 2511: Epoch time = 857.450, Avg epoch time=0.274, Total Time=0.408

[[31869.359375  ]
 [24076.859375  ]
 [28190.6640625 ]
 [16081.11816406]
 [37148.703125  ]
 [24076.859375  ]
 [28190.6640625 ]
 [23763.73046875]
 [21365.25195312]
 [14927.48828125]
 [17310.19921875]
 [27645.234375  ]
 [40919.58203125]
 [28062.21875   ]
 [42764.578125  ]
 [24612.1015625 ]
 [21365.25195312]
 [20440.44335938]
 [13500.68164062]
 [28190.6640625 ]]
Epoch 2521: at batch 1: Training dataset Loss=23558.990, Batch Time=0.023
Epoch 2531: at batch 1: Training dataset Loss=27609.961, Batch Time=0.024
Epoch 2541: at batch 1: Training dataset Loss=27151.604, Batch Time=0.031
Epoch 2551: at batch 1: Training dataset Loss=24298.319, Batch Time=0.025
Epoch 2561: at batch 1: Training dataset Loss=21871.594, Batch Time=0.029
Epoch 2571: at batch 1: Training dataset Loss=22740.697, Batch Time=0.034
Epoch 2581: at batch 1: Training dataset Loss=25955.336, Batch Time=0.031
Epoch 2591: at batch 1: Training dataset Loss=23701.389, Batch Time=0.030
Epoch 2601: at batch 1: Training dataset Loss=22385.223, Batch Time=0.032
Epoch 2611: at batch 1: Training dataset Loss=24537.979, Batch Time=0.026
Epoch 2611: Epoch time = 898.219, Avg epoch time=0.272, Total Time=0.408

[[10909.3359375 ]
 [21787.859375  ]
 [18769.17578125]
 [41679.51171875]
 [41679.51171875]
 [21787.859375  ]
 [22712.08203125]
 [19762.38671875]
 [19486.34960938]
 [22981.88867188]
 [18769.17578125]
 [ 9173.453125  ]
 [ 6416.50732422]
 [20399.02539062]
 [17994.88671875]
 [22712.08203125]
 [18769.17578125]
 [32137.59375   ]
 [10909.3359375 ]
 [21787.859375  ]]
Epoch 2621: at batch 1: Training dataset Loss=21556.867, Batch Time=0.028
Epoch 2631: at batch 1: Training dataset Loss=23067.334, Batch Time=0.023
Epoch 2641: at batch 1: Training dataset Loss=22635.218, Batch Time=0.026
Epoch 2651: at batch 1: Training dataset Loss=22979.841, Batch Time=0.026
Epoch 2661: at batch 1: Training dataset Loss=22292.923, Batch Time=0.029
Epoch 2671: at batch 1: Training dataset Loss=21781.382, Batch Time=0.026
Epoch 2681: at batch 1: Training dataset Loss=22803.706, Batch Time=0.032
Epoch 2691: at batch 1: Training dataset Loss=25139.824, Batch Time=0.026
Epoch 2701: at batch 1: Training dataset Loss=20817.443, Batch Time=0.031
Epoch 2711: at batch 1: Training dataset Loss=20941.235, Batch Time=0.027
Epoch 2711: Epoch time = 938.701, Avg epoch time=0.275, Total Time=0.408

[[19529.9375    ]
 [20096.05078125]
 [25131.01953125]
 [21948.1484375 ]
 [16774.02148438]
 [41015.8203125 ]
 [20096.05078125]
 [21948.1484375 ]
 [18538.5859375 ]
 [22665.0078125 ]
 [36113.53515625]
 [18538.5859375 ]
 [21948.1484375 ]
 [35798.16015625]
 [21032.05078125]
 [20096.05078125]
 [20096.05078125]
 [23897.61914062]
 [12250.25488281]
 [32570.37695312]]
Epoch 2721: at batch 1: Training dataset Loss=23311.419, Batch Time=0.026
Epoch 2731: at batch 1: Training dataset Loss=21093.595, Batch Time=0.030
Epoch 2741: at batch 1: Training dataset Loss=19859.866, Batch Time=0.031
Epoch 2751: at batch 1: Training dataset Loss=21372.914, Batch Time=0.031
Epoch 2761: at batch 1: Training dataset Loss=21286.840, Batch Time=0.032
Epoch 2771: at batch 1: Training dataset Loss=18412.299, Batch Time=0.025
Epoch 2781: at batch 1: Training dataset Loss=22272.761, Batch Time=0.028
Epoch 2791: at batch 1: Training dataset Loss=21390.059, Batch Time=0.030
^CTraceback (most recent call last):
  File "medium_regression_square_loss.py", line 349, in <module>
    saver.save(sess, checkpoint_dir + 'model.ckpt')
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 1458, in save
    meta_graph_filename, strip_default_attrs=strip_default_attrs)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 1503, in export_meta_graph
    strip_default_attrs=strip_default_attrs)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 1792, in export_meta_graph
    **kwargs)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py", line 1000, in export_scoped_meta_graph
    **kwargs)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py", line 579, in create_meta_graph_def
    meta_graph_def.graph_def.MergeFrom(graph_def)
KeyboardInterrupt
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ mkdir gt_Sony_medium_scaled_regression_MSE_lowerLRat200
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ vi medium_regression_square_loss.py
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ python medium_regression_square_loss.py 




Found 161 images to train with

Training on 161 images only

2020-12-12 14:56:30.443203: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-12 14:56:30.585647: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-12 14:56:30.585681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-12 14:56:30.872485: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-12 14:56:30.872520: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-12 14:56:30.872537: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-12 14:56:30.872637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./gt_Sony_medium_scaled_regression_MSE_lowerLRat200/. Hence, will create the folder.
Gamma curve:
Every 8K, 0 to 64K
[[0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]]
Every 125, 64536 to 65536
[[14104 17115 20761 25175 30515 36975 44786 54227]
 [ 1408  2284  3702  5994  9696 15670 25302 40817]
 [  653  1167  2083  3715  6616 11770 20917 37129]]
Every 1, 65526 to 65536
[[64641 64739 64838 64937 65036 65136 65235 65335 65435 65535]
 [63323 63565 63808 64051 64296 64542 64789 65036 65285 65535]
 [62889 63178 63468 63759 64051 64345 64641 64937 65235 65535]]

last epoch of previous run: 0
rawpy read the 0th file at location: ./dataset/Sony/long/00018_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 250.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.95669, 0.00000, 100.00000, 3485919
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.01631, 300.00000, 1
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 250.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00164, 300.00000, 1177951
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00201, 100.00000, 803131
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.02465, 100.00000, 3700
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00427, 250.00000, 2882267
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00001, 250.00000, 5540515
rawpy read the 10th file at location: ./dataset/Sony/long/00038_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00887, 250.00000, 2598691
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00228, 300.00000, 4653941
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00017, 250.00000, 6102869
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00613, 100.00000, 1458187
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00012, 250.00000, 3426263
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00116, 300.00000, 4586373
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00155, 100.00000, 909101
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00327, 100.00000, 3630407
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00002, 300.00000, 6735543
rawpy read the 20th file at location: ./dataset/Sony/long/00072_00_30s.ARW
rawpy read the 30th file at location: ./dataset/Sony/long/00039_00_10s.ARW
rawpy read the 40th file at location: ./dataset/Sony/long/00200_00_10s.ARW
rawpy read the 50th file at location: ./dataset/Sony/long/00024_00_10s.ARW
rawpy read the 60th file at location: ./dataset/Sony/long/00084_00_30s.ARW
rawpy read the 70th file at location: ./dataset/Sony/long/00164_00_30s.ARW
rawpy read the 80th file at location: ./dataset/Sony/long/00128_00_30s.ARW
rawpy read the 90th file at location: ./dataset/Sony/long/00057_00_10s.ARW
rawpy read the 100th file at location: ./dataset/Sony/long/00059_00_10s.ARW
rawpy read the 110th file at location: ./dataset/Sony/long/00026_00_10s.ARW
rawpy read the 120th file at location: ./dataset/Sony/long/00012_00_10s.ARW
rawpy read the 130th file at location: ./dataset/Sony/long/00090_00_30s.ARW
rawpy read the 140th file at location: ./dataset/Sony/long/00114_00_30s.ARW
rawpy read the 150th file at location: ./dataset/Sony/long/00156_00_30s.ARW
rawpy read the 160th file at location: ./dataset/Sony/long/00219_00_10s.ARW
161 images loaded to CPU RAM in Time=40.334 seconds.

Moved images data to a numpy array.
BATCH_SIZE 16 ,final_epoch 4001 ,no_of_batches 10 ,ps 128 ,result_dir ./gt_Sony_medium_scaled_regression_MSE_lowerLRat200/ ,len(train_ids) 161
Scaling the regression labels now.
Starting Training on index [ 46 159  21  59 139  45  83  79  33  82 118  78  40  78 134  12], dataset index: [157 215 143  37 146  92 195  67  36 175 165 108 200 108 232  46]
Starting Training on gammas [100 250 250 100 250 300 100 250 300 100 300 300 300 300 300 250]
Epoch 0: at batch 1: Training dataset Loss=0.727, Batch Time=1.276
[[0.        ]
 [0.2535539 ]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.92831504]
 [0.75299424]
 [0.        ]
 [0.72125202]
 [0.25974879]
 [0.        ]
 [2.60964513]
 [0.75299424]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.2367914 ]
 [0.        ]
 [0.        ]]
Epoch 1: at batch 1: Training dataset Loss=0.853, Batch Time=0.031
Epoch 1: Epoch time = 2.049, Avg epoch time=0.288, Total Time=1.025

[[0.3125    ]
 [0.23081271]
 [0.30203229]
 [0.23935266]
 [0.        ]
 [0.18953325]
 [0.32851955]
 [0.23935266]
 [0.23935266]
 [0.30203229]
 [1.34228063]
 [0.30203229]
 [1.34228063]
 [0.3125    ]
 [0.        ]
 [1.63833594]
 [0.18953325]
 [0.63176805]
 [0.        ]
 [0.32851955]]
Epoch 2: at batch 1: Training dataset Loss=0.708, Batch Time=0.024
[[0.23215362]
 [0.4375    ]
 [0.63988173]
 [0.96520305]
 [0.3125    ]
 [0.23215362]
 [0.4375    ]
 [0.23935266]
 [0.23215362]
 [0.57595515]
 [0.375     ]
 [0.30203229]
 [0.63988173]
 [0.375     ]
 [0.32562953]
 [0.4375    ]
 [0.18953325]
 [0.63176805]
 [0.67236722]
 [0.4375    ]]
Epoch 3: at batch 1: Training dataset Loss=0.520, Batch Time=0.036
[[0.375     ]
 [0.4375    ]
 [0.16790873]
 [0.32697576]
 [0.3125    ]
 [0.23215362]
 [0.25      ]
 [0.3125    ]
 [0.25      ]
 [0.57595515]
 [0.32697576]
 [0.30203229]
 [0.32697576]
 [0.32697576]
 [0.57734835]
 [0.16790873]
 [0.18953325]
 [2.54078388]
 [0.67236722]
 [0.4375    ]]
Epoch 4: at batch 1: Training dataset Loss=0.634, Batch Time=0.033
[[0.69773149]
 [0.45023093]
 [0.16790873]
 [0.1875    ]
 [0.3125    ]
 [0.32554084]
 [0.5122925 ]
 [0.5122925 ]
 [0.45023093]
 [0.34427068]
 [0.59709466]
 [0.45023093]
 [0.69773149]
 [0.1875    ]
 [0.12191591]
 [0.31639349]
 [0.45023093]
 [0.1875    ]
 [0.67236722]
 [0.32554084]]
Epoch 5: at batch 1: Training dataset Loss=0.492, Batch Time=0.026
[[0.23745394]
 [0.35715765]
 [0.16790873]
 [0.43368   ]
 [0.35715765]
 [0.32554084]
 [0.41053924]
 [0.43368   ]
 [0.33658051]
 [0.23745394]
 [0.59709466]
 [0.45023093]
 [0.33658051]
 [0.26159823]
 [0.35715765]
 [0.31639349]
 [0.45023093]
 [0.1875    ]
 [0.33658051]
 [0.41053924]]
Epoch 6: at batch 1: Training dataset Loss=0.403, Batch Time=0.030
[[0.24435467]
 [0.35715765]
 [0.4054091 ]
 [0.25      ]
 [0.4054091 ]
 [0.4375    ]
 [0.2820034 ]
 [0.43368   ]
 [0.33658051]
 [0.46379024]
 [0.2820034 ]
 [0.27990028]
 [0.27990028]
 [0.25      ]
 [0.4054091 ]
 [0.31639349]
 [0.24435467]
 [0.24616528]
 [0.4375    ]
 [0.4375    ]]
Epoch 7: at batch 1: Training dataset Loss=0.389, Batch Time=0.029
[[0.24435467]
 [0.35715765]
 [1.27138686]
 [1.27138686]
 [0.30434054]
 [0.35408095]
 [0.375     ]
 [0.43368   ]
 [0.29367942]
 [0.29367942]
 [0.2820034 ]
 [0.29367942]
 [0.35408095]
 [0.25      ]
 [0.4054091 ]
 [0.31639349]
 [0.3125    ]
 [0.24616528]
 [0.4375    ]
 [0.4375    ]]
Epoch 8: at batch 1: Training dataset Loss=0.532, Batch Time=0.032
[[0.3156389 ]
 [0.2973578 ]
 [1.27138686]
 [1.27138686]
 [0.49878249]
 [0.35408095]
 [0.375     ]
 [0.35960034]
 [0.3156389 ]
 [0.70493555]
 [0.2820034 ]
 [0.29367942]
 [0.2973578 ]
 [0.70493555]
 [0.4054091 ]
 [0.3156389 ]
 [0.3125    ]
 [0.3156389 ]
 [0.2973578 ]
 [0.4375    ]]
Epoch 9: at batch 1: Training dataset Loss=0.793, Batch Time=0.034
[[0.3156389 ]
 [0.37171453]
 [0.28553802]
 [0.25      ]
 [0.35328999]
 [0.30168572]
 [0.375     ]
 [0.35960034]
 [0.25      ]
 [0.70493555]
 [0.2820034 ]
 [0.29367942]
 [0.35328999]
 [0.37171453]
 [2.94740701]
 [0.3156389 ]
 [0.28553802]
 [0.28553802]
 [1.35583222]
 [1.11155009]]
Epoch 11: at batch 1: Training dataset Loss=0.797, Batch Time=0.026
Epoch 21: at batch 1: Training dataset Loss=0.396, Batch Time=0.036
Epoch 31: at batch 1: Training dataset Loss=0.480, Batch Time=0.026
Epoch 41: at batch 1: Training dataset Loss=0.350, Batch Time=0.034
Epoch 51: at batch 1: Training dataset Loss=0.542, Batch Time=0.027
Epoch 61: at batch 1: Training dataset Loss=0.325, Batch Time=0.024
Epoch 71: at batch 1: Training dataset Loss=0.446, Batch Time=0.032
Epoch 81: at batch 1: Training dataset Loss=0.291, Batch Time=0.027
Epoch 91: at batch 1: Training dataset Loss=0.326, Batch Time=0.030
Epoch 101: at batch 1: Training dataset Loss=0.350, Batch Time=0.029
Epoch 101: Epoch time = 42.997, Avg epoch time=0.278, Total Time=0.422

[[0.5       ]
 [0.125     ]
 [0.23391508]
 [0.19484322]
 [0.375     ]
 [0.25      ]
 [0.41595039]
 [0.3125    ]
 [0.16870707]
 [0.32155824]
 [0.16870707]
 [0.3125    ]
 [0.19484322]
 [0.26230016]
 [0.41595039]
 [0.3125    ]
 [0.16870707]
 [0.16870707]
 [0.3125    ]
 [0.3125    ]]
Epoch 111: at batch 1: Training dataset Loss=0.378, Batch Time=0.033
Epoch 121: at batch 1: Training dataset Loss=0.305, Batch Time=0.028
Epoch 131: at batch 1: Training dataset Loss=0.347, Batch Time=0.031
Epoch 141: at batch 1: Training dataset Loss=0.386, Batch Time=0.024
Epoch 151: at batch 1: Training dataset Loss=0.349, Batch Time=0.026
Epoch 161: at batch 1: Training dataset Loss=0.352, Batch Time=0.035
Epoch 171: at batch 1: Training dataset Loss=0.325, Batch Time=0.024
Epoch 181: at batch 1: Training dataset Loss=0.323, Batch Time=0.024
Epoch 191: at batch 1: Training dataset Loss=0.307, Batch Time=0.023
Epoch 201: at batch 1: Training dataset Loss=0.347, Batch Time=0.025
Epoch 201: Epoch time = 83.767, Avg epoch time=0.264, Total Time=0.415

[[0.3125    ]
 [0.32424122]
 [0.625     ]
 [0.25      ]
 [0.25      ]
 [0.39544025]
 [0.25      ]
 [0.24893749]
 [0.625     ]
 [0.375     ]
 [0.33421758]
 [0.3094503 ]
 [0.24893749]
 [0.5       ]
 [0.25      ]
 [0.39544025]
 [0.5       ]
 [0.625     ]
 [0.25      ]
 [0.4375    ]]
Epoch 211: at batch 1: Training dataset Loss=0.379, Batch Time=0.032
Epoch 221: at batch 1: Training dataset Loss=0.335, Batch Time=0.034
Epoch 231: at batch 1: Training dataset Loss=0.318, Batch Time=0.028
Epoch 241: at batch 1: Training dataset Loss=0.317, Batch Time=0.023
Epoch 251: at batch 1: Training dataset Loss=0.295, Batch Time=0.035
Epoch 261: at batch 1: Training dataset Loss=0.368, Batch Time=0.032
Epoch 271: at batch 1: Training dataset Loss=0.366, Batch Time=0.033
Epoch 281: at batch 1: Training dataset Loss=0.295, Batch Time=0.032
Epoch 291: at batch 1: Training dataset Loss=0.335, Batch Time=0.026
Epoch 301: at batch 1: Training dataset Loss=0.324, Batch Time=0.024
Epoch 301: Epoch time = 124.748, Avg epoch time=0.279, Total Time=0.413

[[0.375     ]
 [0.375     ]
 [0.5       ]
 [0.2995798 ]
 [0.3125    ]
 [0.2995798 ]
 [0.25      ]
 [0.32770279]
 [0.32770279]
 [0.1717023 ]
 [0.375     ]
 [0.375     ]
 [0.4375    ]
 [0.5       ]
 [0.3406496 ]
 [0.5625    ]
 [0.375     ]
 [0.3125    ]
 [0.5       ]
 [0.23077254]]
Epoch 311: at batch 1: Training dataset Loss=0.339, Batch Time=0.030
Epoch 321: at batch 1: Training dataset Loss=0.306, Batch Time=0.029
Epoch 331: at batch 1: Training dataset Loss=0.308, Batch Time=0.029
Epoch 341: at batch 1: Training dataset Loss=0.319, Batch Time=0.035
Epoch 351: at batch 1: Training dataset Loss=0.311, Batch Time=0.032
Epoch 361: at batch 1: Training dataset Loss=0.333, Batch Time=0.032
Epoch 371: at batch 1: Training dataset Loss=0.352, Batch Time=0.032
Epoch 381: at batch 1: Training dataset Loss=0.323, Batch Time=0.029
Epoch 391: at batch 1: Training dataset Loss=0.341, Batch Time=0.028
Epoch 401: at batch 1: Training dataset Loss=0.338, Batch Time=0.029
Epoch 401: Epoch time = 165.310, Avg epoch time=0.273, Total Time=0.411

[[0.29447207]
 [0.3125    ]
 [0.3125    ]
 [0.5625    ]
 [0.375     ]
 [0.375     ]
 [0.3125    ]
 [0.25      ]
 [0.20367068]
 [0.3125    ]
 [0.375     ]
 [0.43104464]
 [0.1875    ]
 [0.1875    ]
 [0.3125    ]
 [0.375     ]
 [0.3125    ]
 [0.375     ]
 [0.29447207]
 [0.5625    ]]
Epoch 411: at batch 1: Training dataset Loss=0.293, Batch Time=0.032
Epoch 421: at batch 1: Training dataset Loss=0.362, Batch Time=0.032
Epoch 431: at batch 1: Training dataset Loss=0.283, Batch Time=0.031
Epoch 441: at batch 1: Training dataset Loss=0.347, Batch Time=0.027
Epoch 451: at batch 1: Training dataset Loss=0.337, Batch Time=0.031
Epoch 461: at batch 1: Training dataset Loss=0.335, Batch Time=0.034
Epoch 471: at batch 1: Training dataset Loss=0.269, Batch Time=0.026
Epoch 481: at batch 1: Training dataset Loss=0.320, Batch Time=0.028
Epoch 491: at batch 1: Training dataset Loss=0.316, Batch Time=0.029
Epoch 501: at batch 1: Training dataset Loss=0.343, Batch Time=0.023
Epoch 501: Epoch time = 205.797, Avg epoch time=0.272, Total Time=0.410

[[0.25906792]
 [0.4861331 ]
 [0.28761441]
 [0.26464844]
 [0.4861331 ]
 [0.4375    ]
 [0.4861331 ]
 [0.4861331 ]
 [0.3125    ]
 [0.40898106]
 [0.125     ]
 [0.375     ]
 [0.4375    ]
 [0.4375    ]
 [0.26464844]
 [0.5       ]
 [0.125     ]
 [0.24382037]
 [0.26464844]
 [0.25      ]]
Epoch 511: at batch 1: Training dataset Loss=0.306, Batch Time=0.034
Epoch 521: at batch 1: Training dataset Loss=0.302, Batch Time=0.024
Epoch 531: at batch 1: Training dataset Loss=0.298, Batch Time=0.024
Epoch 541: at batch 1: Training dataset Loss=0.360, Batch Time=0.032
Epoch 551: at batch 1: Training dataset Loss=0.329, Batch Time=0.030
Epoch 561: at batch 1: Training dataset Loss=0.308, Batch Time=0.024
Epoch 571: at batch 1: Training dataset Loss=0.344, Batch Time=0.033
Epoch 581: at batch 1: Training dataset Loss=0.333, Batch Time=0.031
Epoch 591: at batch 1: Training dataset Loss=0.311, Batch Time=0.029
Epoch 601: at batch 1: Training dataset Loss=0.366, Batch Time=0.023
Epoch 601: Epoch time = 246.497, Avg epoch time=0.258, Total Time=0.409

[[0.3125    ]
 [0.27562144]
 [0.375     ]
 [0.375     ]
 [0.375     ]
 [0.4375    ]
 [0.36687082]
 [0.5       ]
 [0.1875    ]
 [0.3125    ]
 [0.3125    ]
 [0.3125    ]
 [0.3125    ]
 [0.3125    ]
 [0.3125    ]
 [0.3125    ]
 [0.3125    ]
 [0.375     ]
 [0.375     ]
 [0.5       ]]
Epoch 611: at batch 1: Training dataset Loss=0.329, Batch Time=0.029
Epoch 621: at batch 1: Training dataset Loss=0.324, Batch Time=0.032
Epoch 631: at batch 1: Training dataset Loss=0.285, Batch Time=0.031
Epoch 641: at batch 1: Training dataset Loss=0.353, Batch Time=0.028
Epoch 651: at batch 1: Training dataset Loss=0.317, Batch Time=0.028
Epoch 661: at batch 1: Training dataset Loss=0.312, Batch Time=0.026
Epoch 671: at batch 1: Training dataset Loss=0.336, Batch Time=0.029
Epoch 681: at batch 1: Training dataset Loss=0.300, Batch Time=0.029
Epoch 691: at batch 1: Training dataset Loss=0.302, Batch Time=0.035
Epoch 701: at batch 1: Training dataset Loss=0.324, Batch Time=0.028
Epoch 701: Epoch time = 287.370, Avg epoch time=0.276, Total Time=0.409

[[0.3125    ]
 [0.30450901]
 [0.5       ]
 [0.3159973 ]
 [0.375     ]
 [0.5       ]
 [0.3125    ]
 [0.3125    ]
 [0.4375    ]
 [0.375     ]
 [0.375     ]
 [0.4375    ]
 [0.1875    ]
 [0.29199299]
 [0.4375    ]
 [0.21948206]
 [0.3125    ]
 [0.30812317]
 [0.3159973 ]
 [0.375     ]]
Epoch 711: at batch 1: Training dataset Loss=0.327, Batch Time=0.032
Epoch 721: at batch 1: Training dataset Loss=0.338, Batch Time=0.027
Epoch 731: at batch 1: Training dataset Loss=0.362, Batch Time=0.028
Epoch 741: at batch 1: Training dataset Loss=0.336, Batch Time=0.025
Epoch 751: at batch 1: Training dataset Loss=0.373, Batch Time=0.023
Epoch 761: at batch 1: Training dataset Loss=0.321, Batch Time=0.024
Epoch 771: at batch 1: Training dataset Loss=0.329, Batch Time=0.032
Epoch 781: at batch 1: Training dataset Loss=0.339, Batch Time=0.031
Epoch 791: at batch 1: Training dataset Loss=0.317, Batch Time=0.026
Epoch 801: at batch 1: Training dataset Loss=0.358, Batch Time=0.033
Epoch 801: Epoch time = 328.190, Avg epoch time=0.282, Total Time=0.409

[[0.21155103]
 [0.3125    ]
 [0.14561853]
 [0.28880185]
 [0.5       ]
 [0.28246969]
 [0.28246969]
 [0.3125    ]
 [0.20248914]
 [0.14561853]
 [0.5492571 ]
 [0.25      ]
 [0.52528083]
 [0.21155103]
 [0.14561853]
 [0.28880185]
 [0.52528083]
 [0.25      ]
 [0.28246969]
 [0.5625    ]]
Epoch 811: at batch 1: Training dataset Loss=0.309, Batch Time=0.029
Epoch 821: at batch 1: Training dataset Loss=0.276, Batch Time=0.026
Epoch 831: at batch 1: Training dataset Loss=0.552, Batch Time=0.034
Epoch 841: at batch 1: Training dataset Loss=0.326, Batch Time=0.023
Epoch 851: at batch 1: Training dataset Loss=0.350, Batch Time=0.024
Epoch 861: at batch 1: Training dataset Loss=0.320, Batch Time=0.034
Epoch 871: at batch 1: Training dataset Loss=0.286, Batch Time=0.026
Epoch 881: at batch 1: Training dataset Loss=0.295, Batch Time=0.030
Epoch 891: at batch 1: Training dataset Loss=0.309, Batch Time=0.028
Epoch 901: at batch 1: Training dataset Loss=0.258, Batch Time=0.026
Epoch 901: Epoch time = 368.737, Avg epoch time=0.261, Total Time=0.409

[[0.23139724]
 [0.4375    ]
 [0.4375    ]
 [0.66205692]
 [0.27223548]
 [0.375     ]
 [0.3125    ]
 [0.4375    ]
 [0.3125    ]
 [0.27223548]
 [0.375     ]
 [0.29561237]
 [0.375     ]
 [0.25480843]
 [0.23139724]
 [0.25480843]
 [0.1875    ]
 [0.1875    ]
 [0.25      ]
 [0.4375    ]]
Epoch 911: at batch 1: Training dataset Loss=0.340, Batch Time=0.031
Epoch 921: at batch 1: Training dataset Loss=0.310, Batch Time=0.025
Epoch 931: at batch 1: Training dataset Loss=0.310, Batch Time=0.034
Epoch 941: at batch 1: Training dataset Loss=0.329, Batch Time=0.029
Epoch 951: at batch 1: Training dataset Loss=0.337, Batch Time=0.029
Epoch 961: at batch 1: Training dataset Loss=0.284, Batch Time=0.026
Epoch 971: at batch 1: Training dataset Loss=0.368, Batch Time=0.028
Epoch 981: at batch 1: Training dataset Loss=0.359, Batch Time=0.031
Epoch 991: at batch 1: Training dataset Loss=0.330, Batch Time=0.034
Epoch 1001: at batch 1: Training dataset Loss=0.377, Batch Time=0.026
Epoch 1001: Epoch time = 409.308, Avg epoch time=0.271, Total Time=0.408

[[0.1875    ]
 [0.375     ]
 [0.375     ]
 [0.375     ]
 [0.30730328]
 [0.5       ]
 [0.4375    ]
 [0.4405672 ]
 [0.4375    ]
 [0.19284026]
 [0.19284026]
 [0.625     ]
 [0.26865196]
 [0.22451317]
 [0.26865196]
 [0.375     ]
 [0.4375    ]
 [0.375     ]
 [0.21807766]
 [0.26865196]]
Epoch 1011: at batch 1: Training dataset Loss=0.322, Batch Time=0.026
Epoch 1021: at batch 1: Training dataset Loss=0.349, Batch Time=0.031
Epoch 1031: at batch 1: Training dataset Loss=0.346, Batch Time=0.032
Epoch 1041: at batch 1: Training dataset Loss=0.324, Batch Time=0.024
Epoch 1051: at batch 1: Training dataset Loss=0.302, Batch Time=0.027
Epoch 1061: at batch 1: Training dataset Loss=0.348, Batch Time=0.029
Epoch 1071: at batch 1: Training dataset Loss=0.303, Batch Time=0.035
Epoch 1081: at batch 1: Training dataset Loss=0.338, Batch Time=0.031
Epoch 1091: at batch 1: Training dataset Loss=0.336, Batch Time=0.027
Epoch 1101: at batch 1: Training dataset Loss=0.310, Batch Time=0.027
Epoch 1101: Epoch time = 449.932, Avg epoch time=0.278, Total Time=0.408

[[0.21564616]
 [0.46478045]
 [0.46478045]
 [0.3125    ]
 [0.3125    ]
 [0.46478045]
 [0.375     ]
 [0.375     ]
 [0.21564616]
 [0.375     ]
 [0.37150186]
 [0.37150186]
 [0.25      ]
 [0.33070365]
 [0.25      ]
 [0.24203134]
 [0.33070365]
 [0.3125    ]
 [0.3125    ]
 [0.3125    ]]
Epoch 1111: at batch 1: Training dataset Loss=0.337, Batch Time=0.032
Epoch 1121: at batch 1: Training dataset Loss=0.342, Batch Time=0.023
Epoch 1131: at batch 1: Training dataset Loss=0.330, Batch Time=0.026
Epoch 1141: at batch 1: Training dataset Loss=0.276, Batch Time=0.031
Epoch 1151: at batch 1: Training dataset Loss=0.360, Batch Time=0.027
Epoch 1161: at batch 1: Training dataset Loss=0.323, Batch Time=0.033
Epoch 1171: at batch 1: Training dataset Loss=0.290, Batch Time=0.035
Epoch 1181: at batch 1: Training dataset Loss=0.293, Batch Time=0.025
Epoch 1191: at batch 1: Training dataset Loss=0.347, Batch Time=0.024
Epoch 1201: at batch 1: Training dataset Loss=0.263, Batch Time=0.026
Epoch 1201: Epoch time = 490.604, Avg epoch time=0.298, Total Time=0.408

[[0.21716517]
 [0.21716517]
 [0.21716517]
 [0.15040374]
 [0.125     ]
 [0.24673635]
 [0.39417389]
 [0.375     ]
 [0.3125    ]
 [0.21716517]
 [0.22007328]
 [0.33112502]
 [0.39417389]
 [0.1875    ]
 [0.21716517]
 [0.30439386]
 [0.39417389]
 [0.30439386]
 [0.3125    ]
 [0.25      ]]
Epoch 1211: at batch 1: Training dataset Loss=0.285, Batch Time=0.024
Epoch 1221: at batch 1: Training dataset Loss=0.347, Batch Time=0.030
Epoch 1231: at batch 1: Training dataset Loss=0.289, Batch Time=0.024
Epoch 1241: at batch 1: Training dataset Loss=0.288, Batch Time=0.026
Epoch 1251: at batch 1: Training dataset Loss=0.309, Batch Time=0.029
Epoch 1261: at batch 1: Training dataset Loss=0.267, Batch Time=0.035
Epoch 1271: at batch 1: Training dataset Loss=0.363, Batch Time=0.026
Epoch 1281: at batch 1: Training dataset Loss=0.282, Batch Time=0.028
Epoch 1291: at batch 1: Training dataset Loss=0.306, Batch Time=0.023
Epoch 1301: at batch 1: Training dataset Loss=0.324, Batch Time=0.032
Epoch 1301: Epoch time = 531.163, Avg epoch time=0.276, Total Time=0.408

[[0.3125    ]
 [0.26364905]
 [0.41596702]
 [0.3125    ]
 [0.375     ]
 [0.3902595 ]
 [0.42918733]
 [0.5       ]
 [0.39699084]
 [0.3125    ]
 [0.375     ]
 [0.375     ]
 [0.41596702]
 [0.26364905]
 [0.39699084]
 [0.29226029]
 [0.375     ]
 [0.3537868 ]
 [0.29226029]
 [0.4375    ]]
Epoch 1311: at batch 1: Training dataset Loss=0.296, Batch Time=0.032
Epoch 1321: at batch 1: Training dataset Loss=0.295, Batch Time=0.029
Epoch 1331: at batch 1: Training dataset Loss=0.348, Batch Time=0.032
Epoch 1341: at batch 1: Training dataset Loss=0.305, Batch Time=0.028
Epoch 1351: at batch 1: Training dataset Loss=0.311, Batch Time=0.032
Epoch 1361: at batch 1: Training dataset Loss=0.325, Batch Time=0.033
Epoch 1371: at batch 1: Training dataset Loss=0.304, Batch Time=0.034
Epoch 1381: at batch 1: Training dataset Loss=0.318, Batch Time=0.026
Epoch 1391: at batch 1: Training dataset Loss=0.302, Batch Time=0.026
Epoch 1401: at batch 1: Training dataset Loss=0.306, Batch Time=0.032
Epoch 1401: Epoch time = 571.973, Avg epoch time=0.274, Total Time=0.408

[[0.3125    ]
 [0.3125    ]
 [0.5       ]
 [0.29410291]
 [0.25      ]
 [0.3125    ]
 [0.33411232]
 [0.125     ]
 [0.3125    ]
 [0.5       ]
 [0.4375    ]
 [0.3125    ]
 [0.3125    ]
 [0.3125    ]
 [0.22182788]
 [0.42902613]
 [0.3125    ]
 [0.5       ]
 [0.3125    ]
 [0.28226084]]
Epoch 1411: at batch 1: Training dataset Loss=0.253, Batch Time=0.025
Epoch 1421: at batch 1: Training dataset Loss=0.320, Batch Time=0.026
Epoch 1431: at batch 1: Training dataset Loss=0.319, Batch Time=0.024
Epoch 1441: at batch 1: Training dataset Loss=0.326, Batch Time=0.029
Epoch 1451: at batch 1: Training dataset Loss=0.330, Batch Time=0.031
Epoch 1461: at batch 1: Training dataset Loss=0.301, Batch Time=0.029
Epoch 1471: at batch 1: Training dataset Loss=0.283, Batch Time=0.034
Epoch 1481: at batch 1: Training dataset Loss=0.308, Batch Time=0.032
Epoch 1491: at batch 1: Training dataset Loss=0.300, Batch Time=0.030
Epoch 1501: at batch 1: Training dataset Loss=0.295, Batch Time=0.027
Epoch 1501: Epoch time = 612.751, Avg epoch time=0.253, Total Time=0.408

[[0.1875    ]
 [0.26925826]
 [0.3125    ]
 [0.27452347]
 [0.3125    ]
 [0.34902683]
 [0.32227117]
 [0.3125    ]
 [0.20519766]
 [0.27452347]
 [0.29958642]
 [0.3125    ]
 [0.3125    ]
 [0.26925826]
 [0.1875    ]
 [0.5625    ]
 [0.2579301 ]
 [0.375     ]
 [0.26925826]
 [0.3125    ]]
Epoch 1511: at batch 1: Training dataset Loss=0.270, Batch Time=0.033
Epoch 1521: at batch 1: Training dataset Loss=0.271, Batch Time=0.023
Epoch 1531: at batch 1: Training dataset Loss=0.303, Batch Time=0.029
Epoch 1541: at batch 1: Training dataset Loss=0.276, Batch Time=0.032
Epoch 1551: at batch 1: Training dataset Loss=0.298, Batch Time=0.032
Epoch 1561: at batch 1: Training dataset Loss=0.300, Batch Time=0.032
Epoch 1571: at batch 1: Training dataset Loss=0.307, Batch Time=0.031
Epoch 1581: at batch 1: Training dataset Loss=0.293, Batch Time=0.025
Epoch 1591: at batch 1: Training dataset Loss=0.305, Batch Time=0.024
Epoch 1601: at batch 1: Training dataset Loss=0.285, Batch Time=0.030
Epoch 1601: Epoch time = 653.685, Avg epoch time=0.283, Total Time=0.408

[[0.125     ]
 [0.17527574]
 [0.4375    ]
 [0.375     ]
 [0.31521514]
 [0.375     ]
 [0.21151206]
 [0.3125    ]
 [0.125     ]
 [0.24647966]
 [0.25      ]
 [0.22624847]
 [0.25      ]
 [0.375     ]
 [0.24647966]
 [0.20780212]
 [0.25      ]
 [0.3125    ]
 [0.23004085]
 [0.3125    ]]
Epoch 1611: at batch 1: Training dataset Loss=0.294, Batch Time=0.029
Epoch 1621: at batch 1: Training dataset Loss=0.338, Batch Time=0.028
Epoch 1631: at batch 1: Training dataset Loss=0.287, Batch Time=0.033
Epoch 1641: at batch 1: Training dataset Loss=0.269, Batch Time=0.026
Epoch 1651: at batch 1: Training dataset Loss=0.329, Batch Time=0.025
Epoch 1661: at batch 1: Training dataset Loss=0.308, Batch Time=0.031
Epoch 1671: at batch 1: Training dataset Loss=0.286, Batch Time=0.026
Epoch 1681: at batch 1: Training dataset Loss=0.305, Batch Time=0.030
Epoch 1691: at batch 1: Training dataset Loss=0.302, Batch Time=0.031
Epoch 1701: at batch 1: Training dataset Loss=0.270, Batch Time=0.023
Epoch 1701: Epoch time = 694.204, Avg epoch time=0.267, Total Time=0.408

[[0.2920137 ]
 [0.3125    ]
 [0.5       ]
 [0.23436093]
 [0.21998492]
 [0.23436093]
 [0.21998492]
 [0.2950989 ]
 [0.3125    ]
 [0.15445948]
 [0.15445948]
 [0.4375    ]
 [0.3125    ]
 [0.2950989 ]
 [0.3125    ]
 [0.23436093]
 [0.15445948]
 [0.4375    ]
 [0.23436093]
 [0.3125    ]]
Epoch 1711: at batch 1: Training dataset Loss=0.334, Batch Time=0.026
Epoch 1721: at batch 1: Training dataset Loss=0.313, Batch Time=0.025
Epoch 1731: at batch 1: Training dataset Loss=0.291, Batch Time=0.031
Epoch 1741: at batch 1: Training dataset Loss=0.367, Batch Time=0.031
Epoch 1751: at batch 1: Training dataset Loss=0.290, Batch Time=0.026
Epoch 1761: at batch 1: Training dataset Loss=0.303, Batch Time=0.028
Epoch 1771: at batch 1: Training dataset Loss=0.309, Batch Time=0.030
Epoch 1781: at batch 1: Training dataset Loss=0.310, Batch Time=0.032
Epoch 1791: at batch 1: Training dataset Loss=0.292, Batch Time=0.029
Epoch 1801: at batch 1: Training dataset Loss=0.320, Batch Time=0.029
Epoch 1801: Epoch time = 735.552, Avg epoch time=0.267, Total Time=0.408

[[0.14869773]
 [0.14869773]
 [0.43393496]
 [0.25      ]
 [0.36336711]
 [0.25      ]
 [0.3479175 ]
 [0.3613959 ]
 [0.14869773]
 [0.28090817]
 [0.25      ]
 [0.3479175 ]
 [0.36042461]
 [0.22327706]
 [0.14869773]
 [0.36042461]
 [0.17176066]
 [0.3479175 ]
 [0.23461376]
 [0.3479175 ]]
Epoch 1811: at batch 1: Training dataset Loss=0.293, Batch Time=0.029
Epoch 1821: at batch 1: Training dataset Loss=0.288, Batch Time=0.029
Epoch 1831: at batch 1: Training dataset Loss=0.281, Batch Time=0.023
Epoch 1841: at batch 1: Training dataset Loss=0.310, Batch Time=0.026
Epoch 1851: at batch 1: Training dataset Loss=0.296, Batch Time=0.025
Epoch 1861: at batch 1: Training dataset Loss=0.318, Batch Time=0.032
Epoch 1871: at batch 1: Training dataset Loss=0.262, Batch Time=0.032
Epoch 1881: at batch 1: Training dataset Loss=0.292, Batch Time=0.034
Epoch 1891: at batch 1: Training dataset Loss=0.377, Batch Time=0.026
Epoch 1901: at batch 1: Training dataset Loss=0.318, Batch Time=0.029
Epoch 1901: Epoch time = 776.296, Avg epoch time=0.247, Total Time=0.408

[[0.375     ]
 [0.375     ]
 [0.5       ]
 [0.3125    ]
 [0.24261051]
 [0.19977844]
 [0.375     ]
 [0.3125    ]
 [0.20312846]
 [0.242347  ]
 [0.29218203]
 [0.32448149]
 [0.5       ]
 [0.22866987]
 [0.43753207]
 [0.24261051]
 [0.3125    ]
 [0.21011153]
 [0.24261051]
 [0.25598991]]
Epoch 1911: at batch 1: Training dataset Loss=0.257, Batch Time=0.029
Epoch 1921: at batch 1: Training dataset Loss=0.341, Batch Time=0.031
Epoch 1931: at batch 1: Training dataset Loss=0.291, Batch Time=0.032
Epoch 1941: at batch 1: Training dataset Loss=0.307, Batch Time=0.025
Epoch 1951: at batch 1: Training dataset Loss=0.250, Batch Time=0.044
Epoch 1961: at batch 1: Training dataset Loss=0.383, Batch Time=0.026
Epoch 1971: at batch 1: Training dataset Loss=0.276, Batch Time=0.034
Epoch 1981: at batch 1: Training dataset Loss=0.272, Batch Time=0.035
Epoch 1991: at batch 1: Training dataset Loss=0.267, Batch Time=0.024
Epoch 2001: at batch 1: Training dataset Loss=0.245, Batch Time=0.029
Epoch 2001: Epoch time = 817.053, Avg epoch time=0.291, Total Time=0.408

[[0.27162927]
 [0.27597803]
 [0.17016679]
 [0.1875    ]
 [0.3125    ]
 [0.4375    ]
 [0.28128672]
 [0.3125    ]
 [0.20284234]
 [0.28140923]
 [0.38198605]
 [0.26879954]
 [0.26682812]
 [0.28140923]
 [0.17016679]
 [0.3125    ]
 [0.38198605]
 [0.28140923]
 [0.26682812]
 [0.26879954]]
Epoch 2011: at batch 1: Training dataset Loss=0.271, Batch Time=0.026
Epoch 2021: at batch 1: Training dataset Loss=0.269, Batch Time=0.032
Epoch 2031: at batch 1: Training dataset Loss=0.295, Batch Time=0.035
Epoch 2041: at batch 1: Training dataset Loss=0.291, Batch Time=0.031
Epoch 2051: at batch 1: Training dataset Loss=0.274, Batch Time=0.023
Epoch 2061: at batch 1: Training dataset Loss=0.288, Batch Time=0.032
Epoch 2071: at batch 1: Training dataset Loss=0.258, Batch Time=0.026
Epoch 2081: at batch 1: Training dataset Loss=0.274, Batch Time=0.034
Epoch 2091: at batch 1: Training dataset Loss=0.286, Batch Time=0.033
Epoch 2101: at batch 1: Training dataset Loss=0.278, Batch Time=0.026
Epoch 2101: Epoch time = 857.864, Avg epoch time=0.268, Total Time=0.408

[[0.4293566 ]
 [0.19869505]
 [0.3125    ]
 [0.30090809]
 [0.4293566 ]
 [0.26491591]
 [0.1875    ]
 [0.26491591]
 [0.1875    ]
 [0.19869505]
 [0.39265764]
 [0.29590309]
 [0.19869505]
 [0.4293566 ]
 [0.39265764]
 [0.26491591]
 [0.27137071]
 [0.3125    ]
 [0.22842817]
 [0.5       ]]
Epoch 2111: at batch 1: Training dataset Loss=0.267, Batch Time=0.032
Epoch 2121: at batch 1: Training dataset Loss=0.262, Batch Time=0.024
Epoch 2131: at batch 1: Training dataset Loss=0.333, Batch Time=0.026
Epoch 2141: at batch 1: Training dataset Loss=0.319, Batch Time=0.033
Epoch 2151: at batch 1: Training dataset Loss=0.308, Batch Time=0.031
Epoch 2161: at batch 1: Training dataset Loss=0.309, Batch Time=0.032
Epoch 2171: at batch 1: Training dataset Loss=0.300, Batch Time=0.029
Epoch 2181: at batch 1: Training dataset Loss=0.249, Batch Time=0.032
Epoch 2191: at batch 1: Training dataset Loss=0.271, Batch Time=0.030
Epoch 2201: at batch 1: Training dataset Loss=0.257, Batch Time=0.026
Epoch 2201: Epoch time = 898.615, Avg epoch time=0.269, Total Time=0.408

[[0.125     ]
 [0.29857242]
 [0.5       ]
 [0.3006967 ]
 [0.3006967 ]
 [0.1875    ]
 [0.5       ]
 [0.20389384]
 [0.38626456]
 [0.38743556]
 [0.27958089]
 [0.20389384]
 [0.5114398 ]
 [0.32872733]
 [0.22457674]
 [0.38626456]
 [0.5114398 ]
 [0.38743556]
 [0.125     ]
 [0.5114398 ]]
Epoch 2211: at batch 1: Training dataset Loss=0.321, Batch Time=0.031
Epoch 2221: at batch 1: Training dataset Loss=0.293, Batch Time=0.026
Epoch 2231: at batch 1: Training dataset Loss=0.260, Batch Time=0.023
Epoch 2241: at batch 1: Training dataset Loss=0.303, Batch Time=0.029
Epoch 2251: at batch 1: Training dataset Loss=0.348, Batch Time=0.029
Epoch 2261: at batch 1: Training dataset Loss=0.292, Batch Time=0.023
Epoch 2271: at batch 1: Training dataset Loss=0.289, Batch Time=0.026
Epoch 2281: at batch 1: Training dataset Loss=0.298, Batch Time=0.023
Epoch 2291: at batch 1: Training dataset Loss=0.264, Batch Time=0.023
Epoch 2301: at batch 1: Training dataset Loss=0.299, Batch Time=0.029
Epoch 2301: Epoch time = 939.260, Avg epoch time=0.285, Total Time=0.408

[[0.3125    ]
 [0.25215745]
 [0.32679784]
 [0.2774443 ]
 [0.30017167]
 [0.4375    ]
 [0.33901888]
 [0.19245392]
 [0.32679784]
 [0.5       ]
 [0.30017167]
 [0.28231341]
 [0.26565722]
 [0.5       ]
 [0.32679784]
 [0.58337361]
 [0.31244743]
 [0.27296025]
 [0.17731571]
 [0.25      ]]
Epoch 2311: at batch 1: Training dataset Loss=0.287, Batch Time=0.028
Epoch 2321: at batch 1: Training dataset Loss=0.300, Batch Time=0.029
Epoch 2331: at batch 1: Training dataset Loss=0.290, Batch Time=0.033
Epoch 2341: at batch 1: Training dataset Loss=0.286, Batch Time=0.026
Epoch 2351: at batch 1: Training dataset Loss=0.285, Batch Time=0.032
Epoch 2361: at batch 1: Training dataset Loss=0.306, Batch Time=0.029
Epoch 2371: at batch 1: Training dataset Loss=0.271, Batch Time=0.031
Epoch 2381: at batch 1: Training dataset Loss=0.307, Batch Time=0.029
Epoch 2391: at batch 1: Training dataset Loss=0.299, Batch Time=0.033
Epoch 2401: at batch 1: Training dataset Loss=0.283, Batch Time=0.029
Epoch 2401: Epoch time = 980.094, Avg epoch time=0.276, Total Time=0.408

[[0.21259761]
 [0.37713134]
 [0.25      ]
 [0.18448803]
 [0.3125    ]
 [0.34851953]
 [0.25005341]
 [0.21406958]
 [0.34851953]
 [0.18448803]
 [0.26460418]
 [0.21259761]
 [0.25      ]
 [0.2807321 ]
 [0.18448803]
 [0.36805061]
 [0.25      ]
 [0.25      ]
 [0.34332019]
 [0.25      ]]
Epoch 2411: at batch 1: Training dataset Loss=0.304, Batch Time=0.030
Epoch 2421: at batch 1: Training dataset Loss=0.281, Batch Time=0.030
Epoch 2431: at batch 1: Training dataset Loss=0.302, Batch Time=0.033
Epoch 2441: at batch 1: Training dataset Loss=0.277, Batch Time=0.026
Epoch 2451: at batch 1: Training dataset Loss=0.290, Batch Time=0.023
Epoch 2461: at batch 1: Training dataset Loss=0.284, Batch Time=0.034
Epoch 2471: at batch 1: Training dataset Loss=0.266, Batch Time=0.029
Epoch 2481: at batch 1: Training dataset Loss=0.300, Batch Time=0.031
Epoch 2491: at batch 1: Training dataset Loss=0.278, Batch Time=0.032
Epoch 2501: at batch 1: Training dataset Loss=0.285, Batch Time=0.035
Epoch 2501: Epoch time = 1020.978, Avg epoch time=0.280, Total Time=0.408

[[0.16454899]
 [0.26343209]
 [0.29362416]
 [0.26343209]
 [0.5       ]
 [0.28158551]
 [0.17564756]
 [0.29274821]
 [0.3125    ]
 [0.16921742]
 [0.29362416]
 [0.5       ]
 [0.5       ]
 [0.375     ]
 [0.15403259]
 [0.34476742]
 [0.29362416]
 [0.26343209]
 [0.31624174]
 [0.5       ]]
Epoch 2511: at batch 1: Training dataset Loss=0.282, Batch Time=0.029
Epoch 2521: at batch 1: Training dataset Loss=0.268, Batch Time=0.029
Epoch 2531: at batch 1: Training dataset Loss=0.285, Batch Time=0.030
Epoch 2541: at batch 1: Training dataset Loss=0.329, Batch Time=0.032
Epoch 2551: at batch 1: Training dataset Loss=0.309, Batch Time=0.025
Epoch 2561: at batch 1: Training dataset Loss=0.300, Batch Time=0.024
Epoch 2571: at batch 1: Training dataset Loss=0.307, Batch Time=0.024
Epoch 2581: at batch 1: Training dataset Loss=0.288, Batch Time=0.029
Epoch 2591: at batch 1: Training dataset Loss=0.270, Batch Time=0.028
Epoch 2601: at batch 1: Training dataset Loss=0.270, Batch Time=0.032
Epoch 2601: Epoch time = 1061.769, Avg epoch time=0.265, Total Time=0.408

[[0.1875    ]
 [0.24976918]
 [0.32225642]
 [0.25      ]
 [0.23948468]
 [0.25      ]
 [0.25482258]
 [0.15006818]
 [0.21474072]
 [0.15006818]
 [0.25      ]
 [0.24976918]
 [0.23948468]
 [0.15006818]
 [0.18543449]
 [0.31398031]
 [0.31398031]
 [0.17868182]
 [0.17493418]
 [0.21474072]]
Epoch 2611: at batch 1: Training dataset Loss=0.311, Batch Time=0.026
Epoch 2621: at batch 1: Training dataset Loss=0.333, Batch Time=0.026
Epoch 2631: at batch 1: Training dataset Loss=0.276, Batch Time=0.026
Epoch 2641: at batch 1: Training dataset Loss=0.276, Batch Time=0.031
Epoch 2651: at batch 1: Training dataset Loss=0.282, Batch Time=0.025
Epoch 2661: at batch 1: Training dataset Loss=0.290, Batch Time=0.025
Epoch 2671: at batch 1: Training dataset Loss=0.313, Batch Time=0.032
Epoch 2681: at batch 1: Training dataset Loss=0.297, Batch Time=0.033
Epoch 2691: at batch 1: Training dataset Loss=0.316, Batch Time=0.024
Epoch 2701: at batch 1: Training dataset Loss=0.312, Batch Time=0.028
Epoch 2701: Epoch time = 1102.333, Avg epoch time=0.297, Total Time=0.408

[[0.33474213]
 [0.25567776]
 [0.375     ]
 [0.375     ]
 [0.20917809]
 [0.28017718]
 [0.28017718]
 [0.200938  ]
 [0.15826292]
 [0.10832344]
 [0.15826292]
 [0.3125    ]
 [0.3125    ]
 [0.33474213]
 [0.33474213]
 [0.34899634]
 [0.20917809]
 [0.20917809]
 [0.10832344]
 [0.25567776]]
Epoch 2711: at batch 1: Training dataset Loss=0.299, Batch Time=0.026
Epoch 2721: at batch 1: Training dataset Loss=0.309, Batch Time=0.032
Epoch 2731: at batch 1: Training dataset Loss=0.272, Batch Time=0.034
Epoch 2741: at batch 1: Training dataset Loss=0.252, Batch Time=0.032
Epoch 2751: at batch 1: Training dataset Loss=0.254, Batch Time=0.023
Epoch 2761: at batch 1: Training dataset Loss=0.294, Batch Time=0.029
Epoch 2771: at batch 1: Training dataset Loss=0.321, Batch Time=0.025
Epoch 2781: at batch 1: Training dataset Loss=0.323, Batch Time=0.033
Epoch 2791: at batch 1: Training dataset Loss=0.251, Batch Time=0.030
Epoch 2801: at batch 1: Training dataset Loss=0.314, Batch Time=0.032
Epoch 2801: Epoch time = 1143.172, Avg epoch time=0.277, Total Time=0.408

[[0.16555434]
 [0.20204554]
 [0.18533048]
 [0.375     ]
 [0.30069563]
 [0.3125    ]
 [0.33433366]
 [0.18133989]
 [0.21723078]
 [0.36010852]
 [0.36010852]
 [0.28028286]
 [0.18533048]
 [0.20204554]
 [0.18133989]
 [0.30529338]
 [0.18533048]
 [0.23863456]
 [0.30529338]
 [0.27558699]]
Epoch 2811: at batch 1: Training dataset Loss=0.316, Batch Time=0.027
Epoch 2821: at batch 1: Training dataset Loss=0.281, Batch Time=0.035
Epoch 2831: at batch 1: Training dataset Loss=0.295, Batch Time=0.023
Epoch 2841: at batch 1: Training dataset Loss=0.292, Batch Time=0.028
Epoch 2851: at batch 1: Training dataset Loss=0.309, Batch Time=0.023
Epoch 2861: at batch 1: Training dataset Loss=0.304, Batch Time=0.032
Epoch 2871: at batch 1: Training dataset Loss=0.272, Batch Time=0.026
Epoch 2881: at batch 1: Training dataset Loss=0.304, Batch Time=0.031
Epoch 2891: at batch 1: Training dataset Loss=0.309, Batch Time=0.034
Epoch 2901: at batch 1: Training dataset Loss=0.277, Batch Time=0.032
Epoch 2901: Epoch time = 1184.111, Avg epoch time=0.260, Total Time=0.408

[[0.16345227]
 [0.16345227]
 [0.33808565]
 [0.3016015 ]
 [0.51464933]
 [0.17915583]
 [0.24330422]
 [0.16345227]
 [0.3016015 ]
 [0.3125    ]
 [0.29705143]
 [0.35708797]
 [0.51464933]
 [0.24687126]
 [0.4375    ]
 [0.39535069]
 [0.375     ]
 [0.17915583]
 [0.30292356]
 [0.51464933]]
Epoch 2911: at batch 1: Training dataset Loss=0.283, Batch Time=0.024
Epoch 2921: at batch 1: Training dataset Loss=0.281, Batch Time=0.029
Epoch 2931: at batch 1: Training dataset Loss=0.304, Batch Time=0.031
Epoch 2941: at batch 1: Training dataset Loss=0.295, Batch Time=0.029
Epoch 2951: at batch 1: Training dataset Loss=0.267, Batch Time=0.026
Epoch 2961: at batch 1: Training dataset Loss=0.282, Batch Time=0.032
Epoch 2971: at batch 1: Training dataset Loss=0.282, Batch Time=0.026
Epoch 2981: at batch 1: Training dataset Loss=0.330, Batch Time=0.031
Epoch 2991: at batch 1: Training dataset Loss=0.280, Batch Time=0.035
Epoch 3001: at batch 1: Training dataset Loss=0.261, Batch Time=0.034
Epoch 3001: Epoch time = 1224.743, Avg epoch time=0.269, Total Time=0.408

[[0.18956468]
 [0.20753357]
 [0.26458091]
 [0.30147052]
 [0.1875    ]
 [0.26458091]
 [0.25541052]
 [0.30853558]
 [0.5       ]
 [0.5625    ]
 [0.20753357]
 [0.375     ]
 [0.30908081]
 [0.5       ]
 [0.34674007]
 [0.27096298]
 [0.1875    ]
 [0.44326323]
 [0.27010065]
 [0.32311308]]
Epoch 3011: at batch 1: Training dataset Loss=0.287, Batch Time=0.028
Epoch 3021: at batch 1: Training dataset Loss=0.311, Batch Time=0.031
Epoch 3031: at batch 1: Training dataset Loss=0.302, Batch Time=0.026
Epoch 3041: at batch 1: Training dataset Loss=0.304, Batch Time=0.033
Epoch 3051: at batch 1: Training dataset Loss=0.295, Batch Time=0.035
Epoch 3061: at batch 1: Training dataset Loss=0.317, Batch Time=0.024
Epoch 3071: at batch 1: Training dataset Loss=0.269, Batch Time=0.025
Epoch 3081: at batch 1: Training dataset Loss=0.281, Batch Time=0.033
Epoch 3091: at batch 1: Training dataset Loss=0.268, Batch Time=0.033
Epoch 3101: at batch 1: Training dataset Loss=0.345, Batch Time=0.030
Epoch 3101: Epoch time = 1265.595, Avg epoch time=0.271, Total Time=0.408

[[0.5       ]
 [0.16309579]
 [0.34624258]
 [0.05833393]
 [0.05833393]
 [0.21088837]
 [0.125     ]
 [0.21529984]
 [0.21088837]
 [0.16309579]
 [0.19794179]
 [0.75      ]
 [0.27147749]
 [0.32006446]
 [0.125     ]
 [0.34289944]
 [0.5625    ]
 [0.05833393]
 [0.05833393]
 [0.33047646]]
Epoch 3111: at batch 1: Training dataset Loss=0.273, Batch Time=0.032
Epoch 3121: at batch 1: Training dataset Loss=0.280, Batch Time=0.030
Epoch 3131: at batch 1: Training dataset Loss=0.284, Batch Time=0.024
Epoch 3141: at batch 1: Training dataset Loss=0.302, Batch Time=0.025
Epoch 3151: at batch 1: Training dataset Loss=0.276, Batch Time=0.025
Epoch 3161: at batch 1: Training dataset Loss=0.276, Batch Time=0.034
Epoch 3171: at batch 1: Training dataset Loss=0.289, Batch Time=0.031
Epoch 3181: at batch 1: Training dataset Loss=0.265, Batch Time=0.035
Epoch 3191: at batch 1: Training dataset Loss=0.271, Batch Time=0.029
Epoch 3201: at batch 1: Training dataset Loss=0.294, Batch Time=0.026
Epoch 3201: Epoch time = 1306.201, Avg epoch time=0.273, Total Time=0.408

[[0.5       ]
 [0.17404456]
 [0.31273815]
 [0.4375    ]
 [0.20038828]
 [0.375     ]
 [0.36306483]
 [0.4375    ]
 [0.25      ]
 [0.2736313 ]
 [0.20038828]
 [0.27292421]
 [0.29986185]
 [0.3125    ]
 [0.4375    ]
 [0.36306483]
 [0.31273815]
 [0.10417282]
 [0.24823466]
 [0.43390095]]
Epoch 3211: at batch 1: Training dataset Loss=0.302, Batch Time=0.030
Epoch 3221: at batch 1: Training dataset Loss=0.300, Batch Time=0.026
Epoch 3231: at batch 1: Training dataset Loss=0.300, Batch Time=0.024
Epoch 3241: at batch 1: Training dataset Loss=0.281, Batch Time=0.033
Epoch 3251: at batch 1: Training dataset Loss=0.280, Batch Time=0.026
Epoch 3261: at batch 1: Training dataset Loss=0.288, Batch Time=0.034
Epoch 3271: at batch 1: Training dataset Loss=0.308, Batch Time=0.032
Epoch 3281: at batch 1: Training dataset Loss=0.286, Batch Time=0.025
Epoch 3291: at batch 1: Training dataset Loss=0.322, Batch Time=0.032
Epoch 3301: at batch 1: Training dataset Loss=0.292, Batch Time=0.033
Epoch 3301: Epoch time = 1347.162, Avg epoch time=0.291, Total Time=0.408

[[0.36600864]
 [0.22833213]
 [0.32453877]
 [0.4375    ]
 [0.4375    ]
 [0.4375    ]
 [0.29885009]
 [0.30722558]
 [0.4375    ]
 [0.16915359]
 [0.36600864]
 [0.28778285]
 [0.23558736]
 [0.25      ]
 [0.4375    ]
 [0.4375    ]
 [0.4375    ]
 [0.25      ]
 [0.18153293]
 [0.4375    ]]
Epoch 3311: at batch 1: Training dataset Loss=0.306, Batch Time=0.024
Epoch 3321: at batch 1: Training dataset Loss=0.297, Batch Time=0.034
Epoch 3331: at batch 1: Training dataset Loss=0.290, Batch Time=0.028
Epoch 3341: at batch 1: Training dataset Loss=0.290, Batch Time=0.032
Epoch 3351: at batch 1: Training dataset Loss=0.290, Batch Time=0.033
Epoch 3361: at batch 1: Training dataset Loss=0.295, Batch Time=0.031
Epoch 3371: at batch 1: Training dataset Loss=0.282, Batch Time=0.029
Epoch 3381: at batch 1: Training dataset Loss=0.256, Batch Time=0.032
Epoch 3391: at batch 1: Training dataset Loss=0.273, Batch Time=0.026
Epoch 3401: at batch 1: Training dataset Loss=0.261, Batch Time=0.025
Epoch 3401: Epoch time = 1387.921, Avg epoch time=0.261, Total Time=0.408

[[0.375     ]
 [0.25178796]
 [0.4375    ]
 [0.21880056]
 [0.23929611]
 [0.21880056]
 [0.25178796]
 [0.22482263]
 [0.3714655 ]
 [0.24790122]
 [0.24416678]
 [0.4375    ]
 [0.23706684]
 [0.22482263]
 [0.15652066]
 [0.31444603]
 [0.31444603]
 [0.375     ]
 [0.15652066]
 [0.4375    ]]
Epoch 3411: at batch 1: Training dataset Loss=0.284, Batch Time=0.027
Epoch 3421: at batch 1: Training dataset Loss=0.294, Batch Time=0.026
Epoch 3431: at batch 1: Training dataset Loss=0.277, Batch Time=0.031
Epoch 3441: at batch 1: Training dataset Loss=0.304, Batch Time=0.029
Epoch 3451: at batch 1: Training dataset Loss=0.268, Batch Time=0.030
Epoch 3461: at batch 1: Training dataset Loss=0.256, Batch Time=0.029
Epoch 3471: at batch 1: Training dataset Loss=0.299, Batch Time=0.030
Epoch 3481: at batch 1: Training dataset Loss=0.265, Batch Time=0.026
Epoch 3491: at batch 1: Training dataset Loss=0.261, Batch Time=0.027
Epoch 3501: at batch 1: Training dataset Loss=0.259, Batch Time=0.024
Epoch 3501: Epoch time = 1428.589, Avg epoch time=0.292, Total Time=0.408

[[0.28508407]
 [0.27584878]
 [0.31482077]
 [0.22895488]
 [0.31482077]
 [0.25249717]
 [0.29299217]
 [0.22895488]
 [0.26217699]
 [0.1875    ]
 [0.18479015]
 [0.375     ]
 [0.18340483]
 [0.26217699]
 [0.375     ]
 [0.22895488]
 [0.1875    ]
 [0.27584878]
 [0.22895488]
 [0.21197711]]
Epoch 3511: at batch 1: Training dataset Loss=0.285, Batch Time=0.025
Epoch 3521: at batch 1: Training dataset Loss=0.309, Batch Time=0.026
Epoch 3531: at batch 1: Training dataset Loss=0.287, Batch Time=0.034
Epoch 3541: at batch 1: Training dataset Loss=0.276, Batch Time=0.026
Epoch 3551: at batch 1: Training dataset Loss=0.303, Batch Time=0.026
Epoch 3561: at batch 1: Training dataset Loss=0.256, Batch Time=0.033
Epoch 3571: at batch 1: Training dataset Loss=0.314, Batch Time=0.034
Epoch 3581: at batch 1: Training dataset Loss=0.259, Batch Time=0.029
Epoch 3591: at batch 1: Training dataset Loss=0.266, Batch Time=0.025
Epoch 3601: at batch 1: Training dataset Loss=0.286, Batch Time=0.032
Epoch 3601: Epoch time = 1469.320, Avg epoch time=0.274, Total Time=0.408

[[0.125     ]
 [0.33491021]
 [0.29891607]
 [0.34436241]
 [0.125     ]
 [0.27508238]
 [0.23595868]
 [0.28465471]
 [0.27508238]
 [0.22414419]
 [0.125     ]
 [0.5       ]
 [0.33491021]
 [0.125     ]
 [0.29891607]
 [0.27508238]
 [0.20142926]
 [0.28465471]
 [0.41138774]
 [0.5625    ]]
Epoch 3611: at batch 1: Training dataset Loss=0.289, Batch Time=0.032
Epoch 3621: at batch 1: Training dataset Loss=0.264, Batch Time=0.033
Epoch 3631: at batch 1: Training dataset Loss=0.325, Batch Time=0.026
Epoch 3641: at batch 1: Training dataset Loss=0.278, Batch Time=0.031
Epoch 3651: at batch 1: Training dataset Loss=0.265, Batch Time=0.023
Epoch 3661: at batch 1: Training dataset Loss=0.273, Batch Time=0.028
Epoch 3671: at batch 1: Training dataset Loss=0.285, Batch Time=0.025
Epoch 3681: at batch 1: Training dataset Loss=0.275, Batch Time=0.028
Epoch 3691: at batch 1: Training dataset Loss=0.266, Batch Time=0.032
Epoch 3701: at batch 1: Training dataset Loss=0.258, Batch Time=0.033
Epoch 3701: Epoch time = 1509.949, Avg epoch time=0.289, Total Time=0.408

[[0.20850369]
 [0.375     ]
 [0.29879326]
 [0.200899  ]
 [0.26945794]
 [0.20952372]
 [0.22351107]
 [0.1983746 ]
 [0.375     ]
 [0.26363054]
 [0.375     ]
 [0.11008253]
 [0.20952372]
 [0.11008253]
 [0.24463515]
 [0.44902757]
 [0.36531734]
 [0.24463515]
 [0.22351107]
 [0.44902757]]
Epoch 3711: at batch 1: Training dataset Loss=0.257, Batch Time=0.035
Epoch 3721: at batch 1: Training dataset Loss=0.320, Batch Time=0.032
Epoch 3731: at batch 1: Training dataset Loss=0.259, Batch Time=0.026
Epoch 3741: at batch 1: Training dataset Loss=0.275, Batch Time=0.033
Epoch 3751: at batch 1: Training dataset Loss=0.267, Batch Time=0.029
Epoch 3761: at batch 1: Training dataset Loss=0.278, Batch Time=0.027
Epoch 3771: at batch 1: Training dataset Loss=0.288, Batch Time=0.026
Epoch 3781: at batch 1: Training dataset Loss=0.300, Batch Time=0.029
Epoch 3791: at batch 1: Training dataset Loss=0.287, Batch Time=0.032
Epoch 3801: at batch 1: Training dataset Loss=0.260, Batch Time=0.026
Epoch 3801: Epoch time = 1550.495, Avg epoch time=0.278, Total Time=0.408

[[0.21050446]
 [0.28572339]
 [0.22905858]
 [0.125     ]
 [0.3125    ]
 [0.31810185]
 [0.5       ]
 [0.13380165]
 [0.23671396]
 [0.27136597]
 [0.3697558 ]
 [0.23368752]
 [0.23671396]
 [0.21050446]
 [0.27184433]
 [0.23368752]
 [0.21050446]
 [0.18376949]
 [0.21050446]
 [0.30695558]]
Epoch 3811: at batch 1: Training dataset Loss=0.264, Batch Time=0.026
Epoch 3821: at batch 1: Training dataset Loss=0.287, Batch Time=0.032
Epoch 3831: at batch 1: Training dataset Loss=0.283, Batch Time=0.031
Epoch 3841: at batch 1: Training dataset Loss=0.304, Batch Time=0.026
Epoch 3851: at batch 1: Training dataset Loss=0.305, Batch Time=0.036
Epoch 3861: at batch 1: Training dataset Loss=0.259, Batch Time=0.025
Epoch 3871: at batch 1: Training dataset Loss=0.230, Batch Time=0.034
Epoch 3881: at batch 1: Training dataset Loss=0.265, Batch Time=0.032
Epoch 3891: at batch 1: Training dataset Loss=0.304, Batch Time=0.033
Epoch 3901: at batch 1: Training dataset Loss=0.292, Batch Time=0.033
Epoch 3901: Epoch time = 1591.219, Avg epoch time=0.280, Total Time=0.408

[[0.23836789]
 [0.27540553]
 [0.29082006]
 [0.23017401]
 [0.42205125]
 [0.22049817]
 [0.5       ]
 [0.23940066]
 [0.24200964]
 [0.125     ]
 [0.4375    ]
 [0.26974326]
 [0.25      ]
 [0.23836789]
 [0.25      ]
 [0.24200964]
 [0.24200964]
 [0.2352673 ]
 [0.24200964]
 [0.24932869]]
Epoch 3911: at batch 1: Training dataset Loss=0.331, Batch Time=0.028
Epoch 3921: at batch 1: Training dataset Loss=0.265, Batch Time=0.030
Epoch 3931: at batch 1: Training dataset Loss=0.305, Batch Time=0.032
Epoch 3941: at batch 1: Training dataset Loss=0.266, Batch Time=0.029
Epoch 3951: at batch 1: Training dataset Loss=0.287, Batch Time=0.034
Epoch 3961: at batch 1: Training dataset Loss=0.287, Batch Time=0.032
Epoch 3971: at batch 1: Training dataset Loss=0.285, Batch Time=0.026
Epoch 3981: at batch 1: Training dataset Loss=0.282, Batch Time=0.031
Epoch 3991: at batch 1: Training dataset Loss=0.268, Batch Time=0.024
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ 
