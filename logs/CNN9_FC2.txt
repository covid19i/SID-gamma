(sid2) [ir967@gr020 Learning-to-See-in-the-Dark]$ vi CNN9_FC2_log_regression.py 
(sid2) [ir967@gr020 Learning-to-See-in-the-Dark]$ python CNN9_FC2_log_regression.py 




Current date and time : 
2020-12-12 19:07:22
Found 161 images to train with

Training on 161 images only

2020-12-12 19:07:22.082217: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-12 19:07:22.218597: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-12 19:07:22.218632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-12 19:07:22.510116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-12 19:07:22.510155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-12 19:07:22.510228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-12 19:07:22.510335: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
No checkpoint found at ./gt_Sony_CNN9_FC2_log/. Hence, will create the folder.
Gamma curve:
Every 8K, 0 to 64K
[[0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]]
Every 125, 64536 to 65536
[[14104 17115 20761 25175 30515 36975 44786 54227]
 [ 1408  2284  3702  5994  9696 15670 25302 40817]
 [  653  1167  2083  3715  6616 11770 20917 37129]]
Every 1, 65526 to 65536
[[64641 64739 64838 64937 65036 65136 65235 65335 65435 65535]
 [63323 63565 63808 64051 64296 64542 64789 65036 65285 65535]
 [62889 63178 63468 63759 64051 64345 64641 64937 65235 65535]]

last epoch of previous run: 0
rawpy read the 0th file at location: ./dataset/Sony/long/00018_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.87565, 0.00000, 300.00000, 3485919
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.01648, 100.00000, 1
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 300.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 300.00000, 0
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00165, 100.00000, 1177951
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00190, 250.00000, 803131
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.02310, 250.00000, 3700
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00467, 100.00000, 2882267
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00001, 100.00000, 5540515
rawpy read the 10th file at location: ./dataset/Sony/long/00038_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00887, 250.00000, 2598691
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00228, 300.00000, 4653941
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00016, 300.00000, 6102869
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00613, 100.00000, 1458187
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00012, 100.00000, 3426263
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00116, 300.00000, 4586373
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00146, 250.00000, 909101
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00327, 100.00000, 3630407
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00002, 250.00000, 6735543
rawpy read the 20th file at location: ./dataset/Sony/long/00072_00_30s.ARW
rawpy read the 30th file at location: ./dataset/Sony/long/00039_00_10s.ARW
rawpy read the 40th file at location: ./dataset/Sony/long/00200_00_10s.ARW
rawpy read the 50th file at location: ./dataset/Sony/long/00024_00_10s.ARW
rawpy read the 60th file at location: ./dataset/Sony/long/00084_00_30s.ARW
rawpy read the 70th file at location: ./dataset/Sony/long/00164_00_30s.ARW
rawpy read the 80th file at location: ./dataset/Sony/long/00128_00_30s.ARW
rawpy read the 90th file at location: ./dataset/Sony/long/00057_00_10s.ARW
rawpy read the 100th file at location: ./dataset/Sony/long/00059_00_10s.ARW
rawpy read the 110th file at location: ./dataset/Sony/long/00026_00_10s.ARW
rawpy read the 120th file at location: ./dataset/Sony/long/00012_00_10s.ARW
rawpy read the 130th file at location: ./dataset/Sony/long/00090_00_30s.ARW
rawpy read the 140th file at location: ./dataset/Sony/long/00114_00_30s.ARW
rawpy read the 150th file at location: ./dataset/Sony/long/00156_00_30s.ARW
rawpy read the 160th file at location: ./dataset/Sony/long/00219_00_10s.ARW
161 images loaded to CPU RAM in Time=40.814 seconds.

Moved images data to a numpy array.



BATCH_SIZE 16 ,final_epoch 4001 ,no_of_batches 10 ,ps 128 ,result_dir ./gt_Sony_CNN9_FC2_log/ ,len(train_ids) 161
Scaling the log regression labels now.

Starting Training on index [102 102  72  30  91  18 109  75  69  82 138  72 125  41  96  65]
dataset index: [151 151 197  39 206 123 152 136  19 175 131 197 209 141 100 159]
Starting Training on gammas [100 100 300 300 100 100 250 100 300 300 250 300 250 250 100 100]
Epoch 0: at batch 1: Training dataset Loss=0.312, Batch Time=1.486
[[0.24725448]
 [0.18562061]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.59830487]
 [0.        ]
 [0.59830487]
 [0.18562061]
 [0.64402378]
 [0.18562061]
 [0.18562061]
 [0.        ]
 [0.        ]
 [0.30928531]
 [0.64402378]
 [1.14599526]
 [0.64402378]
 [0.24954522]
 [0.        ]]

Epoch 1: at batch 1: Training dataset Loss=0.441, Batch Time=0.037
		Epoch 1: Epoch time = 2.209, Avg epoch time=0.298, Total Time=1.105

[[0.20209113]
 [0.18562061]
 [0.        ]
 [0.3694829 ]
 [0.30967432]
 [0.3125    ]
 [0.        ]
 [0.25      ]
 [0.20209113]
 [0.3125    ]
 [0.25      ]
 [0.18562061]
 [0.3694829 ]
 [0.        ]
 [0.30928531]
 [0.3694829 ]
 [0.21208522]
 [0.80061114]
 [0.3125    ]
 [0.        ]]

Epoch 2: at batch 1: Training dataset Loss=0.377, Batch Time=0.029
[[0.90168941]
 [0.37195832]
 [0.90168941]
 [0.3694829 ]
 [0.71476662]
 [0.16266239]
 [0.        ]
 [0.3441689 ]
 [0.71476662]
 [0.22688498]
 [0.25      ]
 [0.18562061]
 [0.3694829 ]
 [0.        ]
 [0.3441689 ]
 [0.3694829 ]
 [0.21208522]
 [0.22688498]
 [0.3125    ]
 [0.        ]]

Epoch 3: at batch 1: Training dataset Loss=0.422, Batch Time=0.035
[[0.90168941]
 [0.37195832]
 [0.125     ]
 [0.50013387]
 [0.71476662]
 [0.24689628]
 [0.50013387]
 [0.39410734]
 [0.71476662]
 [0.26086816]
 [0.25      ]
 [0.18562061]
 [0.27699775]
 [0.26086816]
 [0.3441689 ]
 [1.81884241]
 [0.39410734]
 [0.50013387]
 [0.50013387]
 [0.        ]]

Epoch 4: at batch 1: Training dataset Loss=0.428, Batch Time=0.030
[[0.25      ]
 [0.27439088]
 [0.24472554]
 [0.50013387]
 [0.71476662]
 [0.24689628]
 [0.12500934]
 [0.39410734]
 [0.71476662]
 [0.31249815]
 [0.12500934]
 [0.18562061]
 [0.25      ]
 [0.26086816]
 [0.3441689 ]
 [1.81884241]
 [0.12500934]
 [0.12500934]
 [0.41250652]
 [0.        ]]

Epoch 5: at batch 1: Training dataset Loss=0.339, Batch Time=0.029
Epoch 6: at batch 1: Training dataset Loss=0.383, Batch Time=0.032
Epoch 7: at batch 1: Training dataset Loss=0.350, Batch Time=0.033
Epoch 8: at batch 1: Training dataset Loss=0.444, Batch Time=0.031
Epoch 9: at batch 1: Training dataset Loss=0.326, Batch Time=0.038
Epoch 11: at batch 1: Training dataset Loss=0.387, Batch Time=0.029
Epoch 21: at batch 1: Training dataset Loss=0.351, Batch Time=0.028
Epoch 31: at batch 1: Training dataset Loss=0.318, Batch Time=0.031
Epoch 41: at batch 1: Training dataset Loss=0.311, Batch Time=0.030
Epoch 51: at batch 1: Training dataset Loss=0.340, Batch Time=0.032
Epoch 61: at batch 1: Training dataset Loss=0.419, Batch Time=0.029
Epoch 71: at batch 1: Training dataset Loss=0.308, Batch Time=0.036
Epoch 81: at batch 1: Training dataset Loss=0.310, Batch Time=0.033
Epoch 91: at batch 1: Training dataset Loss=0.385, Batch Time=0.039
Epoch 101: at batch 1: Training dataset Loss=0.348, Batch Time=0.030
		Epoch 101: Epoch time = 42.108, Avg epoch time=0.315, Total Time=0.413

[[0.31569219]
 [0.2558769 ]
 [0.3449305 ]
 [0.33302075]
 [0.25      ]
 [0.52129209]
 [0.27265146]
 [0.07032427]
 [0.52129209]
 [0.375     ]
 [0.33447322]
 [0.3449305 ]
 [0.32315707]
 [0.15041189]
 [0.33447322]
 [0.41614598]
 [0.52129209]
 [0.55925345]
 [0.51661092]
 [0.52129209]]

Epoch 111: at batch 1: Training dataset Loss=0.394, Batch Time=0.033
Epoch 121: at batch 1: Training dataset Loss=0.292, Batch Time=0.037
Epoch 131: at batch 1: Training dataset Loss=0.289, Batch Time=0.036
^[[BEpoch 141: at batch 1: Training dataset Loss=0.365, Batch Time=0.031
Epoch 151: at batch 1: Training dataset Loss=0.328, Batch Time=0.035
Epoch 161: at batch 1: Training dataset Loss=0.310, Batch Time=0.032
Epoch 171: at batch 1: Training dataset Loss=0.311, Batch Time=0.031
Epoch 181: at batch 1: Training dataset Loss=0.269, Batch Time=0.035
Epoch 191: at batch 1: Training dataset Loss=0.376, Batch Time=0.035
Epoch 201: at batch 1: Training dataset Loss=0.304, Batch Time=0.030
		Epoch 201: Epoch time = 81.722, Avg epoch time=0.310, Total Time=0.405

[[0.42252535]
 [0.30468267]
 [0.0625    ]
 [0.3414433 ]
 [0.37754828]
 [0.29445216]
 [0.25      ]
 [0.0625    ]
 [0.1650978 ]
 [0.25865009]
 [0.0625    ]
 [0.3414433 ]
 [0.29220217]
 [0.25      ]
 [0.1650978 ]
 [0.24135254]
 [0.47957242]
 [0.3414433 ]
 [0.3125    ]
 [0.33909017]]

Epoch 211: at batch 1: Training dataset Loss=0.326, Batch Time=0.032
Epoch 221: at batch 1: Training dataset Loss=0.329, Batch Time=0.032
Epoch 231: at batch 1: Training dataset Loss=0.271, Batch Time=0.030
Epoch 241: at batch 1: Training dataset Loss=0.292, Batch Time=0.028
Epoch 251: at batch 1: Training dataset Loss=0.317, Batch Time=0.026
Epoch 261: at batch 1: Training dataset Loss=0.345, Batch Time=0.035
Epoch 271: at batch 1: Training dataset Loss=0.318, Batch Time=0.027
Epoch 281: at batch 1: Training dataset Loss=0.278, Batch Time=0.027
Epoch 291: at batch 1: Training dataset Loss=0.283, Batch Time=0.026
Epoch 301: at batch 1: Training dataset Loss=0.339, Batch Time=0.035
		Epoch 301: Epoch time = 121.405, Avg epoch time=0.309, Total Time=0.402

[[0.20489141]
 [0.4375    ]
 [0.29606086]
 [0.29690528]
 [0.4375    ]
 [0.29690528]
 [0.23534212]
 [0.25      ]
 [0.15271768]
 [0.24732366]
 [0.0612456 ]
 [0.37896553]
 [0.23534212]
 [0.0612456 ]
 [0.375     ]
 [0.5625    ]
 [0.0612456 ]
 [0.22182618]
 [0.15271768]
 [0.0612456 ]]

Epoch 311: at batch 1: Training dataset Loss=0.353, Batch Time=0.036
Epoch 321: at batch 1: Training dataset Loss=0.315, Batch Time=0.034
Epoch 331: at batch 1: Training dataset Loss=0.301, Batch Time=0.034
Epoch 341: at batch 1: Training dataset Loss=0.325, Batch Time=0.037
Epoch 351: at batch 1: Training dataset Loss=0.306, Batch Time=0.036
Epoch 361: at batch 1: Training dataset Loss=0.284, Batch Time=0.036
Epoch 371: at batch 1: Training dataset Loss=0.289, Batch Time=0.033
Epoch 381: at batch 1: Training dataset Loss=0.298, Batch Time=0.033
Epoch 391: at batch 1: Training dataset Loss=0.279, Batch Time=0.033
Epoch 401: at batch 1: Training dataset Loss=0.292, Batch Time=0.031
		Epoch 401: Epoch time = 161.206, Avg epoch time=0.332, Total Time=0.401

[[0.10041549]
 [0.4135257 ]
 [0.10041549]
 [0.18147027]
 [0.5       ]
 [0.10041549]
 [0.4135257 ]
 [0.3068341 ]
 [0.29298586]
 [0.5625    ]
 [0.26401559]
 [0.35217309]
 [0.3125    ]
 [0.40925017]
 [0.10041549]
 [0.35217309]
 [0.63850707]
 [0.5       ]
 [0.35376504]
 [0.18457182]]

Epoch 411: at batch 1: Training dataset Loss=0.273, Batch Time=0.028
Epoch 421: at batch 1: Training dataset Loss=0.331, Batch Time=0.028
Epoch 431: at batch 1: Training dataset Loss=0.285, Batch Time=0.037
Epoch 441: at batch 1: Training dataset Loss=0.307, Batch Time=0.036
Epoch 451: at batch 1: Training dataset Loss=0.296, Batch Time=0.035
Epoch 461: at batch 1: Training dataset Loss=0.300, Batch Time=0.036
Epoch 471: at batch 1: Training dataset Loss=0.348, Batch Time=0.033
Epoch 481: at batch 1: Training dataset Loss=0.322, Batch Time=0.029
Epoch 491: at batch 1: Training dataset Loss=0.340, Batch Time=0.028
Epoch 501: at batch 1: Training dataset Loss=0.330, Batch Time=0.035
		Epoch 501: Epoch time = 200.704, Avg epoch time=0.316, Total Time=0.400

[[0.23301257]
 [0.25      ]
 [0.4375    ]
 [0.375     ]
 [0.30319145]
 [0.375     ]
 [0.2096613 ]
 [0.5625    ]
 [0.23301257]
 [0.36917835]
 [0.20655897]
 [0.36917835]
 [0.18411651]
 [0.22650492]
 [0.23301257]
 [0.21641329]
 [0.30319145]
 [0.375     ]
 [0.2096613 ]
 [0.375     ]]

Epoch 511: at batch 1: Training dataset Loss=0.353, Batch Time=0.027
Epoch 521: at batch 1: Training dataset Loss=0.303, Batch Time=0.032
Epoch 531: at batch 1: Training dataset Loss=0.331, Batch Time=0.035
Epoch 541: at batch 1: Training dataset Loss=0.281, Batch Time=0.030
Epoch 551: at batch 1: Training dataset Loss=0.251, Batch Time=0.038
Epoch 561: at batch 1: Training dataset Loss=0.251, Batch Time=0.036
Epoch 571: at batch 1: Training dataset Loss=0.294, Batch Time=0.026
Epoch 581: at batch 1: Training dataset Loss=0.266, Batch Time=0.030
Epoch 591: at batch 1: Training dataset Loss=0.269, Batch Time=0.035
Epoch 601: at batch 1: Training dataset Loss=0.277, Batch Time=0.031
		Epoch 601: Epoch time = 240.196, Avg epoch time=0.300, Total Time=0.399

[[0.125     ]
 [0.5       ]
 [0.2086484 ]
 [0.21776307]
 [0.2086484 ]
 [0.26608735]
 [0.25      ]
 [0.5204345 ]
 [0.26608735]
 [0.22526133]
 [0.20813808]
 [0.26608735]
 [0.26608735]
 [0.17612034]
 [0.4375    ]
 [0.35712355]
 [0.26608735]
 [0.3194772 ]
 [0.26356339]
 [0.20813808]]

Epoch 611: at batch 1: Training dataset Loss=0.293, Batch Time=0.033
Epoch 621: at batch 1: Training dataset Loss=0.295, Batch Time=0.029
Epoch 631: at batch 1: Training dataset Loss=0.282, Batch Time=0.031
Epoch 641: at batch 1: Training dataset Loss=0.260, Batch Time=0.029
Epoch 651: at batch 1: Training dataset Loss=0.306, Batch Time=0.033
Epoch 661: at batch 1: Training dataset Loss=0.257, Batch Time=0.032
Epoch 671: at batch 1: Training dataset Loss=0.281, Batch Time=0.030
Epoch 681: at batch 1: Training dataset Loss=0.263, Batch Time=0.031
Epoch 691: at batch 1: Training dataset Loss=0.279, Batch Time=0.034
Epoch 701: at batch 1: Training dataset Loss=0.233, Batch Time=0.037
		Epoch 701: Epoch time = 279.488, Avg epoch time=0.334, Total Time=0.398

[[0.23785955]
 [0.26894432]
 [0.28823325]
 [0.20999511]
 [0.38476509]
 [0.15870953]
 [0.375     ]
 [0.19143222]
 [0.17916942]
 [0.375     ]
 [0.19143222]
 [0.21658655]
 [0.25582385]
 [0.2707279 ]
 [0.26497141]
 [0.375     ]
 [0.26894432]
 [0.19240278]
 [0.24322172]
 [0.375     ]]

Epoch 711: at batch 1: Training dataset Loss=0.277, Batch Time=0.035
Epoch 721: at batch 1: Training dataset Loss=0.258, Batch Time=0.035
Epoch 731: at batch 1: Training dataset Loss=0.237, Batch Time=0.035
Epoch 741: at batch 1: Training dataset Loss=0.264, Batch Time=0.032
Epoch 751: at batch 1: Training dataset Loss=0.314, Batch Time=0.035
Epoch 761: at batch 1: Training dataset Loss=0.261, Batch Time=0.037
Epoch 771: at batch 1: Training dataset Loss=0.288, Batch Time=0.031
Epoch 781: at batch 1: Training dataset Loss=0.260, Batch Time=0.030
Epoch 791: at batch 1: Training dataset Loss=0.246, Batch Time=0.028
Epoch 801: at batch 1: Training dataset Loss=0.337, Batch Time=0.038
		Epoch 801: Epoch time = 318.824, Avg epoch time=0.313, Total Time=0.398

[[0.20444849]
 [0.5625    ]
 [0.13922405]
 [0.5625    ]
 [0.2347388 ]
 [0.17551096]
 [0.26262605]
 [0.25      ]
 [0.13922405]
 [0.20283172]
 [0.2340681 ]
 [0.2347388 ]
 [0.5625    ]
 [0.5625    ]
 [0.4375    ]
 [0.5625    ]
 [0.20444849]
 [0.24852294]
 [0.5625    ]
 [0.17551096]]

Epoch 811: at batch 1: Training dataset Loss=0.245, Batch Time=0.035
Epoch 821: at batch 1: Training dataset Loss=0.273, Batch Time=0.029
Epoch 831: at batch 1: Training dataset Loss=0.300, Batch Time=0.032
Epoch 841: at batch 1: Training dataset Loss=0.266, Batch Time=0.029
Epoch 851: at batch 1: Training dataset Loss=0.320, Batch Time=0.033
Epoch 861: at batch 1: Training dataset Loss=0.253, Batch Time=0.035
Epoch 871: at batch 1: Training dataset Loss=0.256, Batch Time=0.028
Epoch 881: at batch 1: Training dataset Loss=0.252, Batch Time=0.029
Epoch 891: at batch 1: Training dataset Loss=0.264, Batch Time=0.035
Epoch 901: at batch 1: Training dataset Loss=0.265, Batch Time=0.032
		Epoch 901: Epoch time = 358.435, Avg epoch time=0.298, Total Time=0.397

[[0.375     ]
 [0.24965002]
 [0.24438176]
 [0.19275473]
 [0.375     ]
 [0.48510826]
 [0.24756855]
 [0.4375    ]
 [0.19275473]
 [0.24833792]
 [0.23555967]
 [0.24833792]
 [0.18281589]
 [0.24833792]
 [0.15124407]
 [0.25006837]
 [0.15124407]
 [0.23555967]
 [0.1875    ]
 [0.25006837]]

Epoch 911: at batch 1: Training dataset Loss=0.279, Batch Time=0.032
Epoch 921: at batch 1: Training dataset Loss=0.258, Batch Time=0.029
Epoch 931: at batch 1: Training dataset Loss=0.231, Batch Time=0.029
Epoch 941: at batch 1: Training dataset Loss=0.234, Batch Time=0.030
Epoch 951: at batch 1: Training dataset Loss=0.327, Batch Time=0.028
Epoch 961: at batch 1: Training dataset Loss=0.289, Batch Time=0.035
^CTraceback (most recent call last):
  File "CNN9_FC2_log_regression.py", line 333, in <module>
    feed_dict={in_image: input_patch, gt_gamma: assigned_image_gamma_feed, lr: learning_rate})
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1292, in _do_call
    return fn(*args)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1367, in _call_tf_sessionrun
    run_metadata)
KeyboardInterrupt
(sid2) [ir967@gr020 Learning-to-See-in-the-Dark]$ 
