(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ clear

(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ mkdir gt_Sony_medium_log_regression_MSE_lowerLRat1000
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ vi medium_log_regression.py
(sid2) [ir967@gr011 Learning-to-See-in-the-Dark]$ python medium_log_regression.py 




Current date and time : 
2020-12-12 15:42:32
Found 161 images to train with

Training on 161 images only

2020-12-12 15:42:32.397908: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-12 15:42:32.535981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:86:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-12 15:42:32.536015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-12 15:42:32.823241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-12 15:42:32.823274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-12 15:42:32.823297: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-12 15:42:32.823399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:86:00.0, compute capability: 7.5)
No checkpoint found at ./gt_Sony_medium_log_regression_MSE_lowerLRat1000/. Hence, will create the folder.
Gamma curve:
Every 8K, 0 to 64K
[[0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]
 [0 0 0 0 0 0 0 0]]
Every 125, 64536 to 65536
[[14104 17115 20761 25175 30515 36975 44786 54227]
 [ 1408  2284  3702  5994  9696 15670 25302 40817]
 [  653  1167  2083  3715  6616 11770 20917 37129]]
Every 1, 65526 to 65536
[[64641 64739 64838 64937 65036 65136 65235 65335 65435 65535]
 [63323 63565 63808 64051 64296 64542 64789 65036 65285 65535]
 [62889 63178 63468 63759 64051 64345 64641 64937 65235 65535]]

last epoch of previous run: 0
rawpy read the 0th file at location: ./dataset/Sony/long/00018_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 300.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.87565, 0.00000, 300.00000, 3485919
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.01648, 100.00000, 1
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 100.00000, 0
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00164, 300.00000, 1177951
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00187, 300.00000, 803131
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.02310, 250.00000, 3700
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00427, 250.00000, 2882267
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00001, 250.00000, 5540515
rawpy read the 10th file at location: ./dataset/Sony/long/00038_00_10s.ARW
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00886, 300.00000, 2598691
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00228, 300.00000, 4653941
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00017, 100.00000, 6102869
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00613, 100.00000, 1458187
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00012, 250.00000, 3426263
min, max, mean, gamma, argmax: 0.00000, 1.00000, 0.00116, 250.00000, 4586373
min, max, mean, gamma, argmax: 0.00000, 0.94221, 0.00144, 300.00000, 909101
min, max, mean, gamma, argmax: 0.00000, 0.00000, 0.00000, 300.00000, 0
min, max, mean, gamma, argmax: 0.00000, 0.98035, 0.00327, 100.00000, 3630407
min, max, mean, gamma, argmax: 0.00000, 0.95160, 0.00002, 250.00000, 6735543
rawpy read the 20th file at location: ./dataset/Sony/long/00072_00_30s.ARW
rawpy read the 30th file at location: ./dataset/Sony/long/00039_00_10s.ARW
rawpy read the 40th file at location: ./dataset/Sony/long/00200_00_10s.ARW
rawpy read the 50th file at location: ./dataset/Sony/long/00024_00_10s.ARW
rawpy read the 60th file at location: ./dataset/Sony/long/00084_00_30s.ARW
rawpy read the 70th file at location: ./dataset/Sony/long/00164_00_30s.ARW
rawpy read the 80th file at location: ./dataset/Sony/long/00128_00_30s.ARW
rawpy read the 90th file at location: ./dataset/Sony/long/00057_00_10s.ARW
rawpy read the 100th file at location: ./dataset/Sony/long/00059_00_10s.ARW
rawpy read the 110th file at location: ./dataset/Sony/long/00026_00_10s.ARW
rawpy read the 120th file at location: ./dataset/Sony/long/00012_00_10s.ARW
rawpy read the 130th file at location: ./dataset/Sony/long/00090_00_30s.ARW
rawpy read the 140th file at location: ./dataset/Sony/long/00114_00_30s.ARW
rawpy read the 150th file at location: ./dataset/Sony/long/00156_00_30s.ARW
rawpy read the 160th file at location: ./dataset/Sony/long/00219_00_10s.ARW
161 images loaded to CPU RAM in Time=41.109 seconds.

Moved images data to a numpy array.
BATCH_SIZE 16 ,final_epoch 4001 ,no_of_batches 10 ,ps 128 ,result_dir ./gt_Sony_medium_log_regression_MSE_lowerLRat1000/ ,len(train_ids) 161
Scaling the regression labels now.
Starting Training on index [108 118  99 127  69 138 144  55  44 112  89 118  97 112 128 145], dataset index: [ 76 165  28  75  19 131  58  23  95  62  83 165 135  62  71 218]
Starting Training on gammas [100 100 300 250 100 100 100 300 250 300 250 100 250 300 300 100]
Epoch 0: at batch 1: Training dataset Loss=0.518, Batch Time=1.273
[[0.3125    ]
 [0.22540843]
 [0.        ]
 [0.25      ]
 [1.95092654]
 [0.        ]
 [0.3125    ]
 [0.375     ]
 [0.3125    ]
 [0.42386484]
 [0.375     ]
 [0.3125    ]
 [0.        ]
 [0.        ]
 [0.375     ]
 [0.        ]
 [0.        ]
 [0.        ]
 [0.        ]
 [1.21688938]]
Epoch 1: at batch 1: Training dataset Loss=0.971, Batch Time=0.024
		Epoch 1: Epoch time = 2.027, Avg epoch time=0.279, Total Time=1.013

[[0.30994242]
 [0.22540843]
 [0.28749299]
 [0.5625    ]
 [1.95092654]
 [0.        ]
 [0.3125    ]
 [0.28749299]
 [0.37180167]
 [0.42386484]
 [0.37180167]
 [0.23972735]
 [0.91982579]
 [0.37180167]
 [0.375     ]
 [0.        ]
 [0.        ]
 [0.28749299]
 [0.8237263 ]
 [1.21688938]]
Epoch 2: at batch 1: Training dataset Loss=0.521, Batch Time=0.027
[[0.69069791]
 [1.164536  ]
 [0.30908564]
 [0.23457754]
 [0.55824322]
 [1.164536  ]
 [0.3125    ]
 [0.28749299]
 [0.37180167]
 [0.42386484]
 [0.69069791]
 [1.164536  ]
 [0.91982579]
 [0.23457754]
 [0.69069791]
 [0.69069791]
 [0.30908564]
 [0.23457754]
 [0.55824322]
 [0.55824322]]
Epoch 3: at batch 1: Training dataset Loss=0.524, Batch Time=0.029
[[0.24077018]
 [1.164536  ]
 [0.18603505]
 [0.27508056]
 [0.55824322]
 [0.5625    ]
 [0.3125    ]
 [1.37017298]
 [0.37180167]
 [0.42386484]
 [1.37017298]
 [1.164536  ]
 [0.91982579]
 [0.4830302 ]
 [0.4375    ]
 [0.69069791]
 [0.4375    ]
 [0.24077018]
 [0.55824322]
 [0.4830302 ]]
Epoch 4: at batch 1: Training dataset Loss=0.561, Batch Time=0.029
[[0.25001752]
 [0.25001752]
 [0.18603505]
 [0.25001752]
 [0.56639981]
 [0.5625    ]
 [0.56639981]
 [0.1875    ]
 [0.25001752]
 [0.19702822]
 [1.37017298]
 [0.25001752]
 [0.48542672]
 [0.68686271]
 [0.56639981]
 [0.69069791]
 [0.89611006]
 [0.25001752]
 [0.68686271]
 [0.4830302 ]]
Epoch 5: at batch 1: Training dataset Loss=0.515, Batch Time=0.031
[[0.26913673]
 [0.25001752]
 [0.68574262]
 [0.25001752]
 [0.56639981]
 [0.79445791]
 [0.36367679]
 [0.125     ]
 [0.25001752]
 [0.15593708]
 [1.37017298]
 [0.79445791]
 [0.40660542]
 [0.68686271]
 [0.56639981]
 [0.125     ]
 [0.27164686]
 [0.36367679]
 [0.68686271]
 [0.4830302 ]]
Epoch 6: at batch 1: Training dataset Loss=0.404, Batch Time=0.032
[[0.83165669]
 [0.4375    ]
 [0.52864146]
 [0.18350297]
 [0.4375    ]
 [0.83165669]
 [0.36367679]
 [0.67397636]
 [0.52864146]
 [0.15593708]
 [1.37017298]
 [0.4375    ]
 [0.18350297]
 [0.68686271]
 [0.34222102]
 [0.12636584]
 [0.34222102]
 [0.36367679]
 [0.18518445]
 [0.67397636]]
Epoch 7: at batch 1: Training dataset Loss=0.419, Batch Time=0.028
[[0.40065068]
 [0.4375    ]
 [0.94686115]
 [0.125     ]
 [0.4375    ]
 [0.83165669]
 [0.36367679]
 [0.67397636]
 [0.94686115]
 [0.15593708]
 [0.40065068]
 [0.22676575]
 [1.97148633]
 [0.30528492]
 [0.34222102]
 [0.12636584]
 [0.34222102]
 [0.26118264]
 [0.18518445]
 [0.67397636]]
Epoch 8: at batch 1: Training dataset Loss=0.512, Batch Time=0.031
[[0.4375    ]
 [0.4375    ]
 [0.125     ]
 [0.4375    ]
 [0.5       ]
 [0.4375    ]
 [0.36367679]
 [0.67397636]
 [0.94686115]
 [0.4375    ]
 [0.40065068]
 [0.25      ]
 [0.3125    ]
 [0.25      ]
 [0.4375    ]
 [0.74693489]
 [0.4375    ]
 [0.26771337]
 [0.3125    ]
 [0.67397636]]
Epoch 9: at batch 1: Training dataset Loss=0.430, Batch Time=0.026
[[0.4375    ]
 [0.4375    ]
 [0.36554235]
 [0.36988676]
 [0.5       ]
 [0.375     ]
 [0.36988676]
 [0.33748978]
 [0.94686115]
 [0.4375    ]
 [0.40065068]
 [0.25      ]
 [0.3125    ]
 [0.82785463]
 [0.33748978]
 [0.74693489]
 [0.36554235]
 [0.30877334]
 [0.375     ]
 [0.36988676]]
Epoch 11: at batch 1: Training dataset Loss=0.377, Batch Time=0.036
Epoch 21: at batch 1: Training dataset Loss=0.442, Batch Time=0.027
Epoch 31: at batch 1: Training dataset Loss=0.407, Batch Time=0.032
Epoch 41: at batch 1: Training dataset Loss=0.501, Batch Time=0.034
Epoch 51: at batch 1: Training dataset Loss=0.405, Batch Time=0.035
Epoch 61: at batch 1: Training dataset Loss=0.405, Batch Time=0.034
Epoch 71: at batch 1: Training dataset Loss=0.330, Batch Time=0.029
Epoch 81: at batch 1: Training dataset Loss=0.309, Batch Time=0.031
Epoch 91: at batch 1: Training dataset Loss=0.464, Batch Time=0.035
Epoch 101: at batch 1: Training dataset Loss=0.346, Batch Time=0.028
		Epoch 101: Epoch time = 43.074, Avg epoch time=0.257, Total Time=0.422

[[0.25      ]
 [0.4375    ]
 [0.4375    ]
 [0.125     ]
 [0.3125    ]
 [0.17876175]
 [0.25      ]
 [0.33583349]
 [0.125     ]
 [0.4375    ]
 [0.31110138]
 [0.375     ]
 [0.1875    ]
 [0.3125    ]
 [0.125     ]
 [0.75      ]
 [0.125     ]
 [0.125     ]
 [0.4962014 ]
 [0.25464907]]
Epoch 111: at batch 1: Training dataset Loss=0.353, Batch Time=0.034
Epoch 121: at batch 1: Training dataset Loss=0.358, Batch Time=0.029
Epoch 131: at batch 1: Training dataset Loss=0.313, Batch Time=0.029
Epoch 141: at batch 1: Training dataset Loss=0.334, Batch Time=0.024
Epoch 151: at batch 1: Training dataset Loss=0.283, Batch Time=0.028
Epoch 161: at batch 1: Training dataset Loss=0.369, Batch Time=0.024
Epoch 171: at batch 1: Training dataset Loss=0.299, Batch Time=0.034
Epoch 181: at batch 1: Training dataset Loss=0.334, Batch Time=0.025
Epoch 191: at batch 1: Training dataset Loss=0.367, Batch Time=0.033
Epoch 201: at batch 1: Training dataset Loss=0.398, Batch Time=0.024
		Epoch 201: Epoch time = 83.342, Avg epoch time=0.258, Total Time=0.413

[[0.6875    ]
 [0.3125    ]
 [0.375     ]
 [0.5625    ]
 [0.25      ]
 [0.82406086]
 [0.3125    ]
 [0.25      ]
 [0.25      ]
 [0.375     ]
 [0.375     ]
 [0.375     ]
 [0.37198031]
 [0.3125    ]
 [0.4375    ]
 [0.25      ]
 [0.375     ]
 [0.5       ]
 [0.375     ]
 [0.3125    ]]
Epoch 211: at batch 1: Training dataset Loss=0.342, Batch Time=0.026
Epoch 221: at batch 1: Training dataset Loss=0.342, Batch Time=0.032
Epoch 231: at batch 1: Training dataset Loss=0.422, Batch Time=0.025
Epoch 241: at batch 1: Training dataset Loss=0.392, Batch Time=0.026
Epoch 251: at batch 1: Training dataset Loss=0.338, Batch Time=0.031
Epoch 261: at batch 1: Training dataset Loss=0.339, Batch Time=0.031
Epoch 271: at batch 1: Training dataset Loss=0.378, Batch Time=0.024
Epoch 281: at batch 1: Training dataset Loss=0.333, Batch Time=0.023
Epoch 291: at batch 1: Training dataset Loss=0.331, Batch Time=0.027
Epoch 301: at batch 1: Training dataset Loss=0.367, Batch Time=0.025
		Epoch 301: Epoch time = 123.979, Avg epoch time=0.261, Total Time=0.411

[[0.25      ]
 [0.4375    ]
 [0.25      ]
 [0.375     ]
 [0.25      ]
 [0.5       ]
 [0.2620483 ]
 [0.4375    ]
 [0.25      ]
 [0.375     ]
 [0.4375    ]
 [0.36088908]
 [0.25      ]
 [0.4375    ]
 [0.25      ]
 [0.3125    ]
 [0.4375    ]
 [0.50249785]
 [0.30033481]
 [0.25      ]]
Epoch 311: at batch 1: Training dataset Loss=0.378, Batch Time=0.033
Epoch 321: at batch 1: Training dataset Loss=0.328, Batch Time=0.023
Epoch 331: at batch 1: Training dataset Loss=0.391, Batch Time=0.031
Epoch 341: at batch 1: Training dataset Loss=0.339, Batch Time=0.025
Epoch 351: at batch 1: Training dataset Loss=0.323, Batch Time=0.029
Epoch 361: at batch 1: Training dataset Loss=0.354, Batch Time=0.032
Epoch 371: at batch 1: Training dataset Loss=0.527, Batch Time=0.025
Epoch 381: at batch 1: Training dataset Loss=0.322, Batch Time=0.031
Epoch 391: at batch 1: Training dataset Loss=0.355, Batch Time=0.028
Epoch 401: at batch 1: Training dataset Loss=0.286, Batch Time=0.032
		Epoch 401: Epoch time = 164.586, Avg epoch time=0.264, Total Time=0.409

[[0.375     ]
 [0.3125    ]
 [0.3125    ]
 [0.125     ]
 [0.14807932]
 [0.3125    ]
 [0.375     ]
 [0.78834236]
 [0.1875    ]
 [0.0625    ]
 [0.3125    ]
 [0.43527561]
 [0.0625    ]
 [0.78834236]
 [0.14807932]
 [0.3125    ]
 [0.78834236]
 [0.375     ]
 [0.78834236]
 [0.14807932]]
Epoch 411: at batch 1: Training dataset Loss=0.340, Batch Time=0.030
Epoch 421: at batch 1: Training dataset Loss=0.354, Batch Time=0.031
Epoch 431: at batch 1: Training dataset Loss=0.333, Batch Time=0.024
Epoch 441: at batch 1: Training dataset Loss=0.333, Batch Time=0.024
Epoch 451: at batch 1: Training dataset Loss=0.376, Batch Time=0.026
Epoch 461: at batch 1: Training dataset Loss=0.287, Batch Time=0.033
Epoch 471: at batch 1: Training dataset Loss=0.324, Batch Time=0.030
Epoch 481: at batch 1: Training dataset Loss=0.358, Batch Time=0.027
Epoch 491: at batch 1: Training dataset Loss=0.294, Batch Time=0.031
Epoch 501: at batch 1: Training dataset Loss=0.328, Batch Time=0.025
		Epoch 501: Epoch time = 205.033, Avg epoch time=0.274, Total Time=0.408

[[0.4375    ]
 [0.25      ]
 [0.3125    ]
 [0.28572562]
 [0.375     ]
 [0.3125    ]
 [0.5       ]
 [0.43797508]
 [0.22918123]
 [0.3125    ]
 [0.30303994]
 [0.4375    ]
 [0.1875    ]
 [0.20000219]
 [0.4375    ]
 [0.5       ]
 [0.41559964]
 [0.4375    ]
 [0.24104258]
 [0.4375    ]]
Epoch 511: at batch 1: Training dataset Loss=0.346, Batch Time=0.029
Epoch 521: at batch 1: Training dataset Loss=0.333, Batch Time=0.027
Epoch 531: at batch 1: Training dataset Loss=0.374, Batch Time=0.032
Epoch 541: at batch 1: Training dataset Loss=0.378, Batch Time=0.031
Epoch 551: at batch 1: Training dataset Loss=0.377, Batch Time=0.023
Epoch 561: at batch 1: Training dataset Loss=0.338, Batch Time=0.026
Epoch 571: at batch 1: Training dataset Loss=0.387, Batch Time=0.028
Epoch 581: at batch 1: Training dataset Loss=0.325, Batch Time=0.025
Epoch 591: at batch 1: Training dataset Loss=0.331, Batch Time=0.023
Epoch 601: at batch 1: Training dataset Loss=0.343, Batch Time=0.025
		Epoch 601: Epoch time = 245.709, Avg epoch time=0.282, Total Time=0.408

[[0.43204117]
 [0.375     ]
 [0.18336169]
 [0.3125    ]
 [0.4375    ]
 [0.43204117]
 [0.375     ]
 [0.375     ]
 [0.25      ]
 [0.375     ]
 [0.375     ]
 [0.35744011]
 [0.375     ]
 [0.375     ]
 [0.4375    ]
 [0.1875    ]
 [0.5       ]
 [0.3125    ]
 [0.3125    ]
 [0.25933111]]
Epoch 611: at batch 1: Training dataset Loss=0.337, Batch Time=0.030
Epoch 621: at batch 1: Training dataset Loss=0.351, Batch Time=0.031
Epoch 631: at batch 1: Training dataset Loss=0.340, Batch Time=0.024
Epoch 641: at batch 1: Training dataset Loss=0.298, Batch Time=0.023
Epoch 651: at batch 1: Training dataset Loss=0.307, Batch Time=0.032
Epoch 661: at batch 1: Training dataset Loss=0.331, Batch Time=0.033
Epoch 671: at batch 1: Training dataset Loss=0.352, Batch Time=0.026
Epoch 681: at batch 1: Training dataset Loss=0.320, Batch Time=0.027
Epoch 691: at batch 1: Training dataset Loss=0.339, Batch Time=0.028
Epoch 701: at batch 1: Training dataset Loss=0.296, Batch Time=0.023
		Epoch 701: Epoch time = 286.252, Avg epoch time=0.271, Total Time=0.408

[[0.25449604]
 [0.3125    ]
 [0.375     ]
 [0.1875    ]
 [0.375     ]
 [0.25      ]
 [0.5       ]
 [0.18644485]
 [0.5       ]
 [0.25449604]
 [0.375     ]
 [0.375     ]
 [0.5       ]
 [0.25449604]
 [0.33599132]
 [0.25      ]
 [0.4375    ]
 [0.5       ]
 [0.1875    ]
 [0.3125    ]]
Epoch 711: at batch 1: Training dataset Loss=0.331, Batch Time=0.024
Epoch 721: at batch 1: Training dataset Loss=0.311, Batch Time=0.026
Epoch 731: at batch 1: Training dataset Loss=0.509, Batch Time=0.028
Epoch 741: at batch 1: Training dataset Loss=0.362, Batch Time=0.028
Epoch 751: at batch 1: Training dataset Loss=0.340, Batch Time=0.032
Epoch 761: at batch 1: Training dataset Loss=0.321, Batch Time=0.029
Epoch 771: at batch 1: Training dataset Loss=0.326, Batch Time=0.031
Epoch 781: at batch 1: Training dataset Loss=0.339, Batch Time=0.028
Epoch 791: at batch 1: Training dataset Loss=0.320, Batch Time=0.029
Epoch 801: at batch 1: Training dataset Loss=0.308, Batch Time=0.028
		Epoch 801: Epoch time = 327.198, Avg epoch time=0.308, Total Time=0.408

[[0.375     ]
 [0.30884439]
 [0.25      ]
 [0.5       ]
 [0.375     ]
 [0.375     ]
 [0.4375    ]
 [0.3530497 ]
 [0.4375    ]
 [0.375     ]
 [0.25      ]
 [0.5       ]
 [0.4375    ]
 [0.30884439]
 [0.30884439]
 [0.375     ]
 [0.375     ]
 [0.30884439]
 [0.3125    ]
 [0.3530497 ]]
Epoch 811: at batch 1: Training dataset Loss=0.342, Batch Time=0.026
Epoch 821: at batch 1: Training dataset Loss=0.349, Batch Time=0.034
Epoch 831: at batch 1: Training dataset Loss=0.363, Batch Time=0.026
Epoch 841: at batch 1: Training dataset Loss=0.339, Batch Time=0.034
Epoch 851: at batch 1: Training dataset Loss=0.320, Batch Time=0.034
Epoch 861: at batch 1: Training dataset Loss=0.342, Batch Time=0.031
Epoch 871: at batch 1: Training dataset Loss=0.363, Batch Time=0.034
Epoch 881: at batch 1: Training dataset Loss=0.311, Batch Time=0.026
Epoch 891: at batch 1: Training dataset Loss=0.343, Batch Time=0.026
Epoch 901: at batch 1: Training dataset Loss=0.375, Batch Time=0.026
		Epoch 901: Epoch time = 367.856, Avg epoch time=0.295, Total Time=0.408

[[0.3125    ]
 [0.375     ]
 [0.375     ]
 [0.3125    ]
 [0.4375    ]
 [0.4375    ]
 [0.25      ]
 [0.3125    ]
 [0.29950285]
 [0.34695476]
 [0.4375    ]
 [0.5625    ]
 [0.29263228]
 [0.25      ]
 [0.3125    ]
 [0.25      ]
 [0.375     ]
 [0.4375    ]
 [0.1875    ]
 [0.3125    ]]
Epoch 911: at batch 1: Training dataset Loss=0.358, Batch Time=0.026
Epoch 921: at batch 1: Training dataset Loss=0.362, Batch Time=0.031
Epoch 931: at batch 1: Training dataset Loss=0.372, Batch Time=0.025
Epoch 941: at batch 1: Training dataset Loss=0.343, Batch Time=0.029
Epoch 951: at batch 1: Training dataset Loss=0.334, Batch Time=0.027
Epoch 961: at batch 1: Training dataset Loss=0.279, Batch Time=0.029
Epoch 971: at batch 1: Training dataset Loss=0.365, Batch Time=0.029
Epoch 981: at batch 1: Training dataset Loss=0.330, Batch Time=0.031
Epoch 991: at batch 1: Training dataset Loss=0.347, Batch Time=0.026
Epoch 1001: at batch 1: Training dataset Loss=0.353, Batch Time=0.024
		Epoch 1001: Epoch time = 408.419, Avg epoch time=0.271, Total Time=0.408

[[0.1875    ]
 [0.5       ]
 [0.43635988]
 [0.43635988]
 [0.5       ]
 [0.4375    ]
 [0.5       ]
 [0.25      ]
 [0.44020087]
 [0.125     ]
 [0.5       ]
 [0.4375    ]
 [0.5625    ]
 [0.4375    ]
 [0.5625    ]
 [0.125     ]
 [0.5       ]
 [0.125     ]
 [0.125     ]
 [0.125     ]]
Epoch 1011: at batch 1: Training dataset Loss=0.347, Batch Time=0.026
Epoch 1021: at batch 1: Training dataset Loss=0.311, Batch Time=0.024
Epoch 1031: at batch 1: Training dataset Loss=0.354, Batch Time=0.026
Epoch 1041: at batch 1: Training dataset Loss=0.390, Batch Time=0.029
Epoch 1051: at batch 1: Training dataset Loss=0.392, Batch Time=0.035
Epoch 1061: at batch 1: Training dataset Loss=0.362, Batch Time=0.033
Epoch 1071: at batch 1: Training dataset Loss=0.376, Batch Time=0.031
Epoch 1081: at batch 1: Training dataset Loss=0.310, Batch Time=0.031
Epoch 1091: at batch 1: Training dataset Loss=0.314, Batch Time=0.033
Epoch 1101: at batch 1: Training dataset Loss=0.328, Batch Time=0.033
		Epoch 1101: Epoch time = 449.148, Avg epoch time=0.285, Total Time=0.408

[[0.4375    ]
 [0.1875    ]
 [0.37533516]
 [0.1875    ]
 [0.25      ]
 [0.42569473]
 [0.4375    ]
 [0.25      ]
 [0.1875    ]
 [0.37533516]
 [0.4375    ]
 [0.48194948]
 [0.375     ]
 [0.1875    ]
 [0.4375    ]
 [0.375     ]
 [0.4375    ]
 [0.4375    ]
 [0.375     ]
 [0.2913844 ]]
Epoch 1111: at batch 1: Training dataset Loss=0.340, Batch Time=0.031
Epoch 1121: at batch 1: Training dataset Loss=0.358, Batch Time=0.024
Epoch 1131: at batch 1: Training dataset Loss=0.348, Batch Time=0.025
Epoch 1141: at batch 1: Training dataset Loss=0.345, Batch Time=0.031
Epoch 1151: at batch 1: Training dataset Loss=0.356, Batch Time=0.026
Epoch 1161: at batch 1: Training dataset Loss=0.643, Batch Time=0.033
Epoch 1171: at batch 1: Training dataset Loss=0.347, Batch Time=0.032
Epoch 1181: at batch 1: Training dataset Loss=0.387, Batch Time=0.031
Epoch 1191: at batch 1: Training dataset Loss=0.263, Batch Time=0.026
Epoch 1201: at batch 1: Training dataset Loss=0.394, Batch Time=0.026
		Epoch 1201: Epoch time = 489.820, Avg epoch time=0.275, Total Time=0.408

[[0.4375    ]
 [0.1875    ]
 [0.94801199]
 [0.22219101]
 [0.0625    ]
 [0.22219101]
 [0.375     ]
 [0.625     ]
 [0.3125    ]
 [0.0625    ]
 [0.3125    ]
 [0.6875    ]
 [0.22219101]
 [0.4375    ]
 [0.375     ]
 [0.4375    ]
 [0.25      ]
 [0.0625    ]
 [0.375     ]
 [0.42734867]]
Epoch 1211: at batch 1: Training dataset Loss=0.303, Batch Time=0.027
Epoch 1221: at batch 1: Training dataset Loss=0.334, Batch Time=0.032
Epoch 1231: at batch 1: Training dataset Loss=0.412, Batch Time=0.029
Epoch 1241: at batch 1: Training dataset Loss=0.323, Batch Time=0.028
Epoch 1251: at batch 1: Training dataset Loss=0.311, Batch Time=0.027
Epoch 1261: at batch 1: Training dataset Loss=0.343, Batch Time=0.031
Epoch 1271: at batch 1: Training dataset Loss=0.332, Batch Time=0.025
Epoch 1281: at batch 1: Training dataset Loss=0.337, Batch Time=0.033
Epoch 1291: at batch 1: Training dataset Loss=0.321, Batch Time=0.025
Epoch 1301: at batch 1: Training dataset Loss=0.367, Batch Time=0.031
		Epoch 1301: Epoch time = 530.576, Avg epoch time=0.294, Total Time=0.408

[[0.1875    ]
 [0.375     ]
 [0.4375    ]
 [0.375     ]
 [0.125     ]
 [0.30947739]
 [0.375     ]
 [0.3125    ]
 [0.3125    ]
 [0.3125    ]
 [0.27280068]
 [0.5       ]
 [0.3125    ]
 [0.5625    ]
 [0.27280068]
 [0.375     ]
 [0.30947739]
 [0.375     ]
 [0.25      ]
 [0.375     ]]
Epoch 1311: at batch 1: Training dataset Loss=0.352, Batch Time=0.032
Epoch 1321: at batch 1: Training dataset Loss=0.309, Batch Time=0.033
Epoch 1331: at batch 1: Training dataset Loss=0.318, Batch Time=0.029
Epoch 1341: at batch 1: Training dataset Loss=0.331, Batch Time=0.030
Epoch 1351: at batch 1: Training dataset Loss=0.323, Batch Time=0.033
Epoch 1361: at batch 1: Training dataset Loss=0.295, Batch Time=0.029
Epoch 1371: at batch 1: Training dataset Loss=0.445, Batch Time=0.030
Epoch 1381: at batch 1: Training dataset Loss=0.308, Batch Time=0.025
Epoch 1391: at batch 1: Training dataset Loss=0.357, Batch Time=0.026
Epoch 1401: at batch 1: Training dataset Loss=0.290, Batch Time=0.027
		Epoch 1401: Epoch time = 571.139, Avg epoch time=0.272, Total Time=0.407

[[0.125     ]
 [0.3125    ]
 [0.375     ]
 [0.25      ]
 [0.375     ]
 [0.3125    ]
 [0.25      ]
 [0.375     ]
 [0.4375    ]
 [0.3125    ]
 [0.375     ]
 [0.28333265]
 [0.4375    ]
 [0.4375    ]
 [0.4375    ]
 [0.3125    ]
 [0.375     ]
 [0.40860647]
 [0.375     ]
 [0.4375    ]]
Epoch 1411: at batch 1: Training dataset Loss=0.372, Batch Time=0.025
Epoch 1421: at batch 1: Training dataset Loss=0.328, Batch Time=0.032
Epoch 1431: at batch 1: Training dataset Loss=0.338, Batch Time=0.034
Epoch 1441: at batch 1: Training dataset Loss=0.378, Batch Time=0.026
Epoch 1451: at batch 1: Training dataset Loss=0.303, Batch Time=0.028
Epoch 1461: at batch 1: Training dataset Loss=0.339, Batch Time=0.027
Epoch 1471: at batch 1: Training dataset Loss=0.305, Batch Time=0.032
Epoch 1481: at batch 1: Training dataset Loss=0.305, Batch Time=0.028
Epoch 1491: at batch 1: Training dataset Loss=0.331, Batch Time=0.027
Epoch 1501: at batch 1: Training dataset Loss=0.312, Batch Time=0.026
		Epoch 1501: Epoch time = 611.882, Avg epoch time=0.274, Total Time=0.407

[[0.3125    ]
 [0.375     ]
 [0.3125    ]
 [0.21087523]
 [0.25      ]
 [0.5       ]
 [0.1875    ]
 [0.1875    ]
 [0.18700433]
 [0.625     ]
 [0.625     ]
 [0.375     ]
 [0.1875    ]
 [0.3125    ]
 [0.4375    ]
 [0.3125    ]
 [0.3125    ]
 [0.125     ]
 [0.33088842]
 [0.375     ]]
Epoch 1511: at batch 1: Training dataset Loss=0.296, Batch Time=0.029
Epoch 1521: at batch 1: Training dataset Loss=0.335, Batch Time=0.034
Epoch 1531: at batch 1: Training dataset Loss=0.284, Batch Time=0.026
Epoch 1541: at batch 1: Training dataset Loss=0.305, Batch Time=0.026
Epoch 1551: at batch 1: Training dataset Loss=0.320, Batch Time=0.029
Epoch 1561: at batch 1: Training dataset Loss=0.311, Batch Time=0.032
Epoch 1571: at batch 1: Training dataset Loss=0.392, Batch Time=0.026
Epoch 1581: at batch 1: Training dataset Loss=0.330, Batch Time=0.023
Epoch 1591: at batch 1: Training dataset Loss=0.330, Batch Time=0.030
Epoch 1601: at batch 1: Training dataset Loss=0.316, Batch Time=0.028
		Epoch 1601: Epoch time = 652.178, Avg epoch time=0.275, Total Time=0.407

[[0.5625    ]
 [0.25      ]
 [0.16181964]
 [0.25      ]
 [0.32978356]
 [0.32978356]
 [0.28192157]
 [0.25      ]
 [0.375     ]
 [0.375     ]
 [0.5       ]
 [0.375     ]
 [0.3125    ]
 [0.16181964]
 [0.375     ]
 [0.375     ]
 [0.5       ]
 [0.28531796]
 [0.25      ]
 [0.375     ]]
Epoch 1611: at batch 1: Training dataset Loss=0.378, Batch Time=0.023
Epoch 1621: at batch 1: Training dataset Loss=0.320, Batch Time=0.030
Epoch 1631: at batch 1: Training dataset Loss=0.358, Batch Time=0.027
Epoch 1641: at batch 1: Training dataset Loss=0.407, Batch Time=0.024
Epoch 1651: at batch 1: Training dataset Loss=0.366, Batch Time=0.032
Epoch 1661: at batch 1: Training dataset Loss=0.298, Batch Time=0.026
Epoch 1671: at batch 1: Training dataset Loss=0.406, Batch Time=0.027
Epoch 1681: at batch 1: Training dataset Loss=0.290, Batch Time=0.027
Epoch 1691: at batch 1: Training dataset Loss=0.368, Batch Time=0.032
Epoch 1701: at batch 1: Training dataset Loss=0.280, Batch Time=0.025
		Epoch 1701: Epoch time = 692.767, Avg epoch time=0.257, Total Time=0.407

[[0.375     ]
 [0.4375    ]
 [0.3125    ]
 [0.37186992]
 [0.25      ]
 [0.4375    ]
 [0.17066149]
 [0.1875    ]
 [0.4375    ]
 [0.25      ]
 [0.1875    ]
 [0.30460072]
 [0.4375    ]
 [0.21277186]
 [0.3125    ]
 [0.25      ]
 [0.5625    ]
 [0.375     ]
 [0.4375    ]
 [0.375     ]]
Epoch 1711: at batch 1: Training dataset Loss=0.277, Batch Time=0.029
Epoch 1721: at batch 1: Training dataset Loss=0.320, Batch Time=0.031
Epoch 1731: at batch 1: Training dataset Loss=0.342, Batch Time=0.024
Epoch 1741: at batch 1: Training dataset Loss=0.312, Batch Time=0.026
Epoch 1751: at batch 1: Training dataset Loss=0.387, Batch Time=0.029
Epoch 1761: at batch 1: Training dataset Loss=0.283, Batch Time=0.025
Epoch 1771: at batch 1: Training dataset Loss=0.277, Batch Time=0.029
Epoch 1781: at batch 1: Training dataset Loss=0.290, Batch Time=0.033
Epoch 1791: at batch 1: Training dataset Loss=0.302, Batch Time=0.025
Epoch 1801: at batch 1: Training dataset Loss=0.359, Batch Time=0.032
		Epoch 1801: Epoch time = 733.373, Avg epoch time=0.284, Total Time=0.407

[[0.375     ]
 [0.4375    ]
 [0.23880392]
 [0.33728993]
 [0.47320503]
 [0.25      ]
 [0.4375    ]
 [0.375     ]
 [0.3125    ]
 [0.125     ]
 [0.3125    ]
 [0.4375    ]
 [0.125     ]
 [0.3125    ]
 [0.3125    ]
 [0.47320503]
 [0.4375    ]
 [0.3125    ]
 [0.3125    ]
 [0.4375    ]]
Epoch 1811: at batch 1: Training dataset Loss=0.338, Batch Time=0.029
Epoch 1821: at batch 1: Training dataset Loss=0.295, Batch Time=0.029
Epoch 1831: at batch 1: Training dataset Loss=0.344, Batch Time=0.028
Epoch 1841: at batch 1: Training dataset Loss=0.340, Batch Time=0.028
Epoch 1851: at batch 1: Training dataset Loss=0.284, Batch Time=0.033
Epoch 1861: at batch 1: Training dataset Loss=0.351, Batch Time=0.032
Epoch 1871: at batch 1: Training dataset Loss=0.309, Batch Time=0.032
Epoch 1881: at batch 1: Training dataset Loss=0.320, Batch Time=0.029
Epoch 1891: at batch 1: Training dataset Loss=0.340, Batch Time=0.029
Epoch 1901: at batch 1: Training dataset Loss=0.334, Batch Time=0.028
		Epoch 1901: Epoch time = 774.221, Avg epoch time=0.251, Total Time=0.407

[[0.5625    ]
 [0.19492766]
 [0.5625    ]
 [0.25      ]
 [0.4375    ]
 [0.375     ]
 [0.375     ]
 [0.4375    ]
 [0.3125    ]
 [0.1875    ]
 [0.4375    ]
 [0.5625    ]
 [0.25      ]
 [0.24715675]
 [0.4375    ]
 [0.375     ]
 [0.4375    ]
 [0.5625    ]
 [0.1875    ]
 [0.4375    ]]
Epoch 1911: at batch 1: Training dataset Loss=0.324, Batch Time=0.029
Epoch 1921: at batch 1: Training dataset Loss=0.324, Batch Time=0.027
Epoch 1931: at batch 1: Training dataset Loss=0.325, Batch Time=0.031
Epoch 1941: at batch 1: Training dataset Loss=0.310, Batch Time=0.030
Epoch 1951: at batch 1: Training dataset Loss=0.316, Batch Time=0.029
Epoch 1961: at batch 1: Training dataset Loss=0.354, Batch Time=0.026
Epoch 1971: at batch 1: Training dataset Loss=0.299, Batch Time=0.031
Epoch 1981: at batch 1: Training dataset Loss=0.369, Batch Time=0.025
Epoch 1991: at batch 1: Training dataset Loss=0.292, Batch Time=0.027
Epoch 2001: at batch 1: Training dataset Loss=0.382, Batch Time=0.030
		Epoch 2001: Epoch time = 815.156, Avg epoch time=0.270, Total Time=0.407

[[0.59093338]
 [0.375     ]
 [0.375     ]
 [0.0625    ]
 [0.32389221]
 [0.1875    ]
 [0.25      ]
 [0.5       ]
 [0.125     ]
 [0.125     ]
 [0.375     ]
 [0.4375    ]
 [0.1875    ]
 [0.0625    ]
 [0.4375    ]
 [0.39405489]
 [0.0625    ]
 [0.50285411]
 [0.3125    ]
 [0.39405489]]
Epoch 2011: at batch 1: Training dataset Loss=0.316, Batch Time=0.032
Epoch 2021: at batch 1: Training dataset Loss=0.309, Batch Time=0.028
Epoch 2031: at batch 1: Training dataset Loss=0.334, Batch Time=0.026
Epoch 2041: at batch 1: Training dataset Loss=0.390, Batch Time=0.029
Epoch 2051: at batch 1: Training dataset Loss=0.316, Batch Time=0.028
Epoch 2061: at batch 1: Training dataset Loss=0.308, Batch Time=0.031
Epoch 2071: at batch 1: Training dataset Loss=0.366, Batch Time=0.026
Epoch 2081: at batch 1: Training dataset Loss=0.288, Batch Time=0.031
Epoch 2091: at batch 1: Training dataset Loss=0.320, Batch Time=0.032
Epoch 2101: at batch 1: Training dataset Loss=0.391, Batch Time=0.029
		Epoch 2101: Epoch time = 855.822, Avg epoch time=0.275, Total Time=0.407

[[0.4375    ]
 [0.375     ]
 [0.28034294]
 [0.3125    ]
 [0.3125    ]
 [0.40341085]
 [0.24781494]
 [0.24781494]
 [0.24781494]
 [0.25      ]
 [0.3125    ]
 [1.06811333]
 [0.375     ]
 [0.27956581]
 [0.21556403]
 [0.3125    ]
 [0.3125    ]
 [0.28034294]
 [0.28034294]
 [0.3125    ]]
Epoch 2111: at batch 1: Training dataset Loss=0.309, Batch Time=0.025
Epoch 2121: at batch 1: Training dataset Loss=0.364, Batch Time=0.024
Epoch 2131: at batch 1: Training dataset Loss=0.361, Batch Time=0.031
Epoch 2141: at batch 1: Training dataset Loss=0.329, Batch Time=0.030
Epoch 2151: at batch 1: Training dataset Loss=0.317, Batch Time=0.023
Epoch 2161: at batch 1: Training dataset Loss=0.304, Batch Time=0.037
Epoch 2171: at batch 1: Training dataset Loss=0.334, Batch Time=0.031
Epoch 2181: at batch 1: Training dataset Loss=0.316, Batch Time=0.031
Epoch 2191: at batch 1: Training dataset Loss=0.379, Batch Time=0.028
Epoch 2201: at batch 1: Training dataset Loss=0.316, Batch Time=0.029
		Epoch 2201: Epoch time = 896.166, Avg epoch time=0.296, Total Time=0.407

[[0.31466585]
 [0.3125    ]
 [0.23102629]
 [0.3125    ]
 [0.25      ]
 [0.37417918]
 [0.30796805]
 [0.16666707]
 [0.28437066]
 [0.30796805]
 [0.3125    ]
 [0.16666707]
 [0.3521744 ]
 [0.3125    ]
 [0.3125    ]
 [0.10676502]
 [0.375     ]
 [0.375     ]
 [0.25      ]
 [0.16666707]]
Epoch 2211: at batch 1: Training dataset Loss=0.325, Batch Time=0.025
Epoch 2221: at batch 1: Training dataset Loss=0.339, Batch Time=0.029
Epoch 2231: at batch 1: Training dataset Loss=0.342, Batch Time=0.034
Epoch 2241: at batch 1: Training dataset Loss=0.380, Batch Time=0.035
Epoch 2251: at batch 1: Training dataset Loss=0.314, Batch Time=0.028
Epoch 2261: at batch 1: Training dataset Loss=0.353, Batch Time=0.032
Epoch 2271: at batch 1: Training dataset Loss=0.319, Batch Time=0.027
Epoch 2281: at batch 1: Training dataset Loss=0.364, Batch Time=0.028
Epoch 2291: at batch 1: Training dataset Loss=0.306, Batch Time=0.026
Epoch 2301: at batch 1: Training dataset Loss=0.321, Batch Time=0.024
		Epoch 2301: Epoch time = 936.571, Avg epoch time=0.258, Total Time=0.407

[[0.3125    ]
 [0.5       ]
 [0.17195493]
 [0.1875    ]
 [0.375     ]
 [0.17195493]
 [0.3125    ]
 [0.375     ]
 [0.5625    ]
 [0.33425212]
 [0.375     ]
 [0.375     ]
 [0.25      ]
 [0.35088181]
 [0.35088181]
 [0.27845451]
 [0.375     ]
 [0.375     ]
 [0.22641906]
 [0.3189674 ]]
Epoch 2311: at batch 1: Training dataset Loss=0.342, Batch Time=0.030
Epoch 2321: at batch 1: Training dataset Loss=0.319, Batch Time=0.030
Epoch 2331: at batch 1: Training dataset Loss=0.346, Batch Time=0.031
Epoch 2341: at batch 1: Training dataset Loss=0.307, Batch Time=0.024
Epoch 2351: at batch 1: Training dataset Loss=0.319, Batch Time=0.026
Epoch 2361: at batch 1: Training dataset Loss=0.289, Batch Time=0.032
Epoch 2371: at batch 1: Training dataset Loss=0.337, Batch Time=0.034
Epoch 2381: at batch 1: Training dataset Loss=0.341, Batch Time=0.031
Epoch 2391: at batch 1: Training dataset Loss=0.380, Batch Time=0.032
Epoch 2401: at batch 1: Training dataset Loss=0.306, Batch Time=0.031
		Epoch 2401: Epoch time = 977.181, Avg epoch time=0.265, Total Time=0.407

[[0.23120281]
 [0.29210949]
 [0.4375    ]
 [0.29210949]
 [0.41466498]
 [0.375     ]
 [0.23726055]
 [0.375     ]
 [0.29210949]
 [0.23726055]
 [0.28147164]
 [0.375     ]
 [0.375     ]
 [0.29210949]
 [0.1875    ]
 [0.25      ]
 [0.3125    ]
 [0.29210949]
 [0.23120281]
 [0.4375    ]]
Epoch 2411: at batch 1: Training dataset Loss=0.336, Batch Time=0.034
Epoch 2421: at batch 1: Training dataset Loss=0.318, Batch Time=0.031
Epoch 2431: at batch 1: Training dataset Loss=0.339, Batch Time=0.033
Epoch 2441: at batch 1: Training dataset Loss=0.296, Batch Time=0.023
Epoch 2451: at batch 1: Training dataset Loss=0.347, Batch Time=0.032
Epoch 2461: at batch 1: Training dataset Loss=0.343, Batch Time=0.026
Epoch 2471: at batch 1: Training dataset Loss=0.318, Batch Time=0.031
Epoch 2481: at batch 1: Training dataset Loss=0.344, Batch Time=0.029
Epoch 2491: at batch 1: Training dataset Loss=0.322, Batch Time=0.024
Epoch 2501: at batch 1: Training dataset Loss=0.315, Batch Time=0.026
		Epoch 2501: Epoch time = 1017.457, Avg epoch time=0.270, Total Time=0.407

[[0.3125    ]
 [0.24321085]
 [0.24321085]
 [0.3125    ]
 [0.1875    ]
 [0.34177262]
 [0.5       ]
 [0.375     ]
 [0.24179041]
 [0.25      ]
 [0.34177262]
 [0.24412233]
 [0.24412233]
 [0.24179041]
 [0.20350686]
 [0.34177262]
 [0.34177262]
 [0.3125    ]
 [0.28952289]
 [0.28952289]]
Epoch 2511: at batch 1: Training dataset Loss=0.309, Batch Time=0.025
Epoch 2521: at batch 1: Training dataset Loss=0.328, Batch Time=0.026
Epoch 2531: at batch 1: Training dataset Loss=0.335, Batch Time=0.024
Epoch 2541: at batch 1: Training dataset Loss=0.318, Batch Time=0.028
Epoch 2551: at batch 1: Training dataset Loss=0.367, Batch Time=0.029
Epoch 2561: at batch 1: Training dataset Loss=0.352, Batch Time=0.025
Epoch 2571: at batch 1: Training dataset Loss=0.325, Batch Time=0.024
Epoch 2581: at batch 1: Training dataset Loss=0.322, Batch Time=0.029
Epoch 2591: at batch 1: Training dataset Loss=0.289, Batch Time=0.032
Epoch 2601: at batch 1: Training dataset Loss=0.363, Batch Time=0.032
		Epoch 2601: Epoch time = 1058.038, Avg epoch time=0.248, Total Time=0.407

[[0.48080432]
 [0.23875104]
 [0.22113958]
 [0.26821971]
 [0.375     ]
 [0.26821971]
 [0.23875104]
 [0.13188297]
 [0.16747636]
 [0.4738822 ]
 [0.4375    ]
 [0.25      ]
 [0.38547581]
 [0.375     ]
 [0.35227901]
 [0.23875104]
 [0.35227901]
 [0.26821971]
 [0.3125    ]
 [0.23875104]]
Epoch 2611: at batch 1: Training dataset Loss=0.345, Batch Time=0.029
Epoch 2621: at batch 1: Training dataset Loss=0.337, Batch Time=0.023
Epoch 2631: at batch 1: Training dataset Loss=0.337, Batch Time=0.029
Epoch 2641: at batch 1: Training dataset Loss=0.363, Batch Time=0.029
Epoch 2651: at batch 1: Training dataset Loss=0.349, Batch Time=0.032
Epoch 2661: at batch 1: Training dataset Loss=0.309, Batch Time=0.031
Epoch 2671: at batch 1: Training dataset Loss=0.328, Batch Time=0.031
Epoch 2681: at batch 1: Training dataset Loss=0.326, Batch Time=0.030
Epoch 2691: at batch 1: Training dataset Loss=0.319, Batch Time=0.027
Epoch 2701: at batch 1: Training dataset Loss=0.360, Batch Time=0.023
		Epoch 2701: Epoch time = 1098.587, Avg epoch time=0.274, Total Time=0.407

[[0.1875    ]
 [0.4375    ]
 [0.5625    ]
 [0.3125    ]
 [0.16582061]
 [0.5625    ]
 [0.5       ]
 [0.28763586]
 [0.16582061]
 [0.25      ]
 [0.5625    ]
 [0.29241526]
 [0.25      ]
 [0.5       ]
 [0.41835922]
 [0.4375    ]
 [0.5625    ]
 [0.25      ]
 [0.29241526]
 [0.3125    ]]
Epoch 2711: at batch 1: Training dataset Loss=0.258, Batch Time=0.028
Epoch 2721: at batch 1: Training dataset Loss=0.308, Batch Time=0.027
Epoch 2731: at batch 1: Training dataset Loss=0.333, Batch Time=0.025
Epoch 2741: at batch 1: Training dataset Loss=0.325, Batch Time=0.029
Epoch 2751: at batch 1: Training dataset Loss=0.318, Batch Time=0.031
Epoch 2761: at batch 1: Training dataset Loss=0.327, Batch Time=0.027
Epoch 2771: at batch 1: Training dataset Loss=0.314, Batch Time=0.029
Epoch 2781: at batch 1: Training dataset Loss=0.311, Batch Time=0.032
Epoch 2791: at batch 1: Training dataset Loss=0.294, Batch Time=0.032
Epoch 2801: at batch 1: Training dataset Loss=0.320, Batch Time=0.027
		Epoch 2801: Epoch time = 1139.061, Avg epoch time=0.261, Total Time=0.407

[[0.3125    ]
 [0.25544709]
 [0.1875    ]
 [0.3125    ]
 [0.16855794]
 [0.3125    ]
 [0.375     ]
 [0.26170298]
 [0.3125    ]
 [0.3125    ]
 [0.25      ]
 [0.23837359]
 [0.3125    ]
 [0.25      ]
 [0.625     ]
 [0.5625    ]
 [0.25544709]
 [0.25      ]
 [0.26170298]
 [0.3125    ]]
Epoch 2811: at batch 1: Training dataset Loss=0.314, Batch Time=0.030
Epoch 2821: at batch 1: Training dataset Loss=0.251, Batch Time=0.030
Epoch 2831: at batch 1: Training dataset Loss=0.321, Batch Time=0.022
Epoch 2841: at batch 1: Training dataset Loss=0.302, Batch Time=0.027
Epoch 2851: at batch 1: Training dataset Loss=0.331, Batch Time=0.032
Epoch 2861: at batch 1: Training dataset Loss=0.381, Batch Time=0.032
Epoch 2871: at batch 1: Training dataset Loss=0.297, Batch Time=0.032
Epoch 2881: at batch 1: Training dataset Loss=0.298, Batch Time=0.026
Epoch 2891: at batch 1: Training dataset Loss=0.330, Batch Time=0.026
Epoch 2901: at batch 1: Training dataset Loss=0.298, Batch Time=0.023
		Epoch 2901: Epoch time = 1179.653, Avg epoch time=0.238, Total Time=0.406

[[0.30890864]
 [0.4375    ]
 [0.375     ]
 [0.4375    ]
 [0.20098105]
 [0.0625    ]
 [0.4375    ]
 [0.375     ]
 [0.375     ]
 [0.1875    ]
 [0.20098105]
 [0.375     ]
 [0.375     ]
 [0.375     ]
 [0.23608978]
 [0.21119167]
 [0.30890864]
 [0.4375    ]
 [0.25      ]
 [0.28766471]]
Epoch 2911: at batch 1: Training dataset Loss=0.301, Batch Time=0.030
Epoch 2921: at batch 1: Training dataset Loss=0.308, Batch Time=0.029
Epoch 2931: at batch 1: Training dataset Loss=0.310, Batch Time=0.025
Epoch 2941: at batch 1: Training dataset Loss=0.365, Batch Time=0.029
Epoch 2951: at batch 1: Training dataset Loss=0.322, Batch Time=0.035
Epoch 2961: at batch 1: Training dataset Loss=0.328, Batch Time=0.029
Epoch 2971: at batch 1: Training dataset Loss=0.292, Batch Time=0.025
Epoch 2981: at batch 1: Training dataset Loss=0.351, Batch Time=0.030
Epoch 2991: at batch 1: Training dataset Loss=0.310, Batch Time=0.026
Epoch 3001: at batch 1: Training dataset Loss=0.298, Batch Time=0.031
		Epoch 3001: Epoch time = 1220.379, Avg epoch time=0.279, Total Time=0.407

[[0.44882506]
 [0.2823678 ]
 [0.44882506]
 [0.4375    ]
 [0.38815802]
 [0.2823678 ]
 [0.3125    ]
 [0.27879697]
 [0.44882506]
 [0.48140973]
 [0.25      ]
 [0.38815802]
 [0.3125    ]
 [0.2823678 ]
 [0.14943242]
 [0.28807384]
 [0.32994974]
 [0.28807384]
 [0.3125    ]
 [0.38815802]]
Epoch 3011: at batch 1: Training dataset Loss=0.298, Batch Time=0.029
Epoch 3021: at batch 1: Training dataset Loss=0.364, Batch Time=0.026
Epoch 3031: at batch 1: Training dataset Loss=0.323, Batch Time=0.024
Epoch 3041: at batch 1: Training dataset Loss=0.338, Batch Time=0.030
Epoch 3051: at batch 1: Training dataset Loss=0.287, Batch Time=0.027
Epoch 3061: at batch 1: Training dataset Loss=0.353, Batch Time=0.026
Epoch 3071: at batch 1: Training dataset Loss=0.342, Batch Time=0.033
Epoch 3081: at batch 1: Training dataset Loss=0.321, Batch Time=0.027
Epoch 3091: at batch 1: Training dataset Loss=0.380, Batch Time=0.029
Epoch 3101: at batch 1: Training dataset Loss=0.321, Batch Time=0.034
		Epoch 3101: Epoch time = 1261.200, Avg epoch time=0.317, Total Time=0.407

[[0.3125    ]
 [0.361105  ]
 [0.3837986 ]
 [0.25      ]
 [0.29599363]
 [0.3125    ]
 [0.3125    ]
 [0.24502023]
 [0.26353115]
 [0.26353115]
 [0.375     ]
 [0.3125    ]
 [0.3837986 ]
 [0.3125    ]
 [0.31671777]
 [0.21394104]
 [0.375     ]
 [0.3837986 ]
 [0.31671777]
 [0.43579346]]
Epoch 3111: at batch 1: Training dataset Loss=0.278, Batch Time=0.027
Epoch 3121: at batch 1: Training dataset Loss=0.267, Batch Time=0.026
Epoch 3131: at batch 1: Training dataset Loss=0.288, Batch Time=0.029
Epoch 3141: at batch 1: Training dataset Loss=0.321, Batch Time=0.031
Epoch 3151: at batch 1: Training dataset Loss=0.302, Batch Time=0.024
Epoch 3161: at batch 1: Training dataset Loss=0.375, Batch Time=0.027
Epoch 3171: at batch 1: Training dataset Loss=0.281, Batch Time=0.028
Epoch 3181: at batch 1: Training dataset Loss=0.316, Batch Time=0.032
Epoch 3191: at batch 1: Training dataset Loss=0.297, Batch Time=0.032
Epoch 3201: at batch 1: Training dataset Loss=0.306, Batch Time=0.029
		Epoch 3201: Epoch time = 1301.760, Avg epoch time=0.272, Total Time=0.407

[[0.375     ]
 [0.3125    ]
 [0.28720239]
 [0.0625    ]
 [0.125     ]
 [0.36778986]
 [0.4375    ]
 [0.3125    ]
 [0.35131073]
 [0.375     ]
 [0.5       ]
 [0.3125    ]
 [0.375     ]
 [0.24801779]
 [0.375     ]
 [0.24801779]
 [0.29680154]
 [0.35416767]
 [0.23732574]
 [0.375     ]]
Epoch 3211: at batch 1: Training dataset Loss=0.344, Batch Time=0.029
Epoch 3221: at batch 1: Training dataset Loss=0.363, Batch Time=0.029
Epoch 3231: at batch 1: Training dataset Loss=0.272, Batch Time=0.026
Epoch 3241: at batch 1: Training dataset Loss=0.328, Batch Time=0.033
Epoch 3251: at batch 1: Training dataset Loss=0.341, Batch Time=0.031
Epoch 3261: at batch 1: Training dataset Loss=0.345, Batch Time=0.029
Epoch 3271: at batch 1: Training dataset Loss=0.295, Batch Time=0.031
Epoch 3281: at batch 1: Training dataset Loss=0.306, Batch Time=0.025
Epoch 3291: at batch 1: Training dataset Loss=0.305, Batch Time=0.031
Epoch 3301: at batch 1: Training dataset Loss=0.317, Batch Time=0.030
		Epoch 3301: Epoch time = 1342.463, Avg epoch time=0.262, Total Time=0.407

[[0.3125    ]
 [0.37368059]
 [0.3125    ]
 [0.22347808]
 [0.1875    ]
 [0.3125    ]
 [0.31545448]
 [0.24133144]
 [0.47354656]
 [0.3931393 ]
 [0.3125    ]
 [0.4375    ]
 [0.30789778]
 [0.3690528 ]
 [0.25      ]
 [0.1875    ]
 [0.1875    ]
 [0.3931393 ]
 [0.3931393 ]
 [0.3931393 ]]
Epoch 3311: at batch 1: Training dataset Loss=0.340, Batch Time=0.023
Epoch 3321: at batch 1: Training dataset Loss=0.279, Batch Time=0.032
Epoch 3331: at batch 1: Training dataset Loss=0.279, Batch Time=0.027
Epoch 3341: at batch 1: Training dataset Loss=0.334, Batch Time=0.025
Epoch 3351: at batch 1: Training dataset Loss=0.306, Batch Time=0.034
Epoch 3361: at batch 1: Training dataset Loss=0.294, Batch Time=0.024
Epoch 3371: at batch 1: Training dataset Loss=0.308, Batch Time=0.029
Epoch 3381: at batch 1: Training dataset Loss=0.352, Batch Time=0.029
Epoch 3391: at batch 1: Training dataset Loss=0.328, Batch Time=0.026
Epoch 3401: at batch 1: Training dataset Loss=0.339, Batch Time=0.029
		Epoch 3401: Epoch time = 1383.205, Avg epoch time=0.268, Total Time=0.407

[[0.25      ]
 [0.46157092]
 [0.23564881]
 [0.3125    ]
 [0.375     ]
 [0.3125    ]
 [0.23147984]
 [0.3125    ]
 [0.3125    ]
 [0.1872451 ]
 [0.4752337 ]
 [0.25      ]
 [0.37794125]
 [0.4375    ]
 [0.5       ]
 [0.32382005]
 [0.21663642]
 [0.5       ]
 [0.51193696]
 [0.37850773]]
Epoch 3411: at batch 1: Training dataset Loss=0.335, Batch Time=0.030
Epoch 3421: at batch 1: Training dataset Loss=0.306, Batch Time=0.028
Epoch 3431: at batch 1: Training dataset Loss=0.315, Batch Time=0.024
Epoch 3441: at batch 1: Training dataset Loss=0.317, Batch Time=0.029
Epoch 3451: at batch 1: Training dataset Loss=0.282, Batch Time=0.023
Epoch 3461: at batch 1: Training dataset Loss=0.248, Batch Time=0.029
Epoch 3471: at batch 1: Training dataset Loss=0.312, Batch Time=0.026
Epoch 3481: at batch 1: Training dataset Loss=0.326, Batch Time=0.023
Epoch 3491: at batch 1: Training dataset Loss=0.311, Batch Time=0.027
Epoch 3501: at batch 1: Training dataset Loss=0.311, Batch Time=0.027
		Epoch 3501: Epoch time = 1423.724, Avg epoch time=0.279, Total Time=0.407

[[0.34730542]
 [0.125     ]
 [0.34730542]
 [0.5625    ]
 [0.0625    ]
 [0.26938006]
 [0.44334477]
 [0.24564576]
 [0.0625    ]
 [0.22982858]
 [0.5       ]
 [0.44334477]
 [0.25      ]
 [0.25      ]
 [0.375     ]
 [0.34730542]
 [0.34730542]
 [0.20674802]
 [0.5778206 ]
 [0.125     ]]
Epoch 3511: at batch 1: Training dataset Loss=0.308, Batch Time=0.026
Epoch 3521: at batch 1: Training dataset Loss=0.291, Batch Time=0.032
Epoch 3531: at batch 1: Training dataset Loss=0.307, Batch Time=0.029
srun: Force Terminated job 402750
slurmstepd: error: *** STEP 402750.0 ON gr011-ib0 CANCELLED AT 2020-12-12T16:07:15 DUE TO TIME LIMIT ***
client_loop: send disconnect: Broken pipe
(base) [ir967@gr011 Learning-to-See-in-the-Dark]$ exit
srun: error: gr011-ib0: task 0: Exited with exit code 255
srun: Terminating job step 402750.0
(base) [ir967@log-2 Learning-to-See-in-the-Dark]$ 
