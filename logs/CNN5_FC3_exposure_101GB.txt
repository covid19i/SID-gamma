(sid2) [ir967@gr046 Learning-to-See-in-the-Dark]$ vi CNN5_FC3_exposure_101GB.py
(sid2) [ir967@gr046 Learning-to-See-in-the-Dark]$ python CNN5_FC3_exposure_101GB.py




Current date and time : 
2020-12-13 16:56:10
./dataset/Sony/short/00133_07_0.1s.ARW 133 0.1
./dataset/Sony/short/00015_09_0.1s.ARW 15 0.1
./dataset/Sony/short/00190_08_0.04s.ARW 190 0.04
./dataset/Sony/short/00204_01_0.033s.ARW 204 0.033
./dataset/Sony/short/00047_03_0.1s.ARW 47 0.1
./dataset/Sony/short/00049_07_0.1s.ARW 49 0.1
./dataset/Sony/short/00231_00_0.033s.ARW 231 0.033
Found 1865 images to train with

Training on files with indices: 191 to 1523
Training on 1332 images only

2020-12-13 16:56:10.491835: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-13 16:56:10.648836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-13 16:56:10.648867: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-13 16:56:10.938851: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-13 16:56:10.938885: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-13 16:56:10.938902: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-13 16:56:10.938996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)
No checkpoint found at ./gt_Sony_CNN5_FC3_exposure_101GB/. Hence, will create the folder.

last epoch of previous run: 0


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 83 ,ps 128 ,result_dir= ./gt_Sony_CNN5_FC3_exposure_101GB/ ,len(train_fns)= 1332

Cleared all images in memory.

Starting Training on index [ 662  361 1235  423  627  282  720 1226  934 1019  186  680  751   88
  570 1307]
dataset index: ['./dataset/Sony/short/00205_01_0.1s.ARW'
 './dataset/Sony/short/00141_05_0.1s.ARW'
 './dataset/Sony/short/00202_02_0.04s.ARW'
 './dataset/Sony/short/00221_01_0.033s.ARW'
 './dataset/Sony/short/00200_00_0.033s.ARW'
 './dataset/Sony/short/00133_00_0.1s.ARW'
 './dataset/Sony/short/00182_00_0.1s.ARW'
 './dataset/Sony/short/00084_09_0.1s.ARW'
 './dataset/Sony/short/00205_00_0.033s.ARW'
 './dataset/Sony/short/00078_03_0.1s.ARW'
 './dataset/Sony/short/00053_00_0.04s.ARW'
 './dataset/Sony/short/00062_03_0.1s.ARW'
 './dataset/Sony/short/00037_03_0.1s.ARW'
 './dataset/Sony/short/00151_05_0.1s.ARW'
 './dataset/Sony/short/00009_05_0.1s.ARW'
 './dataset/Sony/short/00223_00_0.033s.ARW']
Starting Training on exposures [0.1   0.1   0.04  0.033 0.033 0.1   0.1   0.1   0.033 0.1   0.04  0.1
 0.1   0.1   0.1   0.033]
Epoch 0: at batch 1: Training dataset Loss=1.002387, Batch Time=2.739
Epoch 0: at batch 1: Training dataset Loss=1.002387, Batch Time=2.739; Early rounds
Epoch 0: at batch 2: Training dataset Loss=1.012291, Batch Time=0.029; Early rounds
Epoch 0: at batch 3: Training dataset Loss=1.009830, Batch Time=0.028; Early rounds
loading ./dataset/Sony/short/00218_00_0.04s.ARW; images_in_memory= 61
rawpy read the 1201th file at location: 00218_00_0.04s.ARW
Epoch 0: at batch 4: Training dataset Loss=1.005655, Batch Time=0.031; Early rounds
Epoch 0: at batch 5: Training dataset Loss=1.004559, Batch Time=0.029; Early rounds
Epoch 0: at batch 6: Training dataset Loss=1.020582, Batch Time=0.027; Early rounds
Epoch 0: at batch 7: Training dataset Loss=1.038659, Batch Time=0.027; Early rounds
Epoch 0: at batch 8: Training dataset Loss=1.034910, Batch Time=0.029; Early rounds
Epoch 0: at batch 9: Training dataset Loss=1.033324, Batch Time=0.026; Early rounds
Epoch 0: at batch 10: Training dataset Loss=1.039758, Batch Time=0.025; Early rounds
Epoch 0: at batch 11: Training dataset Loss=1.057097, Batch Time=0.025; Early rounds
Epoch 0: at batch 12: Training dataset Loss=1.058985, Batch Time=0.030; Early rounds
Epoch 0: at batch 13: Training dataset Loss=1.055153, Batch Time=0.030; Early rounds
Epoch 0: at batch 14: Training dataset Loss=1.060337, Batch Time=0.031; Early rounds
Epoch 0: at batch 15: Training dataset Loss=1.065678, Batch Time=0.030; Early rounds
Epoch 0: at batch 16: Training dataset Loss=1.065748, Batch Time=0.029; Early rounds
Epoch 0: at batch 17: Training dataset Loss=1.069581, Batch Time=0.030; Early rounds
Epoch 0: at batch 18: Training dataset Loss=1.082025, Batch Time=0.029; Early rounds
Epoch 0: at batch 19: Training dataset Loss=1.084139, Batch Time=0.027; Early rounds
Epoch 0: at batch 20: Training dataset Loss=1.082436, Batch Time=0.029; Early rounds
Epoch 0: at batch 21: Training dataset Loss=1.075009, Batch Time=0.026; Early rounds
Epoch 0: at batch 22: Training dataset Loss=1.068399, Batch Time=0.025; Early rounds
Epoch 0: at batch 23: Training dataset Loss=1.064011, Batch Time=0.026; Early rounds
Epoch 0: at batch 24: Training dataset Loss=1.065567, Batch Time=0.025; Early rounds
Epoch 0: at batch 25: Training dataset Loss=1.060669, Batch Time=0.027; Early rounds
Epoch 0: at batch 26: Training dataset Loss=1.055789, Batch Time=0.029; Early rounds
Epoch 0: at batch 27: Training dataset Loss=1.051117, Batch Time=0.032; Early rounds
Epoch 0: at batch 28: Training dataset Loss=1.043427, Batch Time=0.029; Early rounds
Epoch 0: at batch 29: Training dataset Loss=1.041620, Batch Time=0.028; Early rounds
Epoch 0: at batch 30: Training dataset Loss=1.035515, Batch Time=0.029; Early rounds
Epoch 0: at batch 31: Training dataset Loss=1.031846, Batch Time=0.026; Early rounds
Epoch 0: at batch 32: Training dataset Loss=1.023670, Batch Time=0.027; Early rounds
Epoch 0: at batch 33: Training dataset Loss=1.019993, Batch Time=0.030; Early rounds
Epoch 0: at batch 34: Training dataset Loss=1.013334, Batch Time=0.030; Early rounds
Epoch 0: at batch 35: Training dataset Loss=1.006078, Batch Time=0.028; Early rounds
Epoch 0: at batch 36: Training dataset Loss=0.997992, Batch Time=0.030; Early rounds
Epoch 0: at batch 37: Training dataset Loss=0.991043, Batch Time=0.028; Early rounds
Epoch 0: at batch 38: Training dataset Loss=0.985610, Batch Time=0.029; Early rounds
Epoch 0: at batch 39: Training dataset Loss=0.983546, Batch Time=0.031; Early rounds
loading ./dataset/Sony/short/00223_07_0.04s.ARW; images_in_memory= 498
rawpy read the 201th file at location: 00223_07_0.04s.ARW
Epoch 0: at batch 40: Training dataset Loss=0.980452, Batch Time=0.032; Early rounds
Epoch 0: at batch 41: Training dataset Loss=0.977711, Batch Time=0.028; Early rounds
Epoch 0: at batch 42: Training dataset Loss=0.997547, Batch Time=0.027; Early rounds
Epoch 0: at batch 43: Training dataset Loss=0.991750, Batch Time=0.027; Early rounds
Epoch 0: at batch 44: Training dataset Loss=0.984248, Batch Time=0.030; Early rounds
loading ./dataset/Sony/short/00137_00_0.1s.ARW; images_in_memory= 543
rawpy read the 1001th file at location: 00137_00_0.1s.ARW
Epoch 0: at batch 45: Training dataset Loss=0.984311, Batch Time=0.030; Early rounds
Epoch 0: at batch 46: Training dataset Loss=0.982022, Batch Time=0.031; Early rounds
Epoch 0: at batch 47: Training dataset Loss=0.983203, Batch Time=0.031; Early rounds
Epoch 0: at batch 48: Training dataset Loss=0.982666, Batch Time=0.029; Early rounds
loading ./dataset/Sony/short/00151_09_0.1s.ARW; images_in_memory= 581
rawpy read the 801th file at location: 00151_09_0.1s.ARW
Epoch 0: at batch 49: Training dataset Loss=0.975997, Batch Time=0.025; Early rounds
Epoch 0: at batch 50: Training dataset Loss=0.969533, Batch Time=0.030; Early rounds
Epoch 0: at batch 51: Training dataset Loss=0.962918, Batch Time=0.031; Early rounds
Epoch 0: at batch 52: Training dataset Loss=0.959259, Batch Time=0.032; Early rounds
Epoch 0: at batch 53: Training dataset Loss=0.957985, Batch Time=0.028; Early rounds
Epoch 0: at batch 54: Training dataset Loss=0.954794, Batch Time=0.027; Early rounds
Epoch 0: at batch 55: Training dataset Loss=0.948729, Batch Time=0.031; Early rounds
Epoch 0: at batch 56: Training dataset Loss=0.946032, Batch Time=0.031; Early rounds
Epoch 0: at batch 57: Training dataset Loss=0.938089, Batch Time=0.029; Early rounds
Epoch 0: at batch 58: Training dataset Loss=0.932194, Batch Time=0.029; Early rounds
Epoch 0: at batch 59: Training dataset Loss=0.922527, Batch Time=0.026; Early rounds
Epoch 0: at batch 60: Training dataset Loss=0.920907, Batch Time=0.027; Early rounds
Epoch 0: at batch 61: Training dataset Loss=0.917179, Batch Time=0.027; Early rounds
Epoch 0: at batch 62: Training dataset Loss=0.922860, Batch Time=0.030; Early rounds
Epoch 0: at batch 63: Training dataset Loss=0.915976, Batch Time=0.028; Early rounds
Epoch 0: at batch 64: Training dataset Loss=0.911952, Batch Time=0.026; Early rounds
Epoch 0: at batch 65: Training dataset Loss=0.907007, Batch Time=0.027; Early rounds
Epoch 0: at batch 66: Training dataset Loss=0.905206, Batch Time=0.024; Early rounds
Epoch 0: at batch 67: Training dataset Loss=0.901825, Batch Time=0.025; Early rounds
Epoch 0: at batch 68: Training dataset Loss=0.898264, Batch Time=0.025; Early rounds
Epoch 0: at batch 69: Training dataset Loss=0.896409, Batch Time=0.027; Early rounds
Epoch 0: at batch 70: Training dataset Loss=0.893568, Batch Time=0.028; Early rounds
Epoch 0: at batch 71: Training dataset Loss=0.892064, Batch Time=0.029; Early rounds
Epoch 0: at batch 72: Training dataset Loss=0.886763, Batch Time=0.026; Early rounds
Epoch 0: at batch 73: Training dataset Loss=0.884089, Batch Time=0.030; Early rounds
Epoch 0: at batch 74: Training dataset Loss=0.879945, Batch Time=0.027; Early rounds
Epoch 0: at batch 75: Training dataset Loss=0.875922, Batch Time=0.029; Early rounds
Epoch 0: at batch 76: Training dataset Loss=0.879029, Batch Time=0.030; Early rounds
Epoch 0: at batch 77: Training dataset Loss=0.879236, Batch Time=0.031; Early rounds
Epoch 0: at batch 78: Training dataset Loss=0.876819, Batch Time=0.030; Early rounds
Epoch 0: at batch 79: Training dataset Loss=0.868939, Batch Time=0.026; Early rounds
Epoch 0: at batch 80: Training dataset Loss=0.865069, Batch Time=0.025; Early rounds
Found in memory: ./dataset/Sony/short/00151_09_0.1s.ARW; images_in_memory= 805
Epoch 0: at batch 81: Training dataset Loss=0.860673, Batch Time=0.027; Early rounds
Epoch 0: at batch 82: Training dataset Loss=0.855311, Batch Time=0.030; Early rounds
Epoch 0: at batch 83: Training dataset Loss=0.852844, Batch Time=0.028; Early rounds
		Epoch 0:  Time = 126.642, Avg epoch time=126.642, Current epoch Time=126.642

Loss vector (slice for the first 20 images)
[[1.03848433]
 [1.        ]
 [1.        ]
 [0.53139299]
 [0.76025784]
 [0.71770489]
 [1.        ]
 [1.        ]
 [0.76025784]
 [0.7749514 ]
 [1.        ]
 [1.        ]
 [1.08692551]
 [1.28184128]
 [1.        ]
 [1.12430429]
 [1.17463613]
 [1.1881175 ]
 [0.70190465]
 [1.        ]]
Epoch 1: at batch 1: Training dataset Loss=0.851015, Batch Time=0.027
Epoch 1: at batch 1: Training dataset Loss=0.851015, Batch Time=0.027; Early rounds
Epoch 1: at batch 2: Training dataset Loss=0.844370, Batch Time=0.024; Early rounds
Epoch 1: at batch 3: Training dataset Loss=0.839963, Batch Time=0.026; Early rounds
Epoch 1: at batch 4: Training dataset Loss=0.836328, Batch Time=0.029; Early rounds
Epoch 1: at batch 5: Training dataset Loss=0.831994, Batch Time=0.034; Early rounds
Found in memory: ./dataset/Sony/short/00218_00_0.04s.ARW; images_in_memory= 844
Epoch 1: at batch 6: Training dataset Loss=0.828292, Batch Time=0.027; Early rounds
Epoch 1: at batch 7: Training dataset Loss=0.824767, Batch Time=0.029; Early rounds
Epoch 1: at batch 8: Training dataset Loss=0.821130, Batch Time=0.031; Early rounds
Epoch 1: at batch 9: Training dataset Loss=0.816752, Batch Time=0.028; Early rounds
Epoch 1: at batch 10: Training dataset Loss=0.814873, Batch Time=0.024; Early rounds
Epoch 1: at batch 11: Training dataset Loss=0.814951, Batch Time=0.030; Early rounds
^[[AEpoch 1: at batch 12: Training dataset Loss=0.819012, Batch Time=0.027; Early rounds
Epoch 1: at batch 13: Training dataset Loss=0.821907, Batch Time=0.029; Early rounds
Epoch 1: at batch 14: Training dataset Loss=0.818871, Batch Time=0.031; Early rounds
Epoch 1: at batch 15: Training dataset Loss=0.816492, Batch Time=0.029; Early rounds
Epoch 1: at batch 16: Training dataset Loss=0.817517, Batch Time=0.026; Early rounds
Epoch 1: at batch 17: Training dataset Loss=0.812795, Batch Time=0.025; Early rounds
Epoch 1: at batch 18: Training dataset Loss=0.822636, Batch Time=0.027; Early rounds
Epoch 1: at batch 19: Training dataset Loss=0.821469, Batch Time=0.029; Early rounds
Epoch 1: at batch 20: Training dataset Loss=0.820402, Batch Time=0.029; Early rounds
Epoch 1: at batch 21: Training dataset Loss=0.817036, Batch Time=0.029; Early rounds
Epoch 1: at batch 22: Training dataset Loss=0.818912, Batch Time=0.025; Early rounds
Epoch 1: at batch 23: Training dataset Loss=0.822162, Batch Time=0.027; Early rounds
loading ./dataset/Sony/short/00190_02_0.04s.ARW; images_in_memory= 932
Epoch 1: at batch 24: Training dataset Loss=0.819361, Batch Time=0.028; Early rounds
Epoch 1: at batch 25: Training dataset Loss=0.814997, Batch Time=0.024; Early rounds
Epoch 1: at batch 26: Training dataset Loss=0.823722, Batch Time=0.024; Early rounds
Found in memory: ./dataset/Sony/short/00137_00_0.1s.ARW; images_in_memory= 944
Epoch 1: at batch 27: Training dataset Loss=0.819994, Batch Time=0.027; Early rounds
Epoch 1: at batch 28: Training dataset Loss=0.818942, Batch Time=0.028; Early rounds
Found in memory: ./dataset/Sony/short/00151_09_0.1s.ARW; images_in_memory= 954
Epoch 1: at batch 29: Training dataset Loss=0.815543, Batch Time=0.028; Early rounds
Epoch 1: at batch 30: Training dataset Loss=0.810245, Batch Time=0.025; Early rounds
Epoch 1: at batch 31: Training dataset Loss=0.805634, Batch Time=0.027; Early rounds
Epoch 1: at batch 32: Training dataset Loss=0.799336, Batch Time=0.028; Early rounds
Epoch 1: at batch 33: Training dataset Loss=0.793382, Batch Time=0.030; Early rounds
Epoch 1: at batch 34: Training dataset Loss=0.791144, Batch Time=0.027; Early rounds
Epoch 1: at batch 35: Training dataset Loss=0.795510, Batch Time=0.027; Early rounds
Epoch 1: at batch 36: Training dataset Loss=0.794573, Batch Time=0.030; Early rounds
Epoch 1: at batch 37: Training dataset Loss=0.789655, Batch Time=0.027; Early rounds
Epoch 1: at batch 38: Training dataset Loss=0.789970, Batch Time=0.027; Early rounds
Epoch 1: at batch 39: Training dataset Loss=0.786164, Batch Time=0.030; Early rounds
Epoch 1: at batch 40: Training dataset Loss=0.789313, Batch Time=0.028; Early rounds
Epoch 1: at batch 41: Training dataset Loss=0.786207, Batch Time=0.027; Early rounds
Epoch 1: at batch 42: Training dataset Loss=0.781509, Batch Time=0.029; Early rounds
Epoch 1: at batch 43: Training dataset Loss=0.781668, Batch Time=0.026; Early rounds
Epoch 1: at batch 44: Training dataset Loss=0.778822, Batch Time=0.027; Early rounds
Epoch 1: at batch 45: Training dataset Loss=0.775763, Batch Time=0.029; Early rounds
Epoch 1: at batch 46: Training dataset Loss=0.774006, Batch Time=0.028; Early rounds
Epoch 1: at batch 47: Training dataset Loss=0.770595, Batch Time=0.027; Early rounds
Epoch 1: at batch 48: Training dataset Loss=0.770155, Batch Time=0.029; Early rounds
Epoch 1: at batch 49: Training dataset Loss=0.767555, Batch Time=0.032; Early rounds
Epoch 1: at batch 50: Training dataset Loss=0.766525, Batch Time=0.026; Early rounds
Epoch 1: at batch 51: Training dataset Loss=0.765213, Batch Time=0.029; Early rounds
Epoch 1: at batch 52: Training dataset Loss=0.765338, Batch Time=0.026; Early rounds
Epoch 1: at batch 53: Training dataset Loss=0.761980, Batch Time=0.028; Early rounds
Epoch 1: at batch 54: Training dataset Loss=0.759055, Batch Time=0.028; Early rounds
Epoch 1: at batch 55: Training dataset Loss=0.755099, Batch Time=0.027; Early rounds
Epoch 1: at batch 56: Training dataset Loss=0.755748, Batch Time=0.029; Early rounds
Epoch 1: at batch 57: Training dataset Loss=0.752180, Batch Time=0.031; Early rounds
Epoch 1: at batch 58: Training dataset Loss=0.745857, Batch Time=0.031; Early rounds
Epoch 1: at batch 59: Training dataset Loss=0.744075, Batch Time=0.029; Early rounds
Epoch 1: at batch 60: Training dataset Loss=0.740170, Batch Time=0.028; Early rounds
Epoch 1: at batch 61: Training dataset Loss=0.734565, Batch Time=0.030; Early rounds
Epoch 1: at batch 62: Training dataset Loss=0.733959, Batch Time=0.030; Early rounds
Epoch 1: at batch 63: Training dataset Loss=0.730242, Batch Time=0.027; Early rounds
Epoch 1: at batch 64: Training dataset Loss=0.727666, Batch Time=0.025; Early rounds
Found in memory: ./dataset/Sony/short/00218_00_0.04s.ARW; images_in_memory= 1087
Epoch 1: at batch 65: Training dataset Loss=0.723977, Batch Time=0.025; Early rounds
loading ./dataset/Sony/short/00223_04_0.04s.ARW; images_in_memory= 1089
Epoch 1: at batch 66: Training dataset Loss=0.724018, Batch Time=0.026; Early rounds
Epoch 1: at batch 67: Training dataset Loss=0.722777, Batch Time=0.026; Early rounds
Epoch 1: at batch 68: Training dataset Loss=0.720714, Batch Time=0.028; Early rounds
Epoch 1: at batch 69: Training dataset Loss=0.716660, Batch Time=0.027; Early rounds
Epoch 1: at batch 70: Training dataset Loss=0.715833, Batch Time=0.026; Early rounds
Epoch 1: at batch 71: Training dataset Loss=0.713976, Batch Time=0.026; Early rounds
Epoch 1: at batch 72: Training dataset Loss=0.712673, Batch Time=0.026; Early rounds
Epoch 1: at batch 73: Training dataset Loss=0.709412, Batch Time=0.025; Early rounds
Epoch 1: at batch 74: Training dataset Loss=0.706944, Batch Time=0.029; Early rounds
Epoch 1: at batch 75: Training dataset Loss=0.701931, Batch Time=0.030; Early rounds
Epoch 1: at batch 76: Training dataset Loss=0.698561, Batch Time=0.026; Early rounds
Epoch 1: at batch 77: Training dataset Loss=0.695607, Batch Time=0.024; Early rounds
Epoch 1: at batch 78: Training dataset Loss=0.694895, Batch Time=0.026; Early rounds
Epoch 1: at batch 79: Training dataset Loss=0.692236, Batch Time=0.027; Early rounds
Epoch 1: at batch 80: Training dataset Loss=0.684631, Batch Time=0.029; Early rounds
Found in memory: ./dataset/Sony/short/00223_07_0.04s.ARW; images_in_memory= 1128
Epoch 1: at batch 81: Training dataset Loss=0.681705, Batch Time=0.029; Early rounds
Epoch 1: at batch 82: Training dataset Loss=0.680914, Batch Time=0.029; Early rounds
Epoch 1: at batch 83: Training dataset Loss=0.681504, Batch Time=0.030; Early rounds
		Epoch 1:  Time = 177.154, Avg epoch time=50.271, Current epoch Time=88.577

Loss vector (slice for the first 20 images)
[[1.03848433]
 [0.62834841]
 [1.        ]
 [0.53139299]
 [0.6524868 ]
 [0.6751523 ]
 [1.        ]
 [0.49146974]
 [0.6524868 ]
 [0.66409647]
 [0.49146974]
 [0.36637932]
 [0.48838675]
 [1.28184128]
 [0.7068665 ]
 [1.12430429]
 [0.47785491]
 [1.1881175 ]
 [0.42641312]
 [1.        ]]
Epoch 2: at batch 1: Training dataset Loss=0.680631, Batch Time=0.028
Epoch 2: at batch 1: Training dataset Loss=0.680631, Batch Time=0.028; Early rounds
Epoch 2: at batch 2: Training dataset Loss=0.680340, Batch Time=0.026; Early rounds
Epoch 2: at batch 3: Training dataset Loss=0.678558, Batch Time=0.026; Early rounds
Epoch 2: at batch 4: Training dataset Loss=0.674877, Batch Time=0.030; Early rounds
Epoch 2: at batch 5: Training dataset Loss=0.671820, Batch Time=0.029; Early rounds
Epoch 2: at batch 6: Training dataset Loss=0.666229, Batch Time=0.030; Early rounds
Epoch 2: at batch 7: Training dataset Loss=0.666590, Batch Time=0.029; Early rounds
Epoch 2: at batch 8: Training dataset Loss=0.664959, Batch Time=0.027; Early rounds
loading ./dataset/Sony/short/00024_00_0.04s.ARW; images_in_memory= 1157
Epoch 2: at batch 9: Training dataset Loss=0.663476, Batch Time=0.029; Early rounds
Found in memory: ./dataset/Sony/short/00218_00_0.04s.ARW; images_in_memory= 1159
Epoch 2: at batch 10: Training dataset Loss=0.663460, Batch Time=0.024; Early rounds
Found in memory: ./dataset/Sony/short/00024_00_0.04s.ARW; images_in_memory= 1161
Epoch 2: at batch 11: Training dataset Loss=0.659663, Batch Time=0.025; Early rounds
Epoch 2: at batch 12: Training dataset Loss=0.658464, Batch Time=0.027; Early rounds
Epoch 2: at batch 13: Training dataset Loss=0.656635, Batch Time=0.028; Early rounds
Epoch 2: at batch 14: Training dataset Loss=0.653866, Batch Time=0.030; Early rounds
Epoch 2: at batch 15: Training dataset Loss=0.651132, Batch Time=0.029; Early rounds
Epoch 2: at batch 16: Training dataset Loss=0.647720, Batch Time=0.029; Early rounds
Epoch 2: at batch 17: Training dataset Loss=0.643747, Batch Time=0.024; Early rounds
Epoch 2: at batch 18: Training dataset Loss=0.639870, Batch Time=0.025; Early rounds
Epoch 2: at batch 19: Training dataset Loss=0.637918, Batch Time=0.027; Early rounds
Epoch 2: at batch 20: Training dataset Loss=0.635749, Batch Time=0.026; Early rounds
Epoch 2: at batch 21: Training dataset Loss=0.632176, Batch Time=0.027; Early rounds
Epoch 2: at batch 22: Training dataset Loss=0.631729, Batch Time=0.027; Early rounds
Epoch 2: at batch 23: Training dataset Loss=0.629417, Batch Time=0.028; Early rounds
Epoch 2: at batch 24: Training dataset Loss=0.625489, Batch Time=0.028; Early rounds
Epoch 2: at batch 25: Training dataset Loss=0.623083, Batch Time=0.025; Early rounds
Epoch 2: at batch 26: Training dataset Loss=0.619419, Batch Time=0.027; Early rounds
Epoch 2: at batch 27: Training dataset Loss=0.619631, Batch Time=0.026; Early rounds
Epoch 2: at batch 28: Training dataset Loss=0.614848, Batch Time=0.029; Early rounds
Epoch 2: at batch 29: Training dataset Loss=0.618637, Batch Time=0.026; Early rounds
Epoch 2: at batch 30: Training dataset Loss=0.621727, Batch Time=0.028; Early rounds
Epoch 2: at batch 31: Training dataset Loss=0.624486, Batch Time=0.029; Early rounds
Epoch 2: at batch 32: Training dataset Loss=0.622557, Batch Time=0.027; Early rounds
Epoch 2: at batch 33: Training dataset Loss=0.620450, Batch Time=0.029; Early rounds
Epoch 2: at batch 34: Training dataset Loss=0.619145, Batch Time=0.027; Early rounds
Epoch 2: at batch 35: Training dataset Loss=0.616866, Batch Time=0.027; Early rounds
Epoch 2: at batch 36: Training dataset Loss=0.617870, Batch Time=0.026; Early rounds
Found in memory: ./dataset/Sony/short/00223_07_0.04s.ARW; images_in_memory= 1200
Epoch 2: at batch 37: Training dataset Loss=0.617375, Batch Time=0.023; Early rounds
Epoch 2: at batch 38: Training dataset Loss=0.616465, Batch Time=0.029; Early rounds
Found in memory: ./dataset/Sony/short/00024_00_0.04s.ARW; images_in_memory= 1201
Epoch 2: at batch 39: Training dataset Loss=0.612933, Batch Time=0.031; Early rounds
Epoch 2: at batch 40: Training dataset Loss=0.609630, Batch Time=0.029; Early rounds
Epoch 2: at batch 41: Training dataset Loss=0.608313, Batch Time=0.032; Early rounds
Epoch 2: at batch 42: Training dataset Loss=0.606943, Batch Time=0.028; Early rounds
Epoch 2: at batch 43: Training dataset Loss=0.607198, Batch Time=0.028; Early rounds
Epoch 2: at batch 44: Training dataset Loss=0.606092, Batch Time=0.028; Early rounds
Found in memory: ./dataset/Sony/short/00223_04_0.04s.ARW; images_in_memory= 1213
Epoch 2: at batch 45: Training dataset Loss=0.615014, Batch Time=0.025; Early rounds
Epoch 2: at batch 46: Training dataset Loss=0.616109, Batch Time=0.025; Early rounds
Found in memory: ./dataset/Sony/short/00137_00_0.1s.ARW; images_in_memory= 1218
Epoch 2: at batch 47: Training dataset Loss=0.619064, Batch Time=0.027; Early rounds
Epoch 2: at batch 48: Training dataset Loss=0.618282, Batch Time=0.030; Early rounds
Epoch 2: at batch 49: Training dataset Loss=0.613201, Batch Time=0.029; Early rounds
Epoch 2: at batch 50: Training dataset Loss=0.612031, Batch Time=0.029; Early rounds
Epoch 2: at batch 51: Training dataset Loss=0.609771, Batch Time=0.027; Early rounds
Epoch 2: at batch 52: Training dataset Loss=0.609395, Batch Time=0.026; Early rounds
Epoch 2: at batch 53: Training dataset Loss=0.609167, Batch Time=0.023; Early rounds
Epoch 2: at batch 54: Training dataset Loss=0.610338, Batch Time=0.025; Early rounds
Epoch 2: at batch 55: Training dataset Loss=0.608520, Batch Time=0.027; Early rounds
Epoch 2: at batch 56: Training dataset Loss=0.605967, Batch Time=0.028; Early rounds
Epoch 2: at batch 57: Training dataset Loss=0.604715, Batch Time=0.028; Early rounds
Epoch 2: at batch 58: Training dataset Loss=0.604288, Batch Time=0.025; Early rounds
Epoch 2: at batch 59: Training dataset Loss=0.601253, Batch Time=0.026; Early rounds
Epoch 2: at batch 60: Training dataset Loss=0.598800, Batch Time=0.026; Early rounds
Epoch 2: at batch 61: Training dataset Loss=0.598069, Batch Time=0.026; Early rounds
Epoch 2: at batch 62: Training dataset Loss=0.597530, Batch Time=0.027; Early rounds
Epoch 2: at batch 63: Training dataset Loss=0.595053, Batch Time=0.029; Early rounds
Epoch 2: at batch 64: Training dataset Loss=0.591344, Batch Time=0.029; Early rounds
Epoch 2: at batch 65: Training dataset Loss=0.589255, Batch Time=0.026; Early rounds
Epoch 2: at batch 66: Training dataset Loss=0.585584, Batch Time=0.024; Early rounds
Epoch 2: at batch 67: Training dataset Loss=0.585683, Batch Time=0.024; Early rounds
Epoch 2: at batch 68: Training dataset Loss=0.583892, Batch Time=0.028; Early rounds
Epoch 2: at batch 69: Training dataset Loss=0.581283, Batch Time=0.027; Early rounds
Epoch 2: at batch 70: Training dataset Loss=0.580027, Batch Time=0.028; Early rounds
Epoch 2: at batch 71: Training dataset Loss=0.576119, Batch Time=0.031; Early rounds
Epoch 2: at batch 72: Training dataset Loss=0.572703, Batch Time=0.026; Early rounds
Epoch 2: at batch 73: Training dataset Loss=0.567722, Batch Time=0.028; Early rounds
Epoch 2: at batch 74: Training dataset Loss=0.565859, Batch Time=0.032; Early rounds
Epoch 2: at batch 75: Training dataset Loss=0.567091, Batch Time=0.030; Early rounds
Epoch 2: at batch 76: Training dataset Loss=0.562791, Batch Time=0.030; Early rounds
Epoch 2: at batch 77: Training dataset Loss=0.559923, Batch Time=0.028; Early rounds
Epoch 2: at batch 78: Training dataset Loss=0.557106, Batch Time=0.027; Early rounds
Epoch 2: at batch 79: Training dataset Loss=0.554230, Batch Time=0.025; Early rounds
Epoch 2: at batch 80: Training dataset Loss=0.552554, Batch Time=0.025; Early rounds
Epoch 2: at batch 81: Training dataset Loss=0.555045, Batch Time=0.029; Early rounds
Epoch 2: at batch 82: Training dataset Loss=0.554162, Batch Time=0.031; Early rounds
Epoch 2: at batch 83: Training dataset Loss=0.557282, Batch Time=0.028; Early rounds
		Epoch 2:  Time = 198.539, Avg epoch time=21.245, Current epoch Time=66.180

Loss vector (slice for the first 20 images)
[[0.31008843]
 [0.62834841]
 [1.        ]
 [0.28215516]
 [0.49800813]
 [0.42298123]
 [0.80093503]
 [0.49146974]
 [0.6524868 ]
 [0.37614325]
 [0.49146974]
 [0.36637932]
 [0.78142583]
 [0.49800813]
 [0.41605449]
 [0.63556153]
 [0.85945266]
 [0.494405  ]
 [0.41120058]
 [1.        ]]
Epoch 3: at batch 1: Training dataset Loss=0.554672, Batch Time=0.027
Found in memory: ./dataset/Sony/short/00024_00_0.04s.ARW; images_in_memory= 1283
Found in memory: ./dataset/Sony/short/00223_07_0.04s.ARW; images_in_memory= 1294
Found in memory: ./dataset/Sony/short/00218_00_0.04s.ARW; images_in_memory= 1301
Found in memory: ./dataset/Sony/short/00151_09_0.1s.ARW; images_in_memory= 1302
		Epoch 3:  Time = 207.337, Avg epoch time=8.655, Current epoch Time=51.834

Loss vector (slice for the first 20 images)
[[0.31008843]
 [0.62834841]
 [0.50552195]
 [0.53761137]
 [0.5473026 ]
 [0.27995154]
 [0.80093503]
 [0.53761137]
 [0.47351721]
 [0.44906026]
 [0.60929114]
 [0.45952633]
 [0.42742425]
 [0.49800813]
 [0.41605449]
 [0.60218525]
 [0.33020148]
 [0.36700943]
 [0.41120058]
 [0.60929114]]
Found in memory: ./dataset/Sony/short/00024_00_0.04s.ARW; images_in_memory= 1306
Epoch 4: at batch 1: Training dataset Loss=0.482981, Batch Time=0.025
Found in memory: ./dataset/Sony/short/00137_00_0.1s.ARW; images_in_memory= 1310
Found in memory: ./dataset/Sony/short/00137_00_0.1s.ARW; images_in_memory= 1315
		Epoch 4:  Time = 211.750, Avg epoch time=4.272, Current epoch Time=42.350

Loss vector (slice for the first 20 images)
[[0.43361008]
 [0.62834841]
 [0.50552195]
 [0.42125165]
 [0.31938559]
 [0.27995154]
 [0.41338062]
 [0.53761137]
 [0.47351721]
 [0.34523237]
 [0.25158533]
 [0.45952633]
 [0.42742425]
 [0.36363086]
 [0.25158533]
 [0.42125165]
 [0.33020148]
 [0.6178236 ]
 [0.52264059]
 [0.33602139]]
Epoch 5: at batch 1: Training dataset Loss=0.419494, Batch Time=0.029
Found in memory: ./dataset/Sony/short/00024_00_0.04s.ARW; images_in_memory= 1320
Found in memory: ./dataset/Sony/short/00223_07_0.04s.ARW; images_in_memory= 1324
Found in memory: ./dataset/Sony/short/00223_07_0.04s.ARW; images_in_memory= 1324
		Epoch 5:  Time = 215.043, Avg epoch time=3.152, Current epoch Time=35.841

Epoch 6: at batch 1: Training dataset Loss=0.390385, Batch Time=0.025
Found in memory: ./dataset/Sony/short/00137_00_0.1s.ARW; images_in_memory= 1326
Found in memory: ./dataset/Sony/short/00137_00_0.1s.ARW; images_in_memory= 1327
Found in memory: ./dataset/Sony/short/00218_00_0.04s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00190_02_0.04s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00223_04_0.04s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00223_04_0.04s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00223_07_0.04s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00190_02_0.04s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00223_07_0.04s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00190_02_0.04s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00190_02_0.04s.ARW; images_in_memory= 1329
		Epoch 6:  Time = 217.656, Avg epoch time=2.470, Current epoch Time=31.094

Epoch 7: at batch 1: Training dataset Loss=0.335758, Batch Time=0.023
Found in memory: ./dataset/Sony/short/00024_00_0.04s.ARW; images_in_memory= 1330
Found in memory: ./dataset/Sony/short/00137_00_0.1s.ARW; images_in_memory= 1330
Found in memory: ./dataset/Sony/short/00151_09_0.1s.ARW; images_in_memory= 1330
Found in memory: ./dataset/Sony/short/00223_07_0.04s.ARW; images_in_memory= 1330
Found in memory: ./dataset/Sony/short/00218_00_0.04s.ARW; images_in_memory= 1330
		Epoch 7:  Time = 220.039, Avg epoch time=2.227, Current epoch Time=27.505

Epoch 8: at batch 1: Training dataset Loss=0.317067, Batch Time=0.024
Found in memory: ./dataset/Sony/short/00190_02_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00223_07_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00218_00_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00137_00_0.1s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00218_00_0.04s.ARW; images_in_memory= 1331
		Epoch 8:  Time = 222.401, Avg epoch time=2.217, Current epoch Time=24.711

Epoch 9: at batch 1: Training dataset Loss=0.296481, Batch Time=0.027
Found in memory: ./dataset/Sony/short/00190_02_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00151_09_0.1s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00024_00_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00223_04_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00024_00_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00024_00_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00223_04_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00190_02_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00223_04_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00218_00_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00190_02_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00218_00_0.04s.ARW; images_in_memory= 1331
		Epoch 9:  Time = 224.579, Avg epoch time=2.037, Current epoch Time=22.458

Epoch 11: at batch 1: Training dataset Loss=0.257172, Batch Time=0.027
		Epoch 11:  Time = 229.155, Avg epoch time=2.080, Current epoch Time=19.096

Loss vector (slice for the first 20 images)
[[0.20845   ]
 [0.28790659]
 [0.23855621]
 [0.19981179]
 [0.18666393]
 [0.21256945]
 [0.35083222]
 [0.2433753 ]
 [0.24930069]
 [0.23156163]
 [0.52088708]
 [0.24078178]
 [0.23228301]
 [0.37100911]
 [0.23156163]
 [0.16339013]
 [0.22499859]
 [0.40300953]
 [0.11174953]
 [0.28642771]]
Epoch 13: at batch 1: Training dataset Loss=0.223881, Batch Time=0.027
Epoch 15: at batch 1: Training dataset Loss=0.233378, Batch Time=0.027
Epoch 17: at batch 1: Training dataset Loss=0.217183, Batch Time=0.027
Epoch 19: at batch 1: Training dataset Loss=0.186591, Batch Time=0.029
Epoch 21: at batch 1: Training dataset Loss=0.182558, Batch Time=0.027
		Epoch 21:  Time = 251.180, Avg epoch time=2.058, Current epoch Time=11.417

Loss vector (slice for the first 20 images)
[[0.46986622]
 [0.01323574]
 [0.27376431]
 [0.3401677 ]
 [0.18031603]
 [0.01560606]
 [0.16353294]
 [0.20344818]
 [0.28725103]
 [0.20344818]
 [0.24340761]
 [0.12022226]
 [0.2242111 ]
 [0.20085497]
 [0.25776225]
 [0.22770631]
 [0.20314398]
 [0.17894413]
 [0.27227134]
 [0.25394851]]
Epoch 23: at batch 1: Training dataset Loss=0.187618, Batch Time=0.027
Epoch 25: at batch 1: Training dataset Loss=0.164688, Batch Time=0.028
Epoch 27: at batch 1: Training dataset Loss=0.159834, Batch Time=0.027
Epoch 29: at batch 1: Training dataset Loss=0.162684, Batch Time=0.028
Epoch 31: at batch 1: Training dataset Loss=0.157876, Batch Time=0.022
		Epoch 31:  Time = 273.371, Avg epoch time=2.075, Current epoch Time=8.543

Loss vector (slice for the first 20 images)
[[0.19926766]
 [0.1945318 ]
 [0.06649283]
 [0.27320319]
 [0.02335999]
 [0.06257174]
 [0.22195527]
 [0.19267048]
 [0.05893027]
 [0.1656124 ]
 [0.16780724]
 [0.20918018]
 [0.05392919]
 [0.07172164]
 [0.08706731]
 [0.22195527]
 [0.21188243]
 [0.09026789]
 [0.09064597]
 [0.05712052]]
Epoch 33: at batch 1: Training dataset Loss=0.142148, Batch Time=0.023
Epoch 35: at batch 1: Training dataset Loss=0.161873, Batch Time=0.027
Epoch 37: at batch 1: Training dataset Loss=0.144548, Batch Time=0.027
Epoch 39: at batch 1: Training dataset Loss=0.146912, Batch Time=0.023
Epoch 41: at batch 1: Training dataset Loss=0.146563, Batch Time=0.022
		Epoch 41:  Time = 295.319, Avg epoch time=2.028, Current epoch Time=7.031

Loss vector (slice for the first 20 images)
[[0.09555158]
 [0.1709601 ]
 [0.07911246]
 [0.11219281]
 [0.14244223]
 [0.1540377 ]
 [0.2165938 ]
 [0.12013848]
 [0.22884962]
 [0.27327383]
 [0.20200381]
 [0.09958139]
 [0.06204719]
 [0.1627802 ]
 [0.07969142]
 [0.14829518]
 [0.21825372]
 [0.17374775]
 [0.11822519]
 [0.2027024 ]]
Epoch 43: at batch 1: Training dataset Loss=0.139200, Batch Time=0.024
Epoch 45: at batch 1: Training dataset Loss=0.132364, Batch Time=0.025
Epoch 47: at batch 1: Training dataset Loss=0.135895, Batch Time=0.025
Epoch 49: at batch 1: Training dataset Loss=0.132739, Batch Time=0.027
Epoch 51: at batch 1: Training dataset Loss=0.137471, Batch Time=0.027
		Epoch 51:  Time = 317.464, Avg epoch time=2.048, Current epoch Time=6.105

Loss vector (slice for the first 20 images)
[[0.10234119]
 [0.17036882]
 [0.27140906]
 [0.22575587]
 [0.2107214 ]
 [0.13669665]
 [0.18070342]
 [0.07193148]
 [0.17747059]
 [0.24557312]
 [0.165751  ]
 [0.06542298]
 [0.11825544]
 [0.17832576]
 [0.13078922]
 [0.07134628]
 [0.16621724]
 [0.09384181]
 [0.17594533]
 [0.01656131]]
Epoch 53: at batch 1: Training dataset Loss=0.143265, Batch Time=0.024
Epoch 55: at batch 1: Training dataset Loss=0.136753, Batch Time=0.026
Epoch 57: at batch 1: Training dataset Loss=0.138943, Batch Time=0.022
Epoch 59: at batch 1: Training dataset Loss=0.133996, Batch Time=0.026
Epoch 61: at batch 1: Training dataset Loss=0.139576, Batch Time=0.026
		Epoch 61:  Time = 339.591, Avg epoch time=2.031, Current epoch Time=5.477

Loss vector (slice for the first 20 images)
[[0.09505565]
 [0.06809069]
 [0.05958972]
 [0.31408873]
 [0.17328046]
 [0.14255804]
 [0.18618104]
 [0.02362116]
 [0.29897869]
 [0.20566818]
 [0.26131895]
 [0.09618033]
 [0.01793155]
 [0.22246912]
 [0.15333204]
 [0.10702625]
 [0.2234288 ]
 [0.22605452]
 [0.01793155]
 [0.1325644 ]]
Epoch 63: at batch 1: Training dataset Loss=0.133979, Batch Time=0.022
Epoch 65: at batch 1: Training dataset Loss=0.132969, Batch Time=0.023
Epoch 67: at batch 1: Training dataset Loss=0.137380, Batch Time=0.024
Epoch 69: at batch 1: Training dataset Loss=0.128907, Batch Time=0.024
Epoch 71: at batch 1: Training dataset Loss=0.127064, Batch Time=0.022
		Epoch 71:  Time = 361.783, Avg epoch time=2.037, Current epoch Time=5.025

Loss vector (slice for the first 20 images)
[[0.23438588]
 [0.23876221]
 [0.18990232]
 [0.15141912]
 [0.06748124]
 [0.0919392 ]
 [0.24981125]
 [0.16275023]
 [0.16644993]
 [0.21799457]
 [0.17835352]
 [0.10020714]
 [0.04365654]
 [0.07966913]
 [0.20303999]
 [0.06212806]
 [0.12603781]
 [0.1641943 ]
 [0.08634694]
 [0.16275023]]
Epoch 73: at batch 1: Training dataset Loss=0.140659, Batch Time=0.029
Epoch 75: at batch 1: Training dataset Loss=0.134600, Batch Time=0.023
Epoch 77: at batch 1: Training dataset Loss=0.125661, Batch Time=0.026
Epoch 79: at batch 1: Training dataset Loss=0.132031, Batch Time=0.023
Epoch 81: at batch 1: Training dataset Loss=0.133964, Batch Time=0.025
		Epoch 81:  Time = 383.924, Avg epoch time=2.068, Current epoch Time=4.682

Loss vector (slice for the first 20 images)
[[0.17944908]
 [0.22883071]
 [0.04109839]
 [0.07773529]
 [0.21495755]
 [0.13775793]
 [0.16443868]
 [0.17227943]
 [0.06738562]
 [0.17069384]
 [0.2160072 ]
 [0.16327542]
 [0.08361372]
 [0.07379831]
 [0.14725235]
 [0.2141096 ]
 [0.08406872]
 [0.15510951]
 [0.11703792]
 [0.09750104]]
Epoch 83: at batch 1: Training dataset Loss=0.144126, Batch Time=0.023
Epoch 85: at batch 1: Training dataset Loss=0.138708, Batch Time=0.027
Epoch 87: at batch 1: Training dataset Loss=0.138894, Batch Time=0.024
Epoch 89: at batch 1: Training dataset Loss=0.135567, Batch Time=0.026
Epoch 91: at batch 1: Training dataset Loss=0.129737, Batch Time=0.027
		Epoch 91:  Time = 406.181, Avg epoch time=2.093, Current epoch Time=4.415

Loss vector (slice for the first 20 images)
[[0.22334713]
 [0.10787593]
 [0.03135914]
 [0.23891446]
 [0.02491924]
 [0.20531178]
 [0.17108025]
 [0.17588089]
 [0.02114676]
 [0.10562344]
 [0.1844773 ]
 [0.15415968]
 [0.17568615]
 [0.17760512]
 [0.19760269]
 [0.0564938 ]
 [0.09292707]
 [0.10476857]
 [0.11887274]
 [0.28016359]]
Epoch 93: at batch 1: Training dataset Loss=0.132782, Batch Time=0.023
Epoch 95: at batch 1: Training dataset Loss=0.130840, Batch Time=0.025
Epoch 97: at batch 1: Training dataset Loss=0.137189, Batch Time=0.030
Epoch 99: at batch 1: Training dataset Loss=0.134355, Batch Time=0.025
Epoch 101: at batch 1: Training dataset Loss=0.133613, Batch Time=0.022
		Epoch 101:  Time = 428.381, Avg epoch time=2.057, Current epoch Time=4.200

Loss vector (slice for the first 20 images)
[[0.11452365]
 [0.09598619]
 [0.06225695]
 [0.32023546]
 [0.2899003 ]
 [0.12059318]
 [0.17247769]
 [0.19016945]
 [0.11740226]
 [0.25338587]
 [0.26152641]
 [0.07846834]
 [0.19154805]
 [0.07846834]
 [0.05768058]
 [0.11905068]
 [0.12725039]
 [0.07466079]
 [0.15972763]
 [0.09204823]]
Epoch 103: at batch 1: Training dataset Loss=0.131571, Batch Time=0.027
Epoch 105: at batch 1: Training dataset Loss=0.141630, Batch Time=0.027
Epoch 107: at batch 1: Training dataset Loss=0.124357, Batch Time=0.026
Epoch 109: at batch 1: Training dataset Loss=0.126533, Batch Time=0.022
Epoch 111: at batch 1: Training dataset Loss=0.121788, Batch Time=0.024
		Epoch 111:  Time = 450.411, Avg epoch time=2.032, Current epoch Time=4.022

Loss vector (slice for the first 20 images)
[[0.03346995]
 [0.1514844 ]
 [0.15841119]
 [0.19280709]
 [0.18990764]
 [0.15956552]
 [0.2723456 ]
 [0.0506244 ]
 [0.2067748 ]
 [0.08527842]
 [0.13794765]
 [0.05817052]
 [0.07778683]
 [0.14228873]
 [0.16107088]
 [0.19124532]
 [0.17321764]
 [0.14371976]
 [0.0729673 ]
 [0.26420832]]
Epoch 113: at batch 1: Training dataset Loss=0.124741, Batch Time=0.026
Epoch 115: at batch 1: Training dataset Loss=0.123991, Batch Time=0.023
Epoch 117: at batch 1: Training dataset Loss=0.128272, Batch Time=0.027
Epoch 119: at batch 1: Training dataset Loss=0.131426, Batch Time=0.025
Epoch 121: at batch 1: Training dataset Loss=0.128978, Batch Time=0.025
		Epoch 121:  Time = 472.455, Avg epoch time=2.083, Current epoch Time=3.873

Loss vector (slice for the first 20 images)
[[0.07751206]
 [0.2267032 ]
 [0.0666291 ]
 [0.11334999]
 [0.16722265]
 [0.12964487]
 [0.06902742]
 [0.27244049]
 [0.11334999]
 [0.1688859 ]
 [0.14723715]
 [0.09580232]
 [0.09808568]
 [0.10832508]
 [0.06069677]
 [0.21460921]
 [0.14046192]
 [0.10832508]
 [0.19559899]
 [0.15777296]]
Epoch 123: at batch 1: Training dataset Loss=0.129998, Batch Time=0.028
Epoch 125: at batch 1: Training dataset Loss=0.127768, Batch Time=0.026
Epoch 127: at batch 1: Training dataset Loss=0.124197, Batch Time=0.024
Epoch 129: at batch 1: Training dataset Loss=0.134697, Batch Time=0.026
Epoch 131: at batch 1: Training dataset Loss=0.125151, Batch Time=0.022
		Epoch 131:  Time = 494.457, Avg epoch time=2.082, Current epoch Time=3.746

Loss vector (slice for the first 20 images)
[[0.22484134]
 [0.16564062]
 [0.06501229]
 [0.18622714]
 [0.17042123]
 [0.03336205]
 [0.12463246]
 [0.06541096]
 [0.06501229]
 [0.21658398]
 [0.16586694]
 [0.10791181]
 [0.25645393]
 [0.06627429]
 [0.09914276]
 [0.05463713]
 [0.25645393]
 [0.14329079]
 [0.07625561]
 [0.08169785]]
Epoch 133: at batch 1: Training dataset Loss=0.122945, Batch Time=0.025
Epoch 135: at batch 1: Training dataset Loss=0.132301, Batch Time=0.030
Epoch 137: at batch 1: Training dataset Loss=0.126730, Batch Time=0.027
Epoch 139: at batch 1: Training dataset Loss=0.126066, Batch Time=0.026
Epoch 141: at batch 1: Training dataset Loss=0.126907, Batch Time=0.022
		Epoch 141:  Time = 516.632, Avg epoch time=2.095, Current epoch Time=3.638

Loss vector (slice for the first 20 images)
[[0.12709439]
 [0.02003444]
 [0.16578186]
 [0.12709439]
 [0.05583734]
 [0.18662325]
 [0.22305465]
 [0.09050156]
 [0.01248884]
 [0.13088201]
 [0.23934536]
 [0.03862649]
 [0.06982803]
 [0.17157036]
 [0.23934536]
 [0.02028132]
 [0.11298166]
 [0.09590756]
 [0.10733926]
 [0.11488464]]
Epoch 143: at batch 1: Training dataset Loss=0.116097, Batch Time=0.028
Epoch 145: at batch 1: Training dataset Loss=0.121022, Batch Time=0.027
Epoch 147: at batch 1: Training dataset Loss=0.117811, Batch Time=0.022
Epoch 149: at batch 1: Training dataset Loss=0.122823, Batch Time=0.027
Epoch 151: at batch 1: Training dataset Loss=0.124385, Batch Time=0.027
		Epoch 151:  Time = 538.649, Avg epoch time=2.058, Current epoch Time=3.544

Loss vector (slice for the first 20 images)
[[0.09925479]
 [0.10790908]
 [0.09944694]
 [0.08184266]
 [0.19766113]
 [0.08184266]
 [0.1255403 ]
 [0.10220022]
 [0.09786744]
 [0.11349747]
 [0.22409271]
 [0.04230492]
 [0.13614829]
 [0.07900439]
 [0.26108274]
 [0.11915547]
 [0.15559083]
 [0.10095397]
 [0.06461626]
 [0.07557511]]
Epoch 153: at batch 1: Training dataset Loss=0.122835, Batch Time=0.023
Epoch 155: at batch 1: Training dataset Loss=0.126974, Batch Time=0.029
Epoch 157: at batch 1: Training dataset Loss=0.133580, Batch Time=0.029
Epoch 159: at batch 1: Training dataset Loss=0.124712, Batch Time=0.028
Epoch 161: at batch 1: Training dataset Loss=0.129331, Batch Time=0.024
		Epoch 161:  Time = 560.680, Avg epoch time=2.083, Current epoch Time=3.461

Loss vector (slice for the first 20 images)
[[0.08639021]
 [0.20272559]
 [0.14793757]
 [0.10511424]
 [0.06552333]
 [0.16036716]
 [0.19176878]
 [0.09097613]
 [0.09772913]
 [0.16763568]
 [0.28894481]
 [0.07512326]
 [0.0221497 ]
 [0.17849949]
 [0.22342755]
 [0.02170713]
 [0.13168679]
 [0.20463955]
 [0.03236675]
 [0.10496776]]
Epoch 163: at batch 1: Training dataset Loss=0.132509, Batch Time=0.026
Epoch 165: at batch 1: Training dataset Loss=0.115161, Batch Time=0.022
Epoch 167: at batch 1: Training dataset Loss=0.126152, Batch Time=0.028
Epoch 169: at batch 1: Training dataset Loss=0.129504, Batch Time=0.027
Epoch 171: at batch 1: Training dataset Loss=0.125467, Batch Time=0.028
		Epoch 171:  Time = 582.863, Avg epoch time=2.085, Current epoch Time=3.389

Loss vector (slice for the first 20 images)
[[0.12489646]
 [0.19479209]
 [0.30224359]
 [0.18978985]
 [0.15452218]
 [0.04930458]
 [0.10228486]
 [0.09337927]
 [0.13794348]
 [0.14992711]
 [0.04274693]
 [0.10242976]
 [0.21401393]
 [0.15753035]
 [0.1409692 ]
 [0.17119488]
 [0.25965026]
 [0.1637744 ]
 [0.0980277 ]
 [0.25383902]]
Epoch 173: at batch 1: Training dataset Loss=0.130819, Batch Time=0.027
Epoch 175: at batch 1: Training dataset Loss=0.121757, Batch Time=0.029
Epoch 177: at batch 1: Training dataset Loss=0.118662, Batch Time=0.025
Epoch 179: at batch 1: Training dataset Loss=0.128799, Batch Time=0.030
Epoch 181: at batch 1: Training dataset Loss=0.131167, Batch Time=0.025
		Epoch 181:  Time = 604.955, Avg epoch time=2.021, Current epoch Time=3.324

Loss vector (slice for the first 20 images)
[[0.03965553]
 [0.25417942]
 [0.09625093]
 [0.11014815]
 [0.15947852]
 [0.07881003]
 [0.20374003]
 [0.20374003]
 [0.16862419]
 [0.05464423]
 [0.13321322]
 [0.21915363]
 [0.10016862]
 [0.16390239]
 [0.09048931]
 [0.1834538 ]
 [0.02016934]
 [0.13468267]
 [0.10162185]
 [0.08601081]]
Epoch 183: at batch 1: Training dataset Loss=0.138948, Batch Time=0.025
Epoch 185: at batch 1: Training dataset Loss=0.122274, Batch Time=0.026
Epoch 187: at batch 1: Training dataset Loss=0.129032, Batch Time=0.023
Epoch 189: at batch 1: Training dataset Loss=0.131274, Batch Time=0.026
Epoch 191: at batch 1: Training dataset Loss=0.130750, Batch Time=0.029
		Epoch 191:  Time = 626.989, Avg epoch time=2.071, Current epoch Time=3.266

Loss vector (slice for the first 20 images)
[[0.02483516]
 [0.09653348]
 [0.11532714]
 [0.32027566]
 [0.19059452]
 [0.05702176]
 [0.22352146]
 [0.23824605]
 [0.03012665]
 [0.12845525]
 [0.10093172]
 [0.07462575]
 [0.12845525]
 [0.14269552]
 [0.13321251]
 [0.10014393]
 [0.07492644]
 [0.0837062 ]
 [0.11532714]
 [0.0837062 ]]
Epoch 193: at batch 1: Training dataset Loss=0.127933, Batch Time=0.024
Epoch 195: at batch 1: Training dataset Loss=0.128027, Batch Time=0.027
Epoch 197: at batch 1: Training dataset Loss=0.128782, Batch Time=0.030
Epoch 199: at batch 1: Training dataset Loss=0.126438, Batch Time=0.024
Epoch 201: at batch 1: Training dataset Loss=0.135341, Batch Time=0.028
		Epoch 201:  Time = 649.063, Avg epoch time=2.077, Current epoch Time=3.213

Loss vector (slice for the first 20 images)
[[0.13815661]
 [0.01459582]
 [0.12900046]
 [0.23580939]
 [0.17063537]
 [0.14576888]
 [0.06731832]
 [0.06330011]
 [0.01323901]
 [0.09328149]
 [0.21646473]
 [0.04133145]
 [0.08296878]
 [0.18350044]
 [0.19604519]
 [0.1595673 ]
 [0.0110885 ]
 [0.14309883]
 [0.07107157]
 [0.12352962]]
Epoch 203: at batch 1: Training dataset Loss=0.122930, Batch Time=0.030
Epoch 205: at batch 1: Training dataset Loss=0.115055, Batch Time=0.026
Epoch 207: at batch 1: Training dataset Loss=0.120655, Batch Time=0.029
Epoch 209: at batch 1: Training dataset Loss=0.128442, Batch Time=0.024
Epoch 211: at batch 1: Training dataset Loss=0.130445, Batch Time=0.030
		Epoch 211:  Time = 671.138, Avg epoch time=2.079, Current epoch Time=3.166

Loss vector (slice for the first 20 images)
[[0.03466921]
 [0.11966008]
 [0.17651829]
 [0.15556934]
 [0.09477757]
 [0.14842245]
 [0.12129374]
 [0.08438823]
 [0.17651829]
 [0.11253678]
 [0.07799151]
 [0.10341723]
 [0.07733035]
 [0.03586094]
 [0.03466921]
 [0.03586094]
 [0.17466146]
 [0.14375749]
 [0.0696093 ]
 [0.15674818]]
Epoch 213: at batch 1: Training dataset Loss=0.128848, Batch Time=0.028
Epoch 215: at batch 1: Training dataset Loss=0.119415, Batch Time=0.028
Epoch 217: at batch 1: Training dataset Loss=0.118092, Batch Time=0.029
Epoch 219: at batch 1: Training dataset Loss=0.111595, Batch Time=0.025
Epoch 221: at batch 1: Training dataset Loss=0.128152, Batch Time=0.027
		Epoch 221:  Time = 693.353, Avg epoch time=2.042, Current epoch Time=3.123

Loss vector (slice for the first 20 images)
[[0.13760132]
 [0.14033774]
 [0.02203182]
 [0.12821458]
 [0.08915289]
 [0.13870513]
 [0.19145077]
 [0.12821458]
 [0.20866956]
 [0.11585862]
 [0.13371108]
 [0.16939083]
 [0.04306216]
 [0.22683355]
 [0.19972488]
 [0.10481165]
 [0.07204856]
 [0.24024467]
 [0.07393263]
 [0.11929444]]
Epoch 223: at batch 1: Training dataset Loss=0.130016, Batch Time=0.025
Epoch 225: at batch 1: Training dataset Loss=0.124399, Batch Time=0.024
Epoch 227: at batch 1: Training dataset Loss=0.121836, Batch Time=0.025
Epoch 229: at batch 1: Training dataset Loss=0.119665, Batch Time=0.028
Epoch 231: at batch 1: Training dataset Loss=0.115250, Batch Time=0.026
		Epoch 231:  Time = 715.429, Avg epoch time=2.020, Current epoch Time=3.084

Loss vector (slice for the first 20 images)
[[0.10502727]
 [0.19043326]
 [0.20392025]
 [0.15220721]
 [0.18706405]
 [0.24114874]
 [0.26523155]
 [0.09433412]
 [0.10858192]
 [0.20392025]
 [0.17600413]
 [0.1658738 ]
 [0.01566098]
 [0.1123438 ]
 [0.02218855]
 [0.07372083]
 [0.10992464]
 [0.20128199]
 [0.06862129]
 [0.18706405]]
Epoch 233: at batch 1: Training dataset Loss=0.120701, Batch Time=0.027
Epoch 235: at batch 1: Training dataset Loss=0.125256, Batch Time=0.026
Epoch 237: at batch 1: Training dataset Loss=0.130284, Batch Time=0.027
Epoch 239: at batch 1: Training dataset Loss=0.130154, Batch Time=0.027
Epoch 241: at batch 1: Training dataset Loss=0.126788, Batch Time=0.025
		Epoch 241:  Time = 737.579, Avg epoch time=2.073, Current epoch Time=3.048

Loss vector (slice for the first 20 images)
[[0.03387547]
 [0.05153039]
 [0.11728826]
 [0.28396079]
 [0.11101628]
 [0.07220584]
 [0.14379054]
 [0.06419747]
 [0.21644196]
 [0.12540823]
 [0.26535657]
 [0.11422724]
 [0.02676805]
 [0.12484966]
 [0.07491188]
 [0.05470895]
 [0.15103057]
 [0.13523766]
 [0.17012575]
 [0.26950175]]
Epoch 243: at batch 1: Training dataset Loss=0.128616, Batch Time=0.027
Epoch 245: at batch 1: Training dataset Loss=0.121575, Batch Time=0.026
Epoch 247: at batch 1: Training dataset Loss=0.116584, Batch Time=0.025
Epoch 249: at batch 1: Training dataset Loss=0.119941, Batch Time=0.027
^CTraceback (most recent call last):
  File "CNN5_FC3_exposure_101GB.py", line 342, in <module>
    saver.save(sess, checkpoint_dir + 'model.ckpt')
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/training/saver.py", line 1433, in save
    {self.saver_def.filename_tensor_name: checkpoint_file})
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1292, in _do_call
    return fn(*args)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1367, in _call_tf_sessionrun
    run_metadata)
KeyboardInterrupt
(sid2) [ir967@gr046 Learning-to-See-in-the-Dark]$ vi CNN5_FC3_exposure_101GB.py
(sid2) [ir967@gr046 Learning-to-See-in-the-Dark]$ python CNN5_FC3_exposure_101GB.py




Current date and time : 
2020-12-13 17:09:05
./dataset/Sony/short/00133_07_0.1s.ARW 133 0.1
./dataset/Sony/short/00015_09_0.1s.ARW 15 0.1
./dataset/Sony/short/00190_08_0.04s.ARW 190 0.04
./dataset/Sony/short/00204_01_0.033s.ARW 204 0.033
./dataset/Sony/short/00047_03_0.1s.ARW 47 0.1
./dataset/Sony/short/00049_07_0.1s.ARW 49 0.1
./dataset/Sony/short/00231_00_0.033s.ARW 231 0.033
Found 1865 images to train with

Training on files with indices: 330 to 1662
Training on 1332 images only

2020-12-13 17:09:05.342303: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-13 17:09:05.513750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-13 17:09:05.513783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-13 17:09:05.803895: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-13 17:09:05.803934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-13 17:09:05.803950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-13 17:09:05.804049: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)

Loaded ./gt_Sony_CNN5_FC3_exposure_101GB/model.ckpt

last epoch of previous run: 249


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 83 ,ps 128 ,result_dir= ./gt_Sony_CNN5_FC3_exposure_101GB/ ,len(train_fns)= 1332

Cleared all images in memory.

loading ./dataset/Sony/short/00113_05_0.1s.ARW; images_in_memory= 4
loading ./dataset/Sony/short/00225_01_0.1s.ARW; images_in_memory= 9
Traceback (most recent call last):
  File "CNN5_FC3_exposure_101GB.py", line 294, in <module>
    g_loss[ind[k]] = exposures_feed[k] - output[ind[k]]
IndexError: index 1207 is out of bounds for axis 0 with size 16
(sid2) [ir967@gr046 Learning-to-See-in-the-Dark]$ vi CNN5_FC3_exposure_101GB.py
(sid2) [ir967@gr046 Learning-to-See-in-the-Dark]$ python CNN5_FC3_exposure_101GB.py




Current date and time : 
2020-12-13 17:10:07
./dataset/Sony/short/00133_07_0.1s.ARW 133 0.1
./dataset/Sony/short/00015_09_0.1s.ARW 15 0.1
./dataset/Sony/short/00190_08_0.04s.ARW 190 0.04
./dataset/Sony/short/00204_01_0.033s.ARW 204 0.033
./dataset/Sony/short/00047_03_0.1s.ARW 47 0.1
./dataset/Sony/short/00049_07_0.1s.ARW 49 0.1
./dataset/Sony/short/00231_00_0.033s.ARW 231 0.033
Found 1865 images to train with

Training on files with indices: 118 to 1450
Training on 1332 images only

2020-12-13 17:10:07.425305: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA
2020-12-13 17:10:07.578781: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1411] Found device 0 with properties: 
name: Quadro RTX 8000 major: 7 minor: 5 memoryClockRate(GHz): 1.62
pciBusID: 0000:06:00.0
totalMemory: 44.49GiB freeMemory: 44.33GiB
2020-12-13 17:10:07.578810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1490] Adding visible gpu devices: 0
2020-12-13 17:10:07.868934: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-12-13 17:10:07.868970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977]      0 
2020-12-13 17:10:07.868985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990] 0:   N 
2020-12-13 17:10:07.869082: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1103] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 43038 MB memory) -> physical GPU (device: 0, name: Quadro RTX 8000, pci bus id: 0000:06:00.0, compute capability: 7.5)

Loaded ./gt_Sony_CNN5_FC3_exposure_101GB/model.ckpt

last epoch of previous run: 249


BATCH_SIZE= 16 ,final_epoch= 4001 ,no_of_batches= 83 ,ps 128 ,result_dir= ./gt_Sony_CNN5_FC3_exposure_101GB/ ,len(train_fns)= 1332

Cleared all images in memory.

Epoch 250: at batch 1: Training dataset Loss=0.989082, Batch Time=1.325
Epoch 250: at batch 1: Training dataset Loss=0.989082, Batch Time=1.325; Early rounds
Epoch 250: at batch 2: Training dataset Loss=0.979204, Batch Time=0.029; Early rounds
Epoch 250: at batch 3: Training dataset Loss=0.969253, Batch Time=0.031; Early rounds
Epoch 250: at batch 4: Training dataset Loss=0.960288, Batch Time=0.030; Early rounds
Epoch 250: at batch 5: Training dataset Loss=0.950635, Batch Time=0.027; Early rounds
Epoch 250: at batch 6: Training dataset Loss=0.939180, Batch Time=0.026; Early rounds
Epoch 250: at batch 7: Training dataset Loss=0.930015, Batch Time=0.028; Early rounds
loading ./dataset/Sony/short/00174_04_0.1s.ARW; images_in_memory= 109
Epoch 250: at batch 8: Training dataset Loss=0.920553, Batch Time=0.031; Early rounds
Epoch 250: at batch 9: Training dataset Loss=0.910952, Batch Time=0.027; Early rounds
Epoch 250: at batch 10: Training dataset Loss=0.900944, Batch Time=0.026; Early rounds
Epoch 250: at batch 11: Training dataset Loss=0.891612, Batch Time=0.026; Early rounds
Epoch 250: at batch 12: Training dataset Loss=0.883261, Batch Time=0.026; Early rounds
loading ./dataset/Sony/short/00024_01_0.1s.ARW; images_in_memory= 183
Epoch 250: at batch 13: Training dataset Loss=0.874581, Batch Time=0.028; Early rounds
Epoch 250: at batch 14: Training dataset Loss=0.865875, Batch Time=0.031; Early rounds
Epoch 250: at batch 15: Training dataset Loss=0.855862, Batch Time=0.029; Early rounds
Epoch 250: at batch 16: Training dataset Loss=0.846728, Batch Time=0.030; Early rounds
Epoch 250: at batch 17: Training dataset Loss=0.835840, Batch Time=0.028; Early rounds
Epoch 250: at batch 18: Training dataset Loss=0.828009, Batch Time=0.027; Early rounds
Epoch 250: at batch 19: Training dataset Loss=0.819489, Batch Time=0.028; Early rounds
Epoch 250: at batch 20: Training dataset Loss=0.809548, Batch Time=0.025; Early rounds
Epoch 250: at batch 21: Training dataset Loss=0.798750, Batch Time=0.029; Early rounds
loading ./dataset/Sony/short/00051_01_0.04s.ARW; images_in_memory= 305
Epoch 250: at batch 22: Training dataset Loss=0.791184, Batch Time=0.029; Early rounds
Epoch 250: at batch 23: Training dataset Loss=0.783579, Batch Time=0.030; Early rounds
Epoch 250: at batch 24: Training dataset Loss=0.773703, Batch Time=0.031; Early rounds
Epoch 250: at batch 25: Training dataset Loss=0.767421, Batch Time=0.029; Early rounds
Epoch 250: at batch 26: Training dataset Loss=0.759145, Batch Time=0.026; Early rounds
Epoch 250: at batch 27: Training dataset Loss=0.750925, Batch Time=0.027; Early rounds
Epoch 250: at batch 28: Training dataset Loss=0.742726, Batch Time=0.025; Early rounds
Epoch 250: at batch 29: Training dataset Loss=0.734186, Batch Time=0.030; Early rounds
Epoch 250: at batch 30: Training dataset Loss=0.726919, Batch Time=0.029; Early rounds
Epoch 250: at batch 31: Training dataset Loss=0.716773, Batch Time=0.028; Early rounds
Epoch 250: at batch 32: Training dataset Loss=0.708705, Batch Time=0.027; Early rounds
Epoch 250: at batch 33: Training dataset Loss=0.700306, Batch Time=0.028; Early rounds
Epoch 250: at batch 34: Training dataset Loss=0.692691, Batch Time=0.030; Early rounds
Epoch 250: at batch 35: Training dataset Loss=0.686960, Batch Time=0.029; Early rounds
Epoch 250: at batch 36: Training dataset Loss=0.680076, Batch Time=0.028; Early rounds
Epoch 250: at batch 37: Training dataset Loss=0.675181, Batch Time=0.027; Early rounds
Epoch 250: at batch 38: Training dataset Loss=0.668855, Batch Time=0.026; Early rounds
Epoch 250: at batch 39: Training dataset Loss=0.662110, Batch Time=0.027; Early rounds
Epoch 250: at batch 40: Training dataset Loss=0.655628, Batch Time=0.030; Early rounds
Epoch 250: at batch 41: Training dataset Loss=0.649075, Batch Time=0.031; Early rounds
Epoch 250: at batch 42: Training dataset Loss=0.642363, Batch Time=0.031; Early rounds
Epoch 250: at batch 43: Training dataset Loss=0.634841, Batch Time=0.027; Early rounds
Epoch 250: at batch 44: Training dataset Loss=0.625558, Batch Time=0.029; Early rounds
Epoch 250: at batch 45: Training dataset Loss=0.619129, Batch Time=0.026; Early rounds
Epoch 250: at batch 46: Training dataset Loss=0.615308, Batch Time=0.026; Early rounds
Epoch 250: at batch 47: Training dataset Loss=0.608992, Batch Time=0.028; Early rounds
Epoch 250: at batch 48: Training dataset Loss=0.605601, Batch Time=0.024; Early rounds
Found in memory: ./dataset/Sony/short/00024_01_0.1s.ARW; images_in_memory= 599
Epoch 250: at batch 49: Training dataset Loss=0.604410, Batch Time=0.029; Early rounds
Epoch 250: at batch 50: Training dataset Loss=0.598477, Batch Time=0.028; Early rounds
Epoch 250: at batch 51: Training dataset Loss=0.596195, Batch Time=0.026; Early rounds
Epoch 250: at batch 52: Training dataset Loss=0.587102, Batch Time=0.025; Early rounds
Epoch 250: at batch 53: Training dataset Loss=0.584808, Batch Time=0.027; Early rounds
Epoch 250: at batch 54: Training dataset Loss=0.577493, Batch Time=0.027; Early rounds
Epoch 250: at batch 55: Training dataset Loss=0.571216, Batch Time=0.030; Early rounds
Epoch 250: at batch 56: Training dataset Loss=0.566408, Batch Time=0.031; Early rounds
Epoch 250: at batch 57: Training dataset Loss=0.559613, Batch Time=0.029; Early rounds
loading ./dataset/Sony/short/00231_07_0.04s.ARW; images_in_memory= 671
Epoch 250: at batch 58: Training dataset Loss=0.553321, Batch Time=0.028; Early rounds
Epoch 250: at batch 59: Training dataset Loss=0.550124, Batch Time=0.027; Early rounds
Epoch 250: at batch 60: Training dataset Loss=0.546531, Batch Time=0.027; Early rounds
Found in memory: ./dataset/Sony/short/00231_07_0.04s.ARW; images_in_memory= 695
Epoch 250: at batch 61: Training dataset Loss=0.542157, Batch Time=0.026; Early rounds
Epoch 250: at batch 62: Training dataset Loss=0.538276, Batch Time=0.027; Early rounds
Epoch 250: at batch 63: Training dataset Loss=0.531065, Batch Time=0.031; Early rounds
Epoch 250: at batch 64: Training dataset Loss=0.525928, Batch Time=0.029; Early rounds
loading ./dataset/Sony/short/00158_01_0.1s.ARW; images_in_memory= 731
Epoch 250: at batch 65: Training dataset Loss=0.521336, Batch Time=0.031; Early rounds
Epoch 250: at batch 66: Training dataset Loss=0.517781, Batch Time=0.029; Early rounds
Epoch 250: at batch 67: Training dataset Loss=0.515697, Batch Time=0.028; Early rounds
Epoch 250: at batch 68: Training dataset Loss=0.513251, Batch Time=0.024; Early rounds
Epoch 250: at batch 69: Training dataset Loss=0.509678, Batch Time=0.024; Early rounds
Epoch 250: at batch 70: Training dataset Loss=0.506055, Batch Time=0.031; Early rounds
Epoch 250: at batch 71: Training dataset Loss=0.502780, Batch Time=0.026; Early rounds
Epoch 250: at batch 72: Training dataset Loss=0.495183, Batch Time=0.026; Early rounds
Epoch 250: at batch 73: Training dataset Loss=0.492041, Batch Time=0.028; Early rounds
Epoch 250: at batch 74: Training dataset Loss=0.488326, Batch Time=0.027; Early rounds
Epoch 250: at batch 75: Training dataset Loss=0.484411, Batch Time=0.027; Early rounds
Epoch 250: at batch 76: Training dataset Loss=0.479337, Batch Time=0.026; Early rounds
Epoch 250: at batch 77: Training dataset Loss=0.474194, Batch Time=0.030; Early rounds
Epoch 250: at batch 78: Training dataset Loss=0.470785, Batch Time=0.028; Early rounds
Epoch 250: at batch 79: Training dataset Loss=0.464826, Batch Time=0.027; Early rounds
Epoch 250: at batch 80: Training dataset Loss=0.460389, Batch Time=0.029; Early rounds
Epoch 250: at batch 81: Training dataset Loss=0.454629, Batch Time=0.029; Early rounds
Epoch 250: at batch 82: Training dataset Loss=0.451344, Batch Time=0.029; Early rounds
Epoch 250: at batch 83: Training dataset Loss=0.447928, Batch Time=0.030; Early rounds
		Epoch 250:  Time = 148.131, Avg epoch time=148.131, Current epoch Time=74.066

Loss vector (slice for the first 20 images)
[[ 0.07512522]
 [ 0.4       ]
 [ 1.        ]
 [ 0.21787834]
 [ 0.4       ]
 [ 0.0014537 ]
 [ 1.        ]
 [ 1.        ]
 [ 0.4       ]
 [ 0.1988582 ]
 [ 1.        ]
 [ 1.        ]
 [ 0.4       ]
 [-0.03966844]
 [ 1.        ]
 [ 0.42767686]
 [ 0.15910524]
 [ 0.14446008]
 [ 0.10429054]
 [-0.58973483]]
Epoch 251: at batch 1: Training dataset Loss=0.443689, Batch Time=0.031
Epoch 251: at batch 1: Training dataset Loss=0.443689, Batch Time=0.031; Early rounds
Epoch 251: at batch 2: Training dataset Loss=0.441628, Batch Time=0.026; Early rounds
Epoch 251: at batch 3: Training dataset Loss=0.436756, Batch Time=0.024; Early rounds
Epoch 251: at batch 4: Training dataset Loss=0.432548, Batch Time=0.026; Early rounds
Epoch 251: at batch 5: Training dataset Loss=0.432320, Batch Time=0.026; Early rounds
Epoch 251: at batch 6: Training dataset Loss=0.428822, Batch Time=0.028; Early rounds
Epoch 251: at batch 7: Training dataset Loss=0.426357, Batch Time=0.032; Early rounds
Epoch 251: at batch 8: Training dataset Loss=0.423248, Batch Time=0.029; Early rounds
Epoch 251: at batch 9: Training dataset Loss=0.420490, Batch Time=0.028; Early rounds
Epoch 251: at batch 10: Training dataset Loss=0.416069, Batch Time=0.026; Early rounds
Epoch 251: at batch 11: Training dataset Loss=0.409171, Batch Time=0.028; Early rounds
Epoch 251: at batch 12: Training dataset Loss=0.405264, Batch Time=0.029; Early rounds
Epoch 251: at batch 13: Training dataset Loss=0.401527, Batch Time=0.027; Early rounds
Epoch 251: at batch 14: Training dataset Loss=0.395600, Batch Time=0.027; Early rounds
Epoch 251: at batch 15: Training dataset Loss=0.390846, Batch Time=0.026; Early rounds
Found in memory: ./dataset/Sony/short/00231_07_0.04s.ARW; images_in_memory= 934
Epoch 251: at batch 16: Training dataset Loss=0.388321, Batch Time=0.028; Early rounds
Epoch 251: at batch 17: Training dataset Loss=0.386760, Batch Time=0.031; Early rounds
Epoch 251: at batch 18: Training dataset Loss=0.385238, Batch Time=0.029; Early rounds
Epoch 251: at batch 19: Training dataset Loss=0.381225, Batch Time=0.026; Early rounds
Epoch 251: at batch 20: Training dataset Loss=0.379908, Batch Time=0.024; Early rounds
Epoch 251: at batch 21: Training dataset Loss=0.376281, Batch Time=0.028; Early rounds
Epoch 251: at batch 22: Training dataset Loss=0.372256, Batch Time=0.026; Early rounds
Epoch 251: at batch 23: Training dataset Loss=0.370917, Batch Time=0.029; Early rounds
Epoch 251: at batch 24: Training dataset Loss=0.367444, Batch Time=0.029; Early rounds
Epoch 251: at batch 25: Training dataset Loss=0.364955, Batch Time=0.030; Early rounds
Epoch 251: at batch 26: Training dataset Loss=0.362288, Batch Time=0.032; Early rounds
Epoch 251: at batch 27: Training dataset Loss=0.358195, Batch Time=0.032; Early rounds
Epoch 251: at batch 28: Training dataset Loss=0.355132, Batch Time=0.032; Early rounds
Epoch 251: at batch 29: Training dataset Loss=0.350717, Batch Time=0.032; Early rounds
Epoch 251: at batch 30: Training dataset Loss=0.347363, Batch Time=0.027; Early rounds
Epoch 251: at batch 31: Training dataset Loss=0.345873, Batch Time=0.027; Early rounds
Epoch 251: at batch 32: Training dataset Loss=0.343585, Batch Time=0.026; Early rounds
Epoch 251: at batch 33: Training dataset Loss=0.340600, Batch Time=0.030; Early rounds
Epoch 251: at batch 34: Training dataset Loss=0.337380, Batch Time=0.028; Early rounds
Epoch 251: at batch 35: Training dataset Loss=0.334771, Batch Time=0.028; Early rounds
Epoch 251: at batch 36: Training dataset Loss=0.332523, Batch Time=0.029; Early rounds
Epoch 251: at batch 37: Training dataset Loss=0.328138, Batch Time=0.029; Early rounds
Epoch 251: at batch 38: Training dataset Loss=0.325136, Batch Time=0.027; Early rounds
Epoch 251: at batch 39: Training dataset Loss=0.323176, Batch Time=0.027; Early rounds
Epoch 251: at batch 40: Training dataset Loss=0.321474, Batch Time=0.029; Early rounds
Epoch 251: at batch 41: Training dataset Loss=0.318724, Batch Time=0.026; Early rounds
Epoch 251: at batch 42: Training dataset Loss=0.314506, Batch Time=0.027; Early rounds
Epoch 251: at batch 43: Training dataset Loss=0.311340, Batch Time=0.029; Early rounds
Epoch 251: at batch 44: Training dataset Loss=0.309782, Batch Time=0.029; Early rounds
Epoch 251: at batch 45: Training dataset Loss=0.306659, Batch Time=0.029; Early rounds
Epoch 251: at batch 46: Training dataset Loss=0.305655, Batch Time=0.028; Early rounds
Epoch 251: at batch 47: Training dataset Loss=0.303925, Batch Time=0.025; Early rounds
Found in memory: ./dataset/Sony/short/00231_07_0.04s.ARW; images_in_memory= 1063
Epoch 251: at batch 48: Training dataset Loss=0.301080, Batch Time=0.024; Early rounds
Epoch 251: at batch 49: Training dataset Loss=0.299033, Batch Time=0.025; Early rounds
Epoch 251: at batch 50: Training dataset Loss=0.295202, Batch Time=0.025; Early rounds
Epoch 251: at batch 51: Training dataset Loss=0.294944, Batch Time=0.024; Early rounds
Epoch 251: at batch 52: Training dataset Loss=0.292172, Batch Time=0.024; Early rounds
Epoch 251: at batch 53: Training dataset Loss=0.290311, Batch Time=0.024; Early rounds
Epoch 251: at batch 54: Training dataset Loss=0.286006, Batch Time=0.030; Early rounds
Epoch 251: at batch 55: Training dataset Loss=0.284292, Batch Time=0.032; Early rounds
Epoch 251: at batch 56: Training dataset Loss=0.281824, Batch Time=0.031; Early rounds
Found in memory: ./dataset/Sony/short/00051_01_0.04s.ARW; images_in_memory= 1097
Epoch 251: at batch 57: Training dataset Loss=0.278415, Batch Time=0.029; Early rounds
Epoch 251: at batch 58: Training dataset Loss=0.275973, Batch Time=0.032; Early rounds
Found in memory: ./dataset/Sony/short/00158_01_0.1s.ARW; images_in_memory= 1103
Epoch 251: at batch 59: Training dataset Loss=0.274685, Batch Time=0.029; Early rounds
Epoch 251: at batch 60: Training dataset Loss=0.271595, Batch Time=0.029; Early rounds
Epoch 251: at batch 61: Training dataset Loss=0.270741, Batch Time=0.029; Early rounds
Epoch 251: at batch 62: Training dataset Loss=0.269049, Batch Time=0.029; Early rounds
Epoch 251: at batch 63: Training dataset Loss=0.268196, Batch Time=0.029; Early rounds
Epoch 251: at batch 64: Training dataset Loss=0.266435, Batch Time=0.031; Early rounds
Epoch 251: at batch 65: Training dataset Loss=0.263801, Batch Time=0.028; Early rounds
Epoch 251: at batch 66: Training dataset Loss=0.260510, Batch Time=0.025; Early rounds
Epoch 251: at batch 67: Training dataset Loss=0.259678, Batch Time=0.027; Early rounds
Epoch 251: at batch 68: Training dataset Loss=0.257301, Batch Time=0.029; Early rounds
loading ./dataset/Sony/short/00001_02_0.1s.ARW; images_in_memory= 1127
Epoch 251: at batch 69: Training dataset Loss=0.254354, Batch Time=0.029; Early rounds
Epoch 251: at batch 70: Training dataset Loss=0.253133, Batch Time=0.024; Early rounds
Epoch 251: at batch 71: Training dataset Loss=0.248982, Batch Time=0.030; Early rounds
Epoch 251: at batch 72: Training dataset Loss=0.248519, Batch Time=0.029; Early rounds
loading ./dataset/Sony/short/00052_03_0.1s.ARW; images_in_memory= 1137
Epoch 251: at batch 73: Training dataset Loss=0.244392, Batch Time=0.026; Early rounds
Epoch 251: at batch 74: Training dataset Loss=0.241986, Batch Time=0.029; Early rounds
Epoch 251: at batch 75: Training dataset Loss=0.238539, Batch Time=0.026; Early rounds
Epoch 251: at batch 76: Training dataset Loss=0.236488, Batch Time=0.024; Early rounds
Epoch 251: at batch 77: Training dataset Loss=0.236483, Batch Time=0.026; Early rounds
Epoch 251: at batch 78: Training dataset Loss=0.234341, Batch Time=0.030; Early rounds
Found in memory: ./dataset/Sony/short/00231_07_0.04s.ARW; images_in_memory= 1153
Epoch 251: at batch 79: Training dataset Loss=0.234907, Batch Time=0.026; Early rounds
Epoch 251: at batch 80: Training dataset Loss=0.233550, Batch Time=0.025; Early rounds
Epoch 251: at batch 81: Training dataset Loss=0.231007, Batch Time=0.025; Early rounds
Epoch 251: at batch 82: Training dataset Loss=0.228874, Batch Time=0.028; Early rounds
Epoch 251: at batch 83: Training dataset Loss=0.227495, Batch Time=0.026; Early rounds
		Epoch 251:  Time = 206.328, Avg epoch time=58.026, Current epoch Time=68.776

Loss vector (slice for the first 20 images)
[[ 0.12943363]
 [ 0.4       ]
 [ 1.        ]
 [ 0.21787834]
 [ 0.4       ]
 [ 0.08276784]
 [ 1.        ]
 [ 1.        ]
 [ 0.4       ]
 [ 0.19007254]
 [ 1.        ]
 [ 1.        ]
 [ 0.4       ]
 [ 0.10402453]
 [ 0.06282425]
 [ 0.42767686]
 [ 0.02810115]
 [ 0.20267487]
 [ 0.09645492]
 [-0.50914375]]
Epoch 252: at batch 1: Training dataset Loss=0.225406, Batch Time=0.026
Found in memory: ./dataset/Sony/short/00024_01_0.1s.ARW; images_in_memory= 1201
Found in memory: ./dataset/Sony/short/00024_01_0.1s.ARW; images_in_memory= 1202
Found in memory: ./dataset/Sony/short/00051_01_0.04s.ARW; images_in_memory= 1221
Found in memory: ./dataset/Sony/short/00174_04_0.1s.ARW; images_in_memory= 1225
Found in memory: ./dataset/Sony/short/00174_04_0.1s.ARW; images_in_memory= 1243
Found in memory: ./dataset/Sony/short/00052_03_0.1s.ARW; images_in_memory= 1244
		Epoch 252:  Time = 227.898, Avg epoch time=21.430, Current epoch Time=56.975

Loss vector (slice for the first 20 images)
[[ 0.36733377]
 [ 0.4       ]
 [ 1.        ]
 [ 0.09159458]
 [ 0.4       ]
 [ 0.29353207]
 [-0.08703065]
 [ 0.17295623]
 [ 0.4       ]
 [ 0.26486379]
 [ 1.        ]
 [ 0.35441996]
 [ 0.4       ]
 [ 0.10402453]
 [ 0.24255902]
 [ 0.42767686]
 [ 0.12511855]
 [ 0.20670968]
 [ 0.10274684]
 [-0.60680423]]
Epoch 253: at batch 1: Training dataset Loss=0.168546, Batch Time=0.029
Found in memory: ./dataset/Sony/short/00001_02_0.1s.ARW; images_in_memory= 1277
Found in memory: ./dataset/Sony/short/00231_07_0.04s.ARW; images_in_memory= 1284
Found in memory: ./dataset/Sony/short/00052_03_0.1s.ARW; images_in_memory= 1294
Found in memory: ./dataset/Sony/short/00174_04_0.1s.ARW; images_in_memory= 1302
Found in memory: ./dataset/Sony/short/00052_03_0.1s.ARW; images_in_memory= 1303
Found in memory: ./dataset/Sony/short/00024_01_0.1s.ARW; images_in_memory= 1310
Found in memory: ./dataset/Sony/short/00052_03_0.1s.ARW; images_in_memory= 1310
		Epoch 253:  Time = 237.400, Avg epoch time=9.362, Current epoch Time=47.480

Loss vector (slice for the first 20 images)
[[ 0.36733377]
 [ 0.4       ]
 [ 0.17931473]
 [ 0.25618351]
 [ 0.4       ]
 [ 0.29353207]
 [ 0.3343311 ]
 [ 0.03833318]
 [ 0.4       ]
 [ 0.27001143]
 [ 1.        ]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.15739125]
 [ 0.08826357]
 [ 0.12883931]
 [ 0.12511855]
 [ 0.20670968]
 [ 0.10274684]
 [-0.51419125]]
Epoch 254: at batch 1: Training dataset Loss=0.149054, Batch Time=0.031
Found in memory: ./dataset/Sony/short/00158_01_0.1s.ARW; images_in_memory= 1317
Found in memory: ./dataset/Sony/short/00024_01_0.1s.ARW; images_in_memory= 1317
Found in memory: ./dataset/Sony/short/00001_02_0.1s.ARW; images_in_memory= 1320
Found in memory: ./dataset/Sony/short/00174_04_0.1s.ARW; images_in_memory= 1323
Found in memory: ./dataset/Sony/short/00051_01_0.04s.ARW; images_in_memory= 1323
		Epoch 254:  Time = 241.419, Avg epoch time=3.880, Current epoch Time=40.237

Epoch 255: at batch 1: Training dataset Loss=0.135496, Batch Time=0.024
Found in memory: ./dataset/Sony/short/00231_07_0.04s.ARW; images_in_memory= 1326
Found in memory: ./dataset/Sony/short/00001_02_0.1s.ARW; images_in_memory= 1327
Found in memory: ./dataset/Sony/short/00158_01_0.1s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00051_01_0.04s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00231_07_0.04s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00052_03_0.1s.ARW; images_in_memory= 1329
Found in memory: ./dataset/Sony/short/00001_02_0.1s.ARW; images_in_memory= 1329
		Epoch 255:  Time = 244.906, Avg epoch time=3.348, Current epoch Time=34.987

Epoch 256: at batch 1: Training dataset Loss=0.134378, Batch Time=0.027
Found in memory: ./dataset/Sony/short/00024_01_0.1s.ARW; images_in_memory= 1330
Found in memory: ./dataset/Sony/short/00052_03_0.1s.ARW; images_in_memory= 1330
Found in memory: ./dataset/Sony/short/00001_02_0.1s.ARW; images_in_memory= 1330
Found in memory: ./dataset/Sony/short/00174_04_0.1s.ARW; images_in_memory= 1330
Found in memory: ./dataset/Sony/short/00024_01_0.1s.ARW; images_in_memory= 1330
Found in memory: ./dataset/Sony/short/00001_02_0.1s.ARW; images_in_memory= 1331
		Epoch 256:  Time = 247.326, Avg epoch time=2.271, Current epoch Time=30.916

Epoch 257: at batch 1: Training dataset Loss=0.128667, Batch Time=0.025
Found in memory: ./dataset/Sony/short/00174_04_0.1s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00001_02_0.1s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00174_04_0.1s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00158_01_0.1s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00051_01_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00231_07_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00231_07_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00052_03_0.1s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00001_02_0.1s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00051_01_0.04s.ARW; images_in_memory= 1331
		Epoch 257:  Time = 249.599, Avg epoch time=2.132, Current epoch Time=27.733

Epoch 258: at batch 1: Training dataset Loss=0.139539, Batch Time=0.030
Found in memory: ./dataset/Sony/short/00001_02_0.1s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00052_03_0.1s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00231_07_0.04s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00052_03_0.1s.ARW; images_in_memory= 1331
Found in memory: ./dataset/Sony/short/00052_03_0.1s.ARW; images_in_memory= 1331
		Epoch 258:  Time = 251.783, Avg epoch time=2.044, Current epoch Time=25.178

Epoch 260: at batch 1: Training dataset Loss=0.140502, Batch Time=0.025
		Epoch 260:  Time = 256.403, Avg epoch time=2.114, Current epoch Time=21.367

Loss vector (slice for the first 20 images)
[[ 0.14200056]
 [ 0.4       ]
 [ 0.03768182]
 [ 0.0403654 ]
 [ 0.1442624 ]
 [ 0.02825189]
 [ 0.14094943]
 [ 0.0868572 ]
 [ 0.33083317]
 [ 0.29737979]
 [ 0.05600065]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.05031586]
 [ 0.21079993]
 [ 0.29459596]
 [ 0.18232834]
 [ 0.00427353]
 [ 0.29733521]
 [-0.37328032]]
Epoch 262: at batch 1: Training dataset Loss=0.132303, Batch Time=0.029
Epoch 264: at batch 1: Training dataset Loss=0.128735, Batch Time=0.025
Epoch 266: at batch 1: Training dataset Loss=0.131561, Batch Time=0.024
Epoch 268: at batch 1: Training dataset Loss=0.127919, Batch Time=0.029
Epoch 270: at batch 1: Training dataset Loss=0.128591, Batch Time=0.028
		Epoch 270:  Time = 278.526, Avg epoch time=2.102, Current epoch Time=12.660

Loss vector (slice for the first 20 images)
[[ 0.04721522]
 [ 0.4       ]
 [ 0.25677925]
 [ 0.1676988 ]
 [ 0.4       ]
 [ 0.04872364]
 [ 0.16457057]
 [ 0.07583565]
 [ 0.4       ]
 [ 0.17543411]
 [ 0.12514269]
 [ 0.27515135]
 [ 0.2960216 ]
 [ 0.02065349]
 [ 0.03247267]
 [ 0.02782053]
 [ 0.24383026]
 [ 0.16370481]
 [ 0.24229527]
 [-0.41433874]]
Epoch 272: at batch 1: Training dataset Loss=0.115707, Batch Time=0.028
Epoch 274: at batch 1: Training dataset Loss=0.136078, Batch Time=0.029
Epoch 276: at batch 1: Training dataset Loss=0.128446, Batch Time=0.030
Epoch 278: at batch 1: Training dataset Loss=0.124919, Batch Time=0.025
Epoch 280: at batch 1: Training dataset Loss=0.129358, Batch Time=0.028
		Epoch 280:  Time = 300.848, Avg epoch time=2.103, Current epoch Time=9.402

Loss vector (slice for the first 20 images)
[[ 0.05507183]
 [ 0.4       ]
 [ 0.31623805]
 [ 0.25606209]
 [ 0.4       ]
 [ 0.04005778]
 [ 0.23360312]
 [ 0.00838792]
 [ 0.4       ]
 [ 0.22776812]
 [ 0.24613088]
 [ 0.4       ]
 [ 0.4       ]
 [-0.02268815]
 [ 0.16209745]
 [ 0.07248354]
 [ 0.21727258]
 [ 0.338121  ]
 [ 0.04755843]
 [-0.51729802]]
Epoch 282: at batch 1: Training dataset Loss=0.125904, Batch Time=0.024
Epoch 284: at batch 1: Training dataset Loss=0.124996, Batch Time=0.026
Epoch 286: at batch 1: Training dataset Loss=0.129989, Batch Time=0.023
Epoch 288: at batch 1: Training dataset Loss=0.134063, Batch Time=0.028
Epoch 290: at batch 1: Training dataset Loss=0.131803, Batch Time=0.023
		Epoch 290:  Time = 323.008, Avg epoch time=2.109, Current epoch Time=7.691

Loss vector (slice for the first 20 images)
[[-0.0134666 ]
 [ 0.2023811 ]
 [ 0.10916209]
 [ 0.11600041]
 [ 0.4       ]
 [ 0.03181785]
 [ 0.21097404]
 [ 0.87686262]
 [ 0.4       ]
 [ 0.29881644]
 [ 0.06043512]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.08984309]
 [ 0.21270448]
 [ 0.10714316]
 [ 0.29911345]
 [ 0.10105413]
 [ 0.18901616]
 [-0.38841054]]
Epoch 292: at batch 1: Training dataset Loss=0.140065, Batch Time=0.024
Epoch 294: at batch 1: Training dataset Loss=0.130147, Batch Time=0.025
Epoch 296: at batch 1: Training dataset Loss=0.125065, Batch Time=0.028
Epoch 298: at batch 1: Training dataset Loss=0.123117, Batch Time=0.029
Epoch 300: at batch 1: Training dataset Loss=0.128628, Batch Time=0.022
		Epoch 300:  Time = 345.295, Avg epoch time=2.071, Current epoch Time=6.640

Loss vector (slice for the first 20 images)
[[ 0.17188525]
 [ 0.38792435]
 [ 0.14655429]
 [ 0.17425519]
 [ 0.4       ]
 [ 0.07072574]
 [-0.02404344]
 [ 0.06469047]
 [ 0.4       ]
 [ 0.20290387]
 [-0.01666355]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.06863028]
 [-0.03101969]
 [ 0.11323023]
 [ 0.22507155]
 [ 0.26184976]
 [ 0.11972231]
 [-0.47469399]]
Epoch 302: at batch 1: Training dataset Loss=0.127952, Batch Time=0.022
Epoch 304: at batch 1: Training dataset Loss=0.127826, Batch Time=0.030
Epoch 306: at batch 1: Training dataset Loss=0.135709, Batch Time=0.022
Epoch 308: at batch 1: Training dataset Loss=0.115352, Batch Time=0.029
Epoch 310: at batch 1: Training dataset Loss=0.125412, Batch Time=0.030
		Epoch 310:  Time = 367.599, Avg epoch time=2.079, Current epoch Time=5.929

Loss vector (slice for the first 20 images)
[[ 0.12236702]
 [ 0.4       ]
 [ 0.19128114]
 [ 0.20703107]
 [-0.41554157]
 [ 0.10213917]
 [ 0.13364649]
 [ 0.04180193]
 [ 0.4       ]
 [ 0.27911246]
 [ 0.13400084]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.10397124]
 [ 0.03190577]
 [ 0.15468132]
 [ 0.12220752]
 [ 0.0595302 ]
 [ 0.13139546]
 [-0.6213958 ]]
Epoch 312: at batch 1: Training dataset Loss=0.128633, Batch Time=0.030
Epoch 314: at batch 1: Training dataset Loss=0.128287, Batch Time=0.025
Epoch 316: at batch 1: Training dataset Loss=0.127628, Batch Time=0.025
Epoch 318: at batch 1: Training dataset Loss=0.122245, Batch Time=0.022
Epoch 320: at batch 1: Training dataset Loss=0.123036, Batch Time=0.027
		Epoch 320:  Time = 389.632, Avg epoch time=2.034, Current epoch Time=5.412

Loss vector (slice for the first 20 images)
[[ 0.03392375]
 [ 0.4       ]
 [ 0.39608109]
 [ 0.14396542]
 [ 0.13308438]
 [ 0.07782644]
 [ 0.16525811]
 [ 0.01176834]
 [ 0.4       ]
 [ 0.18482143]
 [-0.13081074]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.01928216]
 [ 0.17760837]
 [ 0.29593343]
 [ 0.06123549]
 [ 0.19911736]
 [ 0.05725467]
 [-0.37100551]]
Epoch 322: at batch 1: Training dataset Loss=0.119291, Batch Time=0.022
Epoch 324: at batch 1: Training dataset Loss=0.134652, Batch Time=0.026
Epoch 326: at batch 1: Training dataset Loss=0.133202, Batch Time=0.025
Epoch 328: at batch 1: Training dataset Loss=0.120202, Batch Time=0.026
Epoch 330: at batch 1: Training dataset Loss=0.124428, Batch Time=0.025
		Epoch 330:  Time = 411.826, Avg epoch time=2.082, Current epoch Time=5.022

Loss vector (slice for the first 20 images)
[[ 0.11762708]
 [ 0.32756531]
 [ 0.1818828 ]
 [ 0.18527907]
 [-0.52983252]
 [ 0.1572175 ]
 [ 0.28849465]
 [ 0.13248062]
 [ 0.4       ]
 [ 0.01895356]
 [ 0.06002963]
 [ 0.4       ]
 [ 0.2010471 ]
 [ 0.06198406]
 [ 0.09024632]
 [ 0.17910933]
 [ 0.24113464]
 [ 0.09378129]
 [ 0.14527899]
 [-0.50030873]]
Epoch 332: at batch 1: Training dataset Loss=0.116128, Batch Time=0.024
Epoch 334: at batch 1: Training dataset Loss=0.127657, Batch Time=0.025
Epoch 336: at batch 1: Training dataset Loss=0.133871, Batch Time=0.030
Epoch 338: at batch 1: Training dataset Loss=0.121559, Batch Time=0.022
Epoch 340: at batch 1: Training dataset Loss=0.137563, Batch Time=0.024
		Epoch 340:  Time = 434.059, Avg epoch time=2.074, Current epoch Time=4.718

Loss vector (slice for the first 20 images)
[[ 0.45917547]
 [ 0.4       ]
 [ 0.17137426]
 [ 0.15246558]
 [ 0.4       ]
 [ 0.0866822 ]
 [ 0.31990731]
 [ 0.15568203]
 [ 0.4       ]
 [ 0.17566931]
 [ 0.06157017]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.02115381]
 [ 0.25074083]
 [ 0.25385445]
 [ 0.2466436 ]
 [ 0.09812826]
 [ 0.04833233]
 [-0.41539172]]
Epoch 342: at batch 1: Training dataset Loss=0.122618, Batch Time=0.024
Epoch 344: at batch 1: Training dataset Loss=0.136711, Batch Time=0.028
Epoch 346: at batch 1: Training dataset Loss=0.136020, Batch Time=0.022
Epoch 348: at batch 1: Training dataset Loss=0.133326, Batch Time=0.027
Epoch 350: at batch 1: Training dataset Loss=0.133652, Batch Time=0.024
		Epoch 350:  Time = 456.225, Avg epoch time=2.063, Current epoch Time=4.473

Loss vector (slice for the first 20 images)
[[ 0.05055642]
 [ 0.4       ]
 [ 0.01675451]
 [ 0.4910937 ]
 [-0.02602825]
 [ 0.23944336]
 [ 0.10469574]
 [ 0.05125558]
 [ 0.4       ]
 [ 0.19755298]
 [ 0.01039356]
 [ 0.4       ]
 [ 0.3072163 ]
 [ 0.09424913]
 [ 0.30247253]
 [ 0.36753589]
 [ 0.11181831]
 [ 0.03394961]
 [-0.13889945]
 [-0.36018137]]
Epoch 352: at batch 1: Training dataset Loss=0.131900, Batch Time=0.025
Epoch 354: at batch 1: Training dataset Loss=0.128608, Batch Time=0.029
Epoch 356: at batch 1: Training dataset Loss=0.128702, Batch Time=0.023
Epoch 358: at batch 1: Training dataset Loss=0.129943, Batch Time=0.025
Epoch 360: at batch 1: Training dataset Loss=0.108751, Batch Time=0.023
		Epoch 360:  Time = 478.408, Avg epoch time=2.088, Current epoch Time=4.272

Loss vector (slice for the first 20 images)
[[ 0.09748656]
 [ 0.20766222]
 [ 0.24134064]
 [-0.08717012]
 [ 0.36763144]
 [ 0.13020784]
 [ 0.2208668 ]
 [ 0.05069327]
 [ 0.23370973]
 [ 0.16795605]
 [ 0.09372157]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.0795247 ]
 [ 0.09279346]
 [ 0.1710723 ]
 [ 0.1020667 ]
 [ 0.00171196]
 [ 0.13066679]
 [-0.56671283]]
Epoch 362: at batch 1: Training dataset Loss=0.121382, Batch Time=0.023
Epoch 364: at batch 1: Training dataset Loss=0.128099, Batch Time=0.026
Epoch 366: at batch 1: Training dataset Loss=0.123889, Batch Time=0.024
Epoch 368: at batch 1: Training dataset Loss=0.122521, Batch Time=0.028
Epoch 370: at batch 1: Training dataset Loss=0.128446, Batch Time=0.027
		Epoch 370:  Time = 500.462, Avg epoch time=2.031, Current epoch Time=4.102

Loss vector (slice for the first 20 images)
[[ 0.15975875]
 [ 0.4       ]
 [ 0.16643006]
 [ 0.12770021]
 [ 0.07178762]
 [ 0.01216537]
 [-0.03530347]
 [ 0.19596481]
 [ 0.4       ]
 [ 0.05602694]
 [-0.06899798]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.61640209]
 [ 0.23096323]
 [ 0.0795086 ]
 [ 0.05060983]
 [ 0.04501414]
 [ 0.06698281]
 [-0.28581301]]
Epoch 372: at batch 1: Training dataset Loss=0.128672, Batch Time=0.026
Epoch 374: at batch 1: Training dataset Loss=0.128616, Batch Time=0.027
Epoch 376: at batch 1: Training dataset Loss=0.132841, Batch Time=0.027
Epoch 378: at batch 1: Training dataset Loss=0.138124, Batch Time=0.030
Epoch 380: at batch 1: Training dataset Loss=0.133121, Batch Time=0.026
		Epoch 380:  Time = 522.673, Avg epoch time=2.077, Current epoch Time=3.960

Loss vector (slice for the first 20 images)
[[ 0.20399439]
 [ 0.4       ]
 [ 0.04864496]
 [ 0.32654339]
 [ 0.4       ]
 [ 0.05962926]
 [ 0.19742948]
 [ 0.19947386]
 [ 0.31064967]
 [ 0.21020907]
 [ 0.2403273 ]
 [ 0.4       ]
 [ 0.21544468]
 [ 0.15586442]
 [ 0.15380251]
 [ 0.07913709]
 [ 0.17086852]
 [ 0.17039144]
 [ 0.10405689]
 [-0.38808331]]
Epoch 382: at batch 1: Training dataset Loss=0.120225, Batch Time=0.028
Epoch 384: at batch 1: Training dataset Loss=0.128502, Batch Time=0.026
Epoch 386: at batch 1: Training dataset Loss=0.131440, Batch Time=0.024
Epoch 388: at batch 1: Training dataset Loss=0.127809, Batch Time=0.028
Epoch 390: at batch 1: Training dataset Loss=0.126423, Batch Time=0.028
		Epoch 390:  Time = 544.829, Avg epoch time=2.075, Current epoch Time=3.837

Loss vector (slice for the first 20 images)
[[ 0.11592788]
 [ 0.4       ]
 [ 0.42842227]
 [ 0.11375976]
 [ 0.4       ]
 [ 0.08699232]
 [ 0.3050372 ]
 [-0.0027169 ]
 [ 0.4       ]
 [ 0.1978001 ]
 [ 0.10207957]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.22076541]
 [ 0.2709918 ]
 [ 0.04022485]
 [ 0.03876191]
 [ 0.21341211]
 [ 0.05635005]
 [-0.58357403]]
Epoch 392: at batch 1: Training dataset Loss=0.130614, Batch Time=0.022
Epoch 394: at batch 1: Training dataset Loss=0.131870, Batch Time=0.028
Epoch 396: at batch 1: Training dataset Loss=0.119665, Batch Time=0.026
Epoch 398: at batch 1: Training dataset Loss=0.113234, Batch Time=0.029
Epoch 400: at batch 1: Training dataset Loss=0.125271, Batch Time=0.025
		Epoch 400:  Time = 566.918, Avg epoch time=2.064, Current epoch Time=3.730

Loss vector (slice for the first 20 images)
[[ 0.21893299]
 [ 0.19627651]
 [-0.02709234]
 [ 0.107831  ]
 [ 0.4       ]
 [ 0.13421881]
 [ 0.15924209]
 [ 0.05280888]
 [ 0.4       ]
 [ 0.00202662]
 [ 0.0926308 ]
 [ 0.30984138]
 [ 0.4       ]
 [-0.01713574]
 [ 0.09004653]
 [ 0.12310779]
 [ 0.10994154]
 [ 0.04137897]
 [ 0.12190431]
 [-0.54662642]]
Epoch 402: at batch 1: Training dataset Loss=0.126849, Batch Time=0.027
Epoch 404: at batch 1: Training dataset Loss=0.122543, Batch Time=0.027
Epoch 406: at batch 1: Training dataset Loss=0.134083, Batch Time=0.027
Epoch 408: at batch 1: Training dataset Loss=0.136939, Batch Time=0.029
Epoch 410: at batch 1: Training dataset Loss=0.121571, Batch Time=0.028
		Epoch 410:  Time = 589.088, Avg epoch time=2.047, Current epoch Time=3.636

Loss vector (slice for the first 20 images)
[[ 0.13983351]
 [ 0.4       ]
 [ 0.07580376]
 [ 0.23701435]
 [ 0.4       ]
 [ 0.17830455]
 [ 0.1458981 ]
 [-0.01242626]
 [ 0.4       ]
 [ 0.2849136 ]
 [ 0.01657432]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.06512177]
 [ 0.21150464]
 [ 0.18132305]
 [ 0.00814092]
 [ 0.27134794]
 [ 0.40775234]
 [-0.42457489]]
Epoch 412: at batch 1: Training dataset Loss=0.111213, Batch Time=0.027
Epoch 414: at batch 1: Training dataset Loss=0.135946, Batch Time=0.027
Epoch 416: at batch 1: Training dataset Loss=0.134419, Batch Time=0.025
Epoch 418: at batch 1: Training dataset Loss=0.119823, Batch Time=0.028
Epoch 420: at batch 1: Training dataset Loss=0.127258, Batch Time=0.025
		Epoch 420:  Time = 611.178, Avg epoch time=2.080, Current epoch Time=3.553

Loss vector (slice for the first 20 images)
[[ 0.09516776]
 [ 0.4       ]
 [ 0.04549128]
 [ 0.0536412 ]
 [-0.31639776]
 [ 0.2050932 ]
 [ 0.19855547]
 [ 0.22694606]
 [ 0.4       ]
 [ 0.28362507]
 [ 0.15804619]
 [ 0.13247117]
 [ 0.4       ]
 [-0.02701569]
 [ 0.1929422 ]
 [ 0.07290238]
 [ 0.20079207]
 [ 0.04494256]
 [-0.06384575]
 [-0.15026686]]
Epoch 422: at batch 1: Training dataset Loss=0.143860, Batch Time=0.025
Epoch 424: at batch 1: Training dataset Loss=0.130654, Batch Time=0.026
Epoch 426: at batch 1: Training dataset Loss=0.116321, Batch Time=0.027
Epoch 428: at batch 1: Training dataset Loss=0.136190, Batch Time=0.025
Epoch 430: at batch 1: Training dataset Loss=0.129731, Batch Time=0.030
		Epoch 430:  Time = 633.447, Avg epoch time=2.035, Current epoch Time=3.480

Loss vector (slice for the first 20 images)
[[ 0.25536668]
 [ 0.4       ]
 [ 0.08736593]
 [ 0.25574094]
 [ 0.4       ]
 [ 0.28149891]
 [ 0.15732628]
 [ 0.05926383]
 [ 0.4       ]
 [ 0.19288772]
 [ 0.06841964]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.15025938]
 [ 0.1594699 ]
 [ 0.12531424]
 [ 0.16000938]
 [ 0.06547743]
 [ 0.04150486]
 [-0.36429009]]
Epoch 432: at batch 1: Training dataset Loss=0.134206, Batch Time=0.030
Epoch 434: at batch 1: Training dataset Loss=0.142875, Batch Time=0.025
Epoch 436: at batch 1: Training dataset Loss=0.130676, Batch Time=0.024
Epoch 438: at batch 1: Training dataset Loss=0.119630, Batch Time=0.025
Epoch 440: at batch 1: Training dataset Loss=0.118519, Batch Time=0.028
		Epoch 440:  Time = 655.635, Avg epoch time=2.075, Current epoch Time=3.415

Loss vector (slice for the first 20 images)
[[ 0.06668943]
 [ 0.4       ]
 [-0.04294395]
 [ 0.11749709]
 [ 0.4       ]
 [ 0.19085115]
 [ 0.12388229]
 [-0.04074001]
 [ 0.36160266]
 [ 0.34512699]
 [ 0.07529902]
 [ 0.4       ]
 [ 0.4       ]
 [-0.13853991]
 [ 0.03610313]
 [ 0.06344664]
 [ 0.13066095]
 [ 0.03493935]
 [-0.15662467]
 [-0.54135916]]
Epoch 442: at batch 1: Training dataset Loss=0.129611, Batch Time=0.025
Epoch 444: at batch 1: Training dataset Loss=0.135985, Batch Time=0.027
Epoch 446: at batch 1: Training dataset Loss=0.120001, Batch Time=0.028
Epoch 448: at batch 1: Training dataset Loss=0.130463, Batch Time=0.024
Epoch 450: at batch 1: Training dataset Loss=0.137296, Batch Time=0.025
		Epoch 450:  Time = 677.770, Avg epoch time=2.069, Current epoch Time=3.355

Loss vector (slice for the first 20 images)
[[ 0.09709889]
 [ 0.4       ]
 [ 0.16467816]
 [ 0.16621554]
 [ 0.16516315]
 [ 0.06791675]
 [ 0.09479648]
 [ 0.06289268]
 [ 0.4       ]
 [ 0.24418539]
 [ 0.03810102]
 [ 0.4       ]
 [ 0.4       ]
 [ 0.04398692]
 [ 0.04426181]
 [-0.01733196]
 [ 0.26365757]
 [ 0.46397346]
 [ 0.23790807]
 [-0.56635437]]
Epoch 452: at batch 1: Training dataset Loss=0.122536, Batch Time=0.026
Epoch 454: at batch 1: Training dataset Loss=0.128008, Batch Time=0.028
Epoch 456: at batch 1: Training dataset Loss=0.134479, Batch Time=0.030
Epoch 458: at batch 1: Training dataset Loss=0.118858, Batch Time=0.029
Epoch 460: at batch 1: Training dataset Loss=0.129457, Batch Time=0.025
		Epoch 460:  Time = 699.963, Avg epoch time=2.093, Current epoch Time=3.302

Loss vector (slice for the first 20 images)
[[ 0.0563398 ]
 [ 0.4       ]
 [ 0.14158642]
 [ 0.11550754]
 [ 0.4       ]
 [ 0.29824799]
 [ 0.26414174]
 [-0.08287752]
 [ 0.4       ]
 [ 0.56441635]
 [ 0.06150037]
 [ 0.4       ]
 [ 0.05348294]
 [-0.0290854 ]
 [ 0.20495236]
 [ 0.09622782]
 [ 0.10566854]
 [ 0.16391581]
 [ 0.13046676]
 [-0.39337144]]
Epoch 462: at batch 1: Training dataset Loss=0.133327, Batch Time=0.024
Epoch 464: at batch 1: Training dataset Loss=0.135935, Batch Time=0.028
Epoch 466: at batch 1: Training dataset Loss=0.116948, Batch Time=0.028
^CTraceback (most recent call last):
  File "CNN5_FC3_exposure_101GB.py", line 290, in <module>
    feed_dict={in_image: input_patch, gt_exposure: exposures_feed, lr: learning_rate})
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 887, in run
    run_metadata_ptr)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1110, in _run
    feed_dict_tensor, options, run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1286, in _do_run
    run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1292, in _do_call
    return fn(*args)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1277, in _run_fn
    options, feed_dict, fetch_list, target_list, run_metadata)
  File "/scratch/ir967/install/miniconda3/envs/sid2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1367, in _call_tf_sessionrun
    run_metadata)
KeyboardInterrupt
(sid2) [ir967@gr046 Learning-to-See-in-the-Dark]$